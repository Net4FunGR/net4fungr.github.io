[{"categories":["Art of Labbing"],"collections":null,"content":"Intro Cisco IOL, aka IOU, and I go back a long way. Back in the day, when I decided to sit for the CCIE R\u0026amp;S v4 exam, there weren\u0026rsquo;t many options to get hands on on the cli other than having access to real devices or using dynamips and GNS3 for simulating routers and switches, but with limited feature support, especially on the S side. Hence, since I have also been known as a big time pack rat, I decided to go for a real LAB. So, after spending nearly over $6k, I managed to build a real home lab with all the quirks. Terminal server, AUI converters , x-over and serial cables, remote managed PSUs, I remember being so happy! The joy went on for a couple of months, while I was setting up the scenarios, practicing the theory, re-configuring and starting over. Then one day, as I was googling for something, I bumped to some news that Cisco IOS-on-Unix has been leaked to the public and people were already using it to build virtual labs. Then an IOS-on-Linux version was also leaked and you can imagine how stupid I felt after this. I, of course, ended up in using it and even got a refurb SPARCstation to run IOU in the interim before I could get my hands on a stable IOL version without major issues. Anyway, this was just to give you the context of my history with IOL. I never sat for the lab in the end, but that\u0026rsquo;s another story. So, imagine my excitement when the recent news of IOL support in containerlab and netlab reached my eyes. It brought back so many reminisces, that I had to write something about it. Containerlab and netlab have always been in my back-blog, and I wanted to post about them, but I feel that the community is doing such a great job on them, whilst in this case I felt that the time is right for me as well. As a bottom line here, IOL has always been the preferred choice due to its low demand on resources for the supported and relevant use cases, and the fact that it can now be supported in containerised topologies increases its network automation friendliness (NAFness :)) level, not too much though, since it is lacking some features as we will see later on. ","date":"2024-11-05","objectID":"/posts/iou-love/:1:0","tags":["netsim","netlab","containerlab","automation"],"title":"For those about to LAB...⚡","uri":"/posts/iou-love/#intro"},{"categories":["Art of Labbing"],"collections":null,"content":"Use Case Brief Everything starts with a use case, the intent! In my case, I want to spin up topologies and test different things in Kentik. The idea came from the srl-telemetry-lab, so we will be following the same approach but for Kentik related use cases. Hence, Dynamic topology that is sending netflow to Kentik via the kproxy agent Traffic simulation using iperf3 on the devices Kentik Synthetic agents attached to the topology performing tests Devices registered in Kentik NMS for monitoring ","date":"2024-11-05","objectID":"/posts/iou-love/:2:0","tags":["netsim","netlab","containerlab","automation"],"title":"For those about to LAB...⚡","uri":"/posts/iou-love/#use-case-brief"},{"categories":["Art of Labbing"],"collections":null,"content":"Why containerlab? Dynamic topology: support CRUDs on the infrastructure IaC model support: everything defined in structured format Integrates well with netlab as a provider Minimal effort on the above / more focus on the use cases ","date":"2024-11-05","objectID":"/posts/iou-love/:2:1","tags":["netsim","netlab","containerlab","automation"],"title":"For those about to LAB...⚡","uri":"/posts/iou-love/#why-containerlab"},{"categories":["Art of Labbing"],"collections":null,"content":"Why netlab? Automatic provisioning of underlay connectivity details and protocols Flexibility in adding custom templates and using custom playbooks based on a common ansible inventory file Good integration with containerlab Minimal effort on the above / more focus on the use cases I think netlab is the perfect example of Automating the Boring Stuff. I never counted the times or will ever forget that I had to type those commands \u0026#x1f604; : Boring Stuff conf t ⏎ no ip do lo ⏎ line con 0 ⏎ logg syn ⏎ exec-t 0 ⏎ ^Z wr ⏎ ","date":"2024-11-05","objectID":"/posts/iou-love/:2:2","tags":["netsim","netlab","containerlab","automation"],"title":"For those about to LAB...⚡","uri":"/posts/iou-love/#why-netlab"},{"categories":["Art of Labbing"],"collections":null,"content":"Why IOL? Demand on resources is very low Fit my use case since netflow export is supported No licensing limits on traffic volumes Seems IOL reports interface errors when traffic is pushed. A bug or a feature, don\u0026rsquo;t care at the moment since it has always been a burden to fire up synthetic errors on interface counters. ","date":"2024-11-05","objectID":"/posts/iou-love/:2:3","tags":["netsim","netlab","containerlab","automation"],"title":"For those about to LAB...⚡","uri":"/posts/iou-love/#why-iol"},{"categories":["Art of Labbing"],"collections":null,"content":"Things Covered Now that we have picked up the correct tools for the job, let\u0026rsquo;s see what we are going to cover so you can assess if this is still interesting enough to carry on reading. Using netlab to provision a lab by using containerlab for spinning up the nodes Using IOL for the network device type Using various containers to integrate with the topology as the linux device type Using iperf3 for generating traffic load on the topology Using OSPF, BGP, MPLS and VRF modules in netlab Using jinja templates to provision additional commands to the nodes either ad-hoc or during lab bring-up. Defining custom variables in the topology so we can use them later on either in the templates or in ansible plays. We will be using Windows Subsystem for Linux (WSL) for this use case. So by using all the above we can deploy a topology like the one depicted in the following section, register the devices in Kentik, produce traffic and explore how to visualise, monitor, and run synthetic tests on it via Kentik. And all this by using automated tasks on the way. ","date":"2024-11-05","objectID":"/posts/iou-love/:2:4","tags":["netsim","netlab","containerlab","automation"],"title":"For those about to LAB...⚡","uri":"/posts/iou-love/#things-covered"},{"categories":["Art of Labbing"],"collections":null,"content":"Topology Walk-through LAB Topology As you see, the topology is rather simple. We have four core devices (rcX) running an MPLS-LDP backbone and offering L3VPN services over MP-BGP to four CE devices (ceX). The PC nodes are used to generate traffic and the ksynth nodes are Kentik private synthetics agents. As you may have guessed, we have two VPNs, red and blue. Network devices are IOL containers and PC\u0026rsquo;s are based on the network-multitool container image. Of course, we also have additional containers running and I like to call them service containers, since they are there to interact with the topology and report data back to Kentik, realising our use case. The basic idea behind connectivity in netlab/containerlab is that there is a management docker bridge network that all nodes connect. Each node connects to the management network, usually by its first interface, and netlab uses this to provision commands to it. All other node links are formed based on the desired configuration by bridging the relevant node interfaces. Hence, the service containers, do not have to connect in-line to the topology and all communications go via the management network. But, let\u0026rsquo;s see them one by one: kproxy: Kentik agent that listens to netflow from the devices and reports to Kentik. This agent also polls devices for SNMP metrics and metadata. All communications are going via the management network, from/to the devices in the isolated management VRF. In addition, since this is a container attached to a docker network inside the host machine, it has access to the Internet by default, so it is able to go out to Kentik and report the data. kbgp: Kentik agent that tunnels bgp sessions from the devices all the way up to Kentik. This agent actually needs an inline connection so all other nodes can form the session. In our case, it is connected to rc1 so all others can reach it. Communication with Kentik is done via the container\u0026rsquo;s first interface that belongs to the management network, the same way as kproxy does. kagent: Kentik\u0026rsquo;s Universal Agent that is part of the Kentik NMS product and is polling the devices for SNMP data and reporting it into the NMS part. Here, I have chosen to deploy this as a standard container on the host machine (WSL) and it is not included in the topology. Thus, netlab is not aware of it at all, but still kagent can communicate with the devices via the management network since they are part of the same host, just in a different docker bridge network. So communication is possible since this agent is only polling the devices via their management interfaces and there is no need to expose anything (or should I say ingress path) from its side. We will expand more on the connectivity details as we build the topology further on, but for now, the three basic things to ask yourself regarding how to run your service containers are: Do they need to be controlled somehow via netlab? Does netlab need to know any detail about them? Are your devices capable to communicate with them via their management VRF for their purpose? ","date":"2024-11-05","objectID":"/posts/iou-love/:2:5","tags":["netsim","netlab","containerlab","automation"],"title":"For those about to LAB...⚡","uri":"/posts/iou-love/#topology-walk-through"},{"categories":["Art of Labbing"],"collections":null,"content":"Setting Things Up In this section we are going to focus on how to set things up in WSL. We will start by building the IOL containers for our use case and then see a typical installation of netlab. ","date":"2024-11-05","objectID":"/posts/iou-love/:3:0","tags":["netsim","netlab","containerlab","automation"],"title":"For those about to LAB...⚡","uri":"/posts/iou-love/#setting-things-up"},{"categories":["Art of Labbing"],"collections":null,"content":"Building the IOL containers IOL containers are supported in containerlab via their vrnetlab fork. The current IOL images are those included with the Cisco CML image reference collection (refplat), so you have to get hold of them and copy them to the relevant build directory in vrnetlab in order to build them. Here are the steps: \u0026gt;\u0026gt;\u0026gt; Clone the repo: (wsl)\u0026lt;\u0026lt;· git clone https://github.com/hellt/vrnetlab Cloning into \u0026#39;vrnetlab\u0026#39;... remote: Enumerating objects: 5363, done. remote: Counting objects: 100% (2011/2011), done. remote: Compressing objects: 100% (660/660), done. remote: Total 5363 (delta 1542), reused 1649 (delta 1348), pack-reused 3352 (from 1) Receiving objects: 100% (5363/5363), 2.27 MiB | 664.00 KiB/s, done. Resolving deltas: 100% (3245/3245), done.\u0026gt;\u0026gt;\u0026gt; Copy the images to the appropriate folder and rename them according to the README. I chose to make links to them instead: (wsl)\u0026lt;\u0026lt;· cd vrnetlab/cisco/iol/ (wsl)\u0026lt;\u0026lt;· lll total 516920 -rw-r--r-- 1 289 Makefile -rw-r--r-- 1 1721 README.md lrwxrwxrwx 1 35 cisco_iol-17.12.1.bin -\u0026gt; x86_64_crb_linux-adventerprisek9-ms lrwxrwxrwx 1 38 cisco_iol-L2-17.12.1.bin -\u0026gt; x86_64_crb_linux_l2-adventerprisek9-ms drwxr-xr-x 2 4096 docker -rwxr-xr-x 1 288947184 x86_64_crb_linux-adventerprisek9-ms -rwxr-xr-x 1 240355720 x86_64_crb_linux_l2-adventerprisek9-ms\u0026gt;\u0026gt;\u0026gt; Build with make docker-image and check in docker. You should end up having the two IOL images in the registry. One for the router and one for the switch. (wsl)\u0026lt;\u0026lt;· make docker-image \u0026lt; build output \u0026gt; (wsl)\u0026lt;\u0026lt;· docker images vrnetlab/cisco_iol* REPOSITORY TAG IMAGE ID CREATED SIZE vrnetlab/cisco_iol L2-17.12.1 15d363218573 About a minute ago 607MB vrnetlab/cisco_iol 17.12.1 1d44f7fa6252 About a minute ago 704MB\u0026gt;\u0026gt;\u0026gt; You can test with the following command to see if they are booting (wsl)\u0026lt;\u0026lt;· docker run -it --rm -e IOL_PID=1 \\ --mount type=bind,source=/dev/null,target=/iol/NETMAP vrnetlab/cisco_iol:17.12.1 Launching IOL with PID 1 Failed to send flush request: Operation not permitted Flushed eth0 addresses /entrypoint.sh: line 14: /usr/bin/iouyap: Operation not permitted /entrypoint.sh: line 14: /usr/bin/iouyap: Success IOS On Unix - Cisco Systems confidential, internal use only IOURC: Could not open iourc file Warning: configuration file (config.txt) does not exist Warning: Abnormal ciscoversion string, please notify the IOU team with the name of this branch Warning: we parsed - NULL Restricted Rights Legend Use, duplication, or disclosure by the Government is subject to restrictions as set forth in subparagraph (c) of the Commercial Computer Software - Restricted Rights clause at FAR sec. 52.227-19 and subparagraph (c) (1) (ii) of the Rights in Technical Data and Computer Software clause at DFARS sec. 252.227-7013. Cisco Systems, Inc. 170 West Tasman Drive San Jose, California 95134-1706 Cisco IOS Software [Dublin], Linux Software (X86_64BI_LINUX-ADVENTERPRISEK9-M), Version 17.12.1, RELEASE SOFTWARE (fc5) Technical Support: http://www.cisco.com/techsupport Copyright (c) 1986-2023 by Cisco Systems, Inc. Compiled Thu 27-Jul-23 22:33 by mcpre PM unix notify udp ports APP_ID:0 DISABLE Port1:0 Port2:0 PM unix notify udp ports APP_ID:1 DISABLE Port1:0 Port2:0Linux Unix (i686) processor with 550115K bytes of memory. Processor board ID 1 4 Ethernet interfaces 1024K bytes of NVRAM. No startup-config, starting autoinstall/pnp/ztp... Autoinstall will terminate if any input is detected on console Autoinstall trying DHCPv4 on Ethernet0/0,Ethernet0/1,Ethernet0/2,Ethernet0/3 \u0026lt; snipped \u0026gt;They are based on XE 17.12.1 release so they are more feature rich than the past IOL ones (IOS 15.x), they do not support any of the automation goodies as well as streaming telemetry, but they can push traffic without any licensing restriction. ","date":"2024-11-05","objectID":"/posts/iou-love/:3:1","tags":["netsim","netlab","containerlab","automation"],"title":"For those about to LAB...⚡","uri":"/posts/iou-love/#building-the-iol-containers"},{"categories":["Art of Labbing"],"collections":null,"content":"Installing Netlab in WSL Now that we have our devices, let\u0026rsquo;s go ahead and install netlab in WSL. I am using the following versions and linux flavour: (wsl)·\u0026gt;\u0026gt; wsl.exe -v WSL version: 2.3.26.0 Kernel version: 5.15.167.4-1 WSLg version: 1.0.65 MSRDC version: 1.2.5620 Direct3D version: 1.611.1-81528511 DXCore version: 10.0.26100.1-240331-1435.ge-release Windows version: 10.0.19045.5131 (wsl)\u0026lt;\u0026lt;· cat /etc/os-release PRETTY_NAME=\u0026#34;Ubuntu 22.04.5 LTS\u0026#34; NAME=\u0026#34;Ubuntu\u0026#34; VERSION_ID=\u0026#34;22.04\u0026#34; VERSION=\u0026#34;22.04.5 LTS (Jammy Jellyfish)\u0026#34; VERSION_CODENAME=jammy ID=ubuntu ID_LIKE=debian HOME_URL=\u0026#34;https://www.ubuntu.com/\u0026#34; SUPPORT_URL=\u0026#34;https://help.ubuntu.com/\u0026#34; BUG_REPORT_URL=\u0026#34;https://bugs.launchpad.net/ubuntu/\u0026#34; PRIVACY_POLICY_URL=\u0026#34;https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\u0026#34; UBUNTU_CODENAME=jammy\u0026gt;\u0026gt;\u0026gt; I chose to install netlab in a virtual env: (wsl)\u0026lt;\u0026lt;· sudo apt install python-is-python3 python3-pip python3-venv \u0026lt; snipped \u0026gt; (wsl)\u0026lt;\u0026lt;· mkdir -p netlab/iol (wsl)\u0026lt;\u0026lt;· cd netlab/iol (wsl)\u0026lt;\u0026lt;· python -m venv .venv (wsl)\u0026lt;\u0026lt;· source .venv/bin/activate\u0026gt;\u0026gt;\u0026gt; Install netlab via pip and then all dependencies: (wsl)\u0026lt;\u0026lt;· pip install networklab \u0026lt; snipped \u0026gt; (wsl)\u0026lt;\u0026lt;· netlab install ubuntu ansible containerlab \u0026lt; snipped \u0026gt;\u0026gt;\u0026gt;\u0026gt; Open another terminal to cause re-login and bring up the test topology to verify everything is set up correctly: (wsl)\u0026lt;\u0026lt;· cd netlab/iol (wsl)\u0026lt;\u0026lt;· source .venv/bin/activate (wsl)\u0026lt;\u0026lt;· netlab test clab \u0026lt; snipped \u0026gt;Netalab will provision a 3 node topology based on frr and test if everything is working as expected. Finally it will cleanup after itself. ","date":"2024-11-05","objectID":"/posts/iou-love/:3:2","tags":["netsim","netlab","containerlab","automation"],"title":"For those about to LAB...⚡","uri":"/posts/iou-love/#installing-netlab-in-wsl"},{"categories":["Art of Labbing"],"collections":null,"content":"Topology Definition Let\u0026rsquo;s now start the fun part by exploring how to define our LAB so netlab can spin it up and provision it the way we want it. Netlab expects us to define everything in a single topology definition file, by default topology.yml. We are going to build the base topology that includes just the routers and the PC\u0026rsquo;s, and then move on with adding the service containers. ","date":"2024-11-05","objectID":"/posts/iou-love/:4:0","tags":["netsim","netlab","containerlab","automation"],"title":"For those about to LAB...⚡","uri":"/posts/iou-love/#topology-definition"},{"categories":["Art of Labbing"],"collections":null,"content":"Building the Base Topology - Step-by-step The first thing to do in the topology file is to declare which virtualisation provider we are going to be using. For our case this is clab and here is how to check what others are supported along with their status in the current installation: (wsl)\u0026lt;\u0026lt;· netlab show providers Supported virtualization providers ┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┃ provider ┃ description ┃ status ┃ ┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ │ clab │ containerlab with Docker │ OK │ │ external │ External devices │ OK │ │ libvirt │ Vagrant with libvirt/KVM │ N/A │ │ virtualbox │ Vagrant with Virtualbox │ N/A │ └────────────┴──────────────────────────┴────────┘The minimum sections that netlab expects to find in the topology file is the provider, the nodes and the links. And the minimum attributes that it needs to know about a node before provisioning it is the device type we want to use. Here are the supported device types: (wsl)\u0026lt;\u0026lt;· netlab show devices Virtual network devices supported by netlab ┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ device ┃ description ┃ ┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩ │ arubacx │ ArubaOS-CX │ │ asav │ Cisco ASAv │ │ cat8000v │ Cisco CSR 1000v │ │ csr │ Cisco CSR 1000v │ │ cumulus │ Cumulus VX 4.x or 5.x configured without NVUE │ │ cumulus_nvue │ Cumulus VX 5.x configured with NVUE │ │ dellos10 │ Dell OS10 │ │ eos │ Arista vEOS VM or cEOS container │ │ fortios │ Fortinet FortiOS firewall │ │ frr │ FRR container │ │ iol │ Cisco IOL │ │ ioll2 │ IOSv L2 image │ │ iosv │ Cisco IOSv │ │ iosvl2 │ IOSv L2 image │ │ iosxr │ Cisco IOS XRv │ │ linux │ Generic Linux host │ │ nxos │ Cisco Nexus 9300v │ │ routeros │ Mikrotik RouterOS version 6 │ │ routeros7 │ Mikrotik RouterOS version 7 │ │ sonic │ Sonic VM │ │ srlinux │ Nokia SR Linux container │ │ sros │ Nokia SR OS container │ │ vjunos-switch │ vJunos Switch │ │ vmx │ Juniper vMX container │ │ vptx │ Juniper vPTX │ │ vsrx │ Juniper vSRX 3.0 │ │ vyos │ VyOS VM/container │ └───────────────┴───────────────────────────────────────────────┘ Networking daemons supported by netlab ┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ daemon ┃ description ┃ ┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩ │ bird │ BIRD Internet Routing Daemon │ │ dnsmasq │ BIRD Internet Routing Daemon │ └─────────┴──────────────────────────────┘So iol for the routers and linux for the PC\u0026rsquo;s and as you may have guessed netlab has a defaults dictionary that is pre-populated for us that we can of course override with custom values. For example, let\u0026rsquo;s see which images will be used if we specify device type as iol and linux by using the netlab show command: (wsl)\u0026lt;\u0026lt;· netlab show images -d iol iol image names by virtualization provider ┏━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┓ ┃ device ┃ clab ┃ libvirt ┃ virtualbox ┃ ┡━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━┩ │ iol │ vrnetlab/cisco_iol:17.12.01 │ │ │ └────────┴─────────────────────────────┴─────────┴────────────┘ (wsl)\u0026lt;\u0026lt;· netlab show images -d linux linux image names by virtualization provider ┏━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓ ┃ device ┃ clab ┃ libvirt ┃ virtualbox ┃ ┡━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩ │ linux │ python:3.9-alpine │ generic/ubuntu2004 │ generic/ubuntu2004 │ └────────┴───────────────────┴────────────────────┴────────────────────┘So these container images will be picked up by default if we specify the respective device type in the topology. Of course, we want to override those since our images are different, and here is how we can do this in the topology file by utilising a separate defaults section prior to defining our nodes and their interconnection links: provider: clab defaults: device: iol devices.iol.clab.image: vrnetlab/cisco_iol:17.12.1 devices.linux.clab.image: ghcr.io/srl-labs/network-mu","date":"2024-11-05","objectID":"/posts/iou-love/:4:1","tags":["netsim","netlab","containerlab","automation"],"title":"For those about to LAB...⚡","uri":"/posts/iou-love/#building-the-base-topology---step-by-step"},{"categories":["Art of Labbing"],"collections":null,"content":"Base Topology - Complete Here is the complete basic topology file with adding some extra quirks: provider: clab defaults: device: iol devices.iol.clab.image: vrnetlab/cisco_iol:17.12.1 devices.linux.clab.image: ghcr.io/srl-labs/network-multitool providers.clab.lab_prefix: \u0026#34;\u0026#34; groups._auto_create: True paths.append.custom.dirs: [ templates ] vrf.as: 65515 groups: core: members: [ rc1, rc2, rc3, rc4 ] module: [ ospf, bgp, mpls, vrf ] bgp.as: 65515 mpls.vpn: [ ibgp ] vrf.loopback: true config: [ core_extras ] ces: members: [ ce1, ce2, ce3, ce4 ] module: [ bgp ] pcs: members: [ pc1, pc2, pc3, pc4, pc11, pc12, pc13, pc14 ] device: linux config: [ iperf3_server ] vars: IPERF3_TESTS: [ { \u0026#34;src\u0026#34;:\u0026#34;pc1\u0026#34;, \u0026#34;dst\u0026#34;: \u0026#34;pc4\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5201\u0026#34; }, { \u0026#34;src\u0026#34;:\u0026#34;pc2\u0026#34;, \u0026#34;dst\u0026#34;: \u0026#34;pc1\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5201\u0026#34; }, { \u0026#34;src\u0026#34;:\u0026#34;pc3\u0026#34;, \u0026#34;dst\u0026#34;: \u0026#34;pc2\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5201\u0026#34; }, { \u0026#34;src\u0026#34;:\u0026#34;pc4\u0026#34;, \u0026#34;dst\u0026#34;: \u0026#34;pc3\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5201\u0026#34; }, { \u0026#34;src\u0026#34;:\u0026#34;pc1\u0026#34;, \u0026#34;dst\u0026#34;: \u0026#34;pc3\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5202\u0026#34; }, { \u0026#34;src\u0026#34;:\u0026#34;pc2\u0026#34;, \u0026#34;dst\u0026#34;: \u0026#34;pc4\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5202\u0026#34; }, { \u0026#34;src\u0026#34;:\u0026#34;pc11\u0026#34;, \u0026#34;dst\u0026#34;: \u0026#34;pc14\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5201\u0026#34; }, { \u0026#34;src\u0026#34;:\u0026#34;pc14\u0026#34;, \u0026#34;dst\u0026#34;: \u0026#34;pc11\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5201\u0026#34; }, { \u0026#34;src\u0026#34;:\u0026#34;pc12\u0026#34;, \u0026#34;dst\u0026#34;: \u0026#34;pc13\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5201\u0026#34; }, { \u0026#34;src\u0026#34;:\u0026#34;pc13\u0026#34;, \u0026#34;dst\u0026#34;: \u0026#34;pc12\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;5201\u0026#34; }, ] vrfs: blue: rd: \u0026#39;65515:101\u0026#39; nodes: rc1: rc2: vrfs: red: rd: \u0026#34;10.0.0.2:102\u0026#34; import: \u0026#39;65515:102\u0026#39; export: \u0026#39;65515:102\u0026#39; rc3: vrfs: red: rd: \u0026#34;10.0.0.3:102\u0026#34; import: \u0026#39;65515:102\u0026#39; export: \u0026#39;65515:102\u0026#39; rc4: ce1: id: 11 bgp.as: 65101 ce2: id: 12 bgp.as: 65201 ce3: id: 13 bgp.as: 65202 ce4: id: 14 bgp.as: 65102 links: - rc1-rc2 - rc1-rc3 - rc2-rc4 - rc3-rc4 - rc1-pc1 - rc2-pc2 - rc3-pc3 - rc4-pc4 - rc1: { vrf: blue } ce1: - rc2: { vrf: red } ce2: - rc3: { vrf: red } ce3: - rc4: { vrf: blue } ce4: - ce1-pc11 - ce2-pc12 - ce3-pc13 - ce4-pc14OK, we have now added some things or two here that we will expand on. First of all, we have used the groups section to group our nodes and apply attributes to them rather than having to replicate them for each one in the nodes section. The groups._auto_create: True allows for not having to define each node under the node section and create them ad-hoc from the members list under the groups, but this does not guarantee the id each node will have. Since the addressing considers those id's and the order of specifying them under the nodes section matters, we can have a mix of those two ways to auto-define nodes and also pin some to specific ids. So rc\u0026rsquo;s get 1 to 4, ce\u0026rsquo;s 11 to 14 and for pc\u0026rsquo;s we don\u0026rsquo;t care. The desired protocols to be enabled on each node is done via the module structure. We have used the defaults.providers.clab.lab_prefix: \u0026quot;\u0026quot; in order for the containers not to include the default clab-\u0026lt;directoty\u0026gt; prefix in their name. We are using jinja templates to provision additional commands to the nodes during start up and we define them in the config section of our groups definition. I have all my templates under a templates folder in the directory structure, so we need a way to let netlab know where to search for them. This is done by the defaults.paths.append.custom.dirs: [ templates ] key. For core devices, we are enabling dns server on them via the core_extras template (wsl)\u0026lt;\u0026lt;· cat templates/core_extras.j2 {# Enable DNS server in both GRT and mgmt vrf #} ! ip dns server ip dns view vrf clab-mgmt ! For the PCs, we want them to run iperf3 as server and also have the ability to start and stop traffic generation after the lab is up. So here, we have defined all the tests in a struc","date":"2024-11-05","objectID":"/posts/iou-love/:4:2","tags":["netsim","netlab","containerlab","automation"],"title":"For those about to LAB...⚡","uri":"/posts/iou-love/#base-topology---complete"},{"categories":["Art of Labbing"],"collections":null,"content":"Service Containers Let\u0026rsquo;s add now our additional containers based on our use case. We are going to be defining them in the topology as we did for our PC's, but here we also need to pass some env variables to them in order for containerlab to bring them up properly. Some of these variables have common values for all but some have specific one\u0026rsquo;s. Hence, we are going to use the hierarchy to define them accordingly. Here is how the relevant groups section looks like in the topology: groups: ksynth: members: [ ksynth1, ksynth2, ksynth3, ksynth4 ] device: linux clab: image: kentik/ksynth-agent:latest kentik: members: [ kproxy, kbgp, ksynth1, ksynth2, ksynth3, ksynth4 ] clab: env: KENTIK_COMPANY: \u0026lt; Kentik Company ID \u0026gt; KENTIK_REGION: EU KENTIK_API_TOKEN: \u0026lt; Kentik User Token \u0026gt; KENTIK_API_EMAIL: \u0026lt; Kentik User Email \u0026gt;Here we have used one group to define the ksynth agents that are based on the relevant image, and also, used the kentik group to define the common variables that all the service containers need in order to communicate with Kentik. Now, let\u0026rsquo;s look at the nodes section: nodes: kproxy: device: linux clab: image: kentik/kproxy:latest cmd: \u0026gt;- -api_email=\u0026lt; Kentik kproxy agent email address \u0026gt; -region=EU -healthcheck=0.0.0.0 -dns internal:192.168.121.101:53 ksynth1: clab.binds: - mounts/ksynth1:/var/lib/ksynth-agent ksynth2: clab.binds: - mounts/ksynth2:/var/lib/ksynth-agent ksynth3: clab.binds: - mounts/ksynth3:/var/lib/ksynth-agent ksynth4: clab.binds: - mounts/ksynth4:/var/lib/ksynth-agent kbgp: device: linux clab: image: kentik/kbgp:latest env: KENTIK_REGION: fra1Here we can observe the following: For kproxy: We define the image and pass along the command line to be run on the container. All other env variables defined in the groups section are going to be available to the container as well. Unfortunately, I could not find a way to dynamically pass variables in the clab.cmd For ksynth: Since we want the agents to persist, we have to preserve the relevant directory across deployments, so we bind mount a local directory path for every instance. The relevant directories have to exist in our directory structure. For kbgp: We specify the image and re-define the region variable to override the one defined in the groups section since only kbgp needs it to be like that. We also create the additional connections in our links section: links: - rc1-ksynth1 - rc2-ksynth2 - rc3-ksynth3 - rc4-ksynth4 - rc1-kbgpNow, for our last agent that will be part of Kentik NMS, we are going to bring this up as a standalone container not controlled via netlab, and here is how the compose file looks like: services: kagent: container_name: kagent hostname: iol_kagent image: kentik/kagent:latest restart: unless-stopped network_mode: host environment: K_COMPANY_ID: \u0026lt; Kentik Company ID \u0026gt; K_API_ROOT: grpc.api.kentik.eu:443 cap_add: - NET_RAW volumes: - kagent-data:/opt/kentik volumes: kagent-data: driver: localTwo points here worth exploring: kagent is not part of the topology since currently there is no way to pass the capability to containerlab The agent is going to poll the devices for SNMP so connectivity wise, this works by default since both agent network (host) and topology management networks are on the same host (WSL) Now, this seems the case also for the kproxy container, but here, netlab is going to need its\u0026rsquo; IP address in order to add additional configuration to the devices exporting flows as we are going to see in the following paragraphs. One last part to cover here is the fact that we have to provision our network devices accordingly in order to be ready to integrate with Kentik. This means that: They need to be sending netflow to kproxy They need to have snmp enabled in order to be polled by kproxy and kagent They need to be peering with kbgp in order to send their BGP tables to Kentik Well, it seems as a usual case of templating configurations and passing them on to devices via netlab. For","date":"2024-11-05","objectID":"/posts/iou-love/:4:3","tags":["netsim","netlab","containerlab","automation"],"title":"For those about to LAB...⚡","uri":"/posts/iou-love/#service-containers"},{"categories":["Art of Labbing"],"collections":null,"content":"Topology Bring-Up Workflow In this section we will cover the steps needed to bring up the LAB and start with our use-case. In a high-level here are the steps needed to accomplish this: Register the NMS agent Register the networking devices in the portal Bring Up the LAB Enable the desired netflow export Enable traffic Register the ksynth agents In addition, here are the additional variables we are using in the topology file along with their description and their requirement for the above steps VARIABLE USED IN DESCRIPTION KENTIK_SNMPV2_COMMUNITY 2,3 SNMPv2 community used to poll the devices KENTIK_SAMPLE_RATE 2,4 Netflow sample rate configured on the devices and in the portal KENTIK_REGION 2,3 Portal Account region - US or EU KENTIK_PLAN_ID 2 Portal flow plan to put the devices in KENTIK_SITE_ID 2 Portal site to put the devices in KENTIK_NMS_AGENT_ID 2 NMS agent to poll the devices KENTIK_NMS_CREDS_NAME 2 Portal credential vault having the snmp community KENTIK_API_TOKEN 2,3 Portal API token to authenticate to Kentik KENTIK_API_EMAIL 2,3 Portal Email account to authenticate to Kentik IPERF3_TESTS 3,5 List of dictionaries describing the iperf3 tests ","date":"2024-11-05","objectID":"/posts/iou-love/:5:0","tags":["netsim","netlab","containerlab","automation"],"title":"For those about to LAB...⚡","uri":"/posts/iou-love/#topology-bring-up-workflow"},{"categories":["Art of Labbing"],"collections":null,"content":"1. Register NMS Agent This an once-off action and it is decoupled from netlab. We just need to run the kagent and register it in the portal. This will allow us to get the AGENT_ID for the next step. So, using the docker compose we bring up the agent and then access the portal to activate it and get the ID. (wsl)\u0026lt;\u0026lt;· docker compose up -d [+] Running 2/2 ✔ Volume \u0026#34;iol_kagent-data\u0026#34; Created 0.0s ✔ Container kagent Started 0.2sIn addition we can assign the agent to an existing site or create a new one in the portal that we will use for our LAB. This will also give us the SITE_ID variable for the next step. ","date":"2024-11-05","objectID":"/posts/iou-love/:5:1","tags":["netsim","netlab","containerlab","automation"],"title":"For those about to LAB...⚡","uri":"/posts/iou-love/#1-register-nms-agent"},{"categories":["Art of Labbing"],"collections":null,"content":"2. Register Devices in the Portal This is again an once-off task and we could do it manually, but since netlab is able to give us an ansible inventory structure out of the topology file we can use this and create the devices via Kentik\u0026rsquo;s device API in our own playbook. Here is a playbook that accomplishes this task: --- - name: \u0026#34;Kentik Device Onboarding\u0026#34; hosts: core gather_facts: false vars: KENTIK_API_URL: \u0026#34;https://grpc.api.kentik.{{ KENTIK_REGION|lower }}\u0026#34; KENTIK_DEVICE_API: \u0026#34;{{ KENTIK_API_URL }}/device/v202308beta1/device/\u0026#34; KENTIK_HEADERS: X-CH-Auth-API-Token: \u0026#34;{{ KENTIK_API_TOKEN }}\u0026#34; X-CH-Auth-Email: \u0026#34;{{ KENTIK_API_EMAIL }}\u0026#34; Content-Type: application/json tasks: - name: Get Current Devices uri: url: \u0026#34;{{ KENTIK_DEVICE_API }}\u0026#34; method: GET headers: \u0026#34;{{ KENTIK_HEADERS }}\u0026#34; status_code: 200 register: devices delegate_to: localhost run_once: true - name: Skipped Devices debug: msg: \u0026#34;Device name or IP addresses already existed in portal\u0026#34; when: - \u0026#34;netlab_name+\u0026#39;-\u0026#39;+hostname in devices.json | json_query(\u0026#39;devices[].deviceName\u0026#39;) or mgmt.ipv4 in devices.json | json_query(\u0026#39;devices[].sendingIps[]\u0026#39;) or bgp.router_id in devices.json | json_query(\u0026#39;devices[].deviceBgpNeighborIp\u0026#39;)\u0026#34; delegate_to: localhost - name: Create Device OR Skip If Exists block: - name: Create Device Call ansible.builtin.uri: url: \u0026#34;{{ KENTIK_DEVICE_API }}\u0026#34; method: POST headers: \u0026#34;{{ KENTIK_HEADERS }}\u0026#34; status_code: 200 body: \u0026#34;{{ lookup(\u0026#39;ansible.builtin.template\u0026#39;,\u0026#39;templates/kentik_device.j2\u0026#39;) }}\u0026#34; body_format: json timeout: 60 register: device_created throttle: 1 - name: Devices Created debug: msg: \u0026#34;Device created with id: {{ device_created.json.device.id }}\u0026#34; rescue: - name: Cannot Create Device debug: msg: \u0026#34;Failed to create device: {{ device_created.json.message }}\u0026#34; when: - \u0026#34;netlab_name+\u0026#39;-\u0026#39;+hostname not in devices.json | json_query(\u0026#39;devices[].deviceName\u0026#39;)\u0026#34; - \u0026#34;mgmt.ipv4 not in devices.json | json_query(\u0026#39;devices[].sendingIps[]\u0026#39;)\u0026#34; - \u0026#34;bgp.router_id not in devices.json | json_query(\u0026#39;devices[].deviceBgpNeighborIp\u0026#39;)\u0026#34; delegate_to: localhost We are using the built in uri module to interact with the API and create the devices we are interested in, i.e. the core devices. The playbook will check if there is any existing device with the same name or having any relevant IP that we use in our LAB, and if not it will create the device in the portal. The json payload is templated and looks like this: So we are going to need to provide the remaining values for the variables used in our topology file and once all variables are filled in, we can execute the netlab create command in order for the ansible inventory to be produced. (wsl)\u0026lt;\u0026lt;· netlab create [CREATED] provider configuration file: clab.yml [MAPPED] clab_files/kproxy/hosts to kproxy:/etc/hosts (from templates/provider/clab/linux/hosts.j2) [MAPPED] clab_files/ksynth1/hosts to ksynth1:/etc/hosts (from templates/provider/clab/linux/hosts.j2) [MAPPED] clab_files/ksynth2/hosts to ksynth2:/etc/hosts (from templates/provider/clab/linux/hosts.j2) [MAPPED] clab_files/ksynth3/hosts to ksynth3:/etc/hosts (from templates/provider/clab/linux/hosts.j2) [MAPPED] clab_files/ksynth4/hosts to ksynth4:/etc/hosts (from templates/provider/clab/linux/hosts.j2) [MAPPED] clab_files/kbgp/hosts to kbgp:/etc/hosts (from templates/provider/clab/linux/hosts.j2) [MAPPED] clab_files/pc1/hosts to pc1:/etc/hosts (from templates/provider/clab/linux/hosts.j2) [MAPPED] clab_files/pc2/hosts to pc2:/etc/hosts (from templates/provider/clab/linux/hosts.j2) [MAPPED] clab_files/pc3/hosts to pc3:/etc/hosts (from templates/provider/clab/linux/hosts.j2) [MAPPED] clab_files/pc4/hosts to pc4:/etc/hosts (from templates/provider/clab/linux/hosts.j2) [MAPPED] clab_files/pc11/hosts to pc11:/etc/hosts (from templates/provider/clab/linux/hosts.j2) [MAPPED] clab_file","date":"2024-11-05","objectID":"/posts/iou-love/:5:2","tags":["netsim","netlab","containerlab","automation"],"title":"For those about to LAB...⚡","uri":"/posts/iou-love/#2-register-devices-in-the-portal"},{"categories":["Art of Labbing"],"collections":null,"content":"3. Bring Up the LAB We are ready now to bring up the topology. We have seen that during netlab create netlab will create the required containerlab topology file and populate our ansible inventory with the appropriate files and values. Using the netlab up command it will execute the create phase as well as spin up the nodes and start provisioning them with the appropriate configurations we defined in the topology file. (wsl)\u0026lt;\u0026lt;· ANSIBLE_NOCOWS=1 netlab up \u0026lt; snipped \u0026gt; PLAY RECAP **************************************************************************************************************** ce1 : ok=25 changed=4 unreachable=0 failed=0 skipped=14 rescued=0 ignored=0 ce2 : ok=24 changed=3 unreachable=0 failed=0 skipped=14 rescued=0 ignored=0 ce3 : ok=24 changed=3 unreachable=0 failed=0 skipped=14 rescued=0 ignored=0 ce4 : ok=24 changed=3 unreachable=0 failed=0 skipped=14 rescued=0 ignored=0 kbgp : ok=11 changed=3 unreachable=0 failed=0 skipped=2 rescued=0 ignored=0 kproxy : ok=11 changed=3 unreachable=0 failed=0 skipped=2 rescued=0 ignored=0 ksynth1 : ok=11 changed=3 unreachable=0 failed=0 skipped=2 rescued=0 ignored=0 ksynth2 : ok=11 changed=3 unreachable=0 failed=0 skipped=2 rescued=0 ignored=0 ksynth3 : ok=11 changed=3 unreachable=0 failed=0 skipped=2 rescued=0 ignored=0 ksynth4 : ok=11 changed=3 unreachable=0 failed=0 skipped=2 rescued=0 ignored=0 pc1 : ok=19 changed=5 unreachable=0 failed=0 skipped=5 rescued=0 ignored=0 pc11 : ok=19 changed=5 unreachable=0 failed=0 skipped=5 rescued=0 ignored=0 pc12 : ok=19 changed=5 unreachable=0 failed=0 skipped=5 rescued=0 ignored=0 pc13 : ok=19 changed=5 unreachable=0 failed=0 skipped=5 rescued=0 ignored=0 pc14 : ok=19 changed=5 unreachable=0 failed=0 skipped=5 rescued=0 ignored=0 pc2 : ok=19 changed=5 unreachable=0 failed=0 skipped=5 rescued=0 ignored=0 pc3 : ok=19 changed=5 unreachable=0 failed=0 skipped=5 rescued=0 ignored=0 pc4 : ok=19 changed=5 unreachable=0 failed=0 skipped=5 rescued=0 ignored=0 rc1 : ok=48 changed=8 unreachable=0 failed=0 skipped=11 rescued=0 ignored=0 rc2 : ok=48 changed=8 unreachable=0 failed=0 skipped=11 rescued=0 ignored=0 rc3 : ok=48 changed=8 unreachable=0 failed=0 skipped=11 rescued=0 ignored=0 rc4 : ok=48 changed=8 unreachable=0 failed=0 skipped=11 rescued=0 ignored=0 [SUCCESS] Lab devices configured (wsl)\u0026lt;\u0026lt;· netlab status Lab default in /home/netlab/iol status: started provider(s): clab ┏━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓ ┃ node ┃ device ┃ image ┃ mgmt IPv4 ┃ connection ┃ provider ┃ VM/container ┃ status ┃ ┡━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩ │ ce1 │ iol │ vrnetlab/cisco_iol:17.12.1 │ 192.168.121.111 │ network_cli │ clab │ ce1 │ Up 3 minutes │ ├─────────┼────────┼─────────────────────────────┼─────────────────┼─────────────┼──────────┼──────────────┼──────────────┤ │ ce2 │ iol │ vrnetlab/cisco_iol:17.12.1 │ 192.168.121.112 │ network_cli │ clab │ ce2 │ Up 3 minutes │ ├─────────┼────────┼─────────────────────────────┼─────────────────┼─────────────┼──────────┼──────────────┼──────────────┤ │ ce3 │ iol │ vrnetlab/cisco_iol:17.12.1 │ 192.168.121.113 │ network_cli │ clab │ ce3 │ Up 3 minutes │ ├─────────┼────────┼─────────────────────────────┼─────────────────┼─────────────┼──────────┼──────────────┼──────────────┤ │ ce4 │ iol │ vrnetlab/cisco_iol:17.12.1 │ 192.168.121.114 │ network_cli │ clab │ ce4 │ Up 3 minutes │ ├─────────┼────────┼─────────────────────────────┼─────────────────┼─────────────┼──────────┼──────────────┼──────────────┤ │ kbgp │ linux │ ghcr.io/srl-labs/network-m… │ 192.168.121.110 │ docker │ clab │ kbgp │ Up 3 minutes │ ├─────────┼────────┼─────────────────────────────┼─────────────────┼─────────────┼──────────┼──────────────┼──────────────┤ │ kproxy │ linux │ ghcr.io/srl-labs/network-m… │ 192.168.121.105 │ docker │ clab │ kproxy │ Up 3 minutes │ ├───────","date":"2024-11-05","objectID":"/posts/iou-love/:5:3","tags":["netsim","netlab","containerlab","automation"],"title":"For those about to LAB...⚡","uri":"/posts/iou-love/#3-bring-up-the-lab"},{"categories":["Art of Labbing"],"collections":null,"content":"4. Enable Netflow So now that we have our test network ready, we can enable netflow on the core routers by choosing our netflow flavour and provision the respective commands via the netlab config command: (wsl)\u0026lt;\u0026lt;· ANSIBLE_NOCOWS=1 netlab config kentik_flow -l core -e ACTION=push \u0026lt; snipped \u0026gt; PLAY RECAP **************************************************************************************************************** rc1 : ok=7 changed=1 unreachable=0 failed=0 skipped=4 rescued=0 ignored=0 rc2 : ok=7 changed=1 unreachable=0 failed=0 skipped=3 rescued=0 ignored=0 rc3 : ok=7 changed=1 unreachable=0 failed=0 skipped=3 rescued=0 ignored=0 rc4 : ok=7 changed=1 unreachable=0 failed=0 skipped=3 rescued=0 ignored=0We can use this method to re-configure any other flow configuration ad-hoc by leveraging the ACTION variable. Let\u0026rsquo;s check on the kproxy after a while to see that all devices are sending flows and are correctly registered with the portal: (wsl)\u0026lt;\u0026lt;· telnet kproxy 9996 Trying 192.168.121.105... Connected to kproxy. Escape character is \u0026#39;^]\u0026#39;. GOOD 4 Connected Devices * 29118:iol-rc3:115541 -\u0026gt; 0.0.0.0:40016 (In1: 0.116060, Out1: 0.108333, In15: 0.191401, Out15: 0.009772). Last seen 2024-12-07T18:17:32.509264 (8.195s ago). Sources: 192.168.121.103. Channel highwater: 0, 0, 0. Flow: netflow.v9 * 29118:iol-rc1:115531 -\u0026gt; 0.0.0.0:40010 (In1: 0.119930, Out1: 0.081798, In15: 0.191431, Out15: 0.008598). Last seen 2024-12-07T18:17:35.463030 (7.186s ago). Sources: 192.168.121.101. Channel highwater: 0, 0, 0. Flow: netflow.v9 * 29118:iol-rc2:115536 -\u0026gt; 0.0.0.0:40012 (In1: 0.094682, Out1: 0.027546, In15: 0.189246, Out15: 0.003203). Last seen 2024-12-07T18:17:28.505189 (12.199s ago). Sources: 192.168.121.102. Channel highwater: 0, 0, 0. Flow: netflow.v9 * 29118:iol-rc4:115546 -\u0026gt; 0.0.0.0:40014 (In1: 0.102966, Out1: 0.108333, In15: 0.190311, Out15: 0.009772). Last seen 2024-12-07T18:17:19.681646 (21.022s ago). Sources: 192.168.121.104. Channel highwater: 0, 0, 0. Flow: netflow.v9 0 Unregistered Devices Connection closed by foreign host.","date":"2024-11-05","objectID":"/posts/iou-love/:5:4","tags":["netsim","netlab","containerlab","automation"],"title":"For those about to LAB...⚡","uri":"/posts/iou-love/#4-enable-netflow"},{"categories":["Art of Labbing"],"collections":null,"content":"5. Enable Traffic Our PC's are already listening to the appropriate ports for the traffic tests, so we can run the clients using the respective template: (wsl)·\u0026gt;\u0026gt; ANSIBLE_NOCOWS=1 netlab config iperf3_clients -l pcs -e ACTION=run \u0026lt; snipped \u0026gt; PLAY RECAP **************************************************************************************************************** pc1 : ok=9 changed=3 unreachable=0 failed=0 skipped=4 rescued=0 ignored=0 pc11 : ok=9 changed=3 unreachable=0 failed=0 skipped=3 rescued=0 ignored=0 pc12 : ok=9 changed=3 unreachable=0 failed=0 skipped=3 rescued=0 ignored=0 pc13 : ok=9 changed=3 unreachable=0 failed=0 skipped=3 rescued=0 ignored=0 pc14 : ok=9 changed=3 unreachable=0 failed=0 skipped=3 rescued=0 ignored=0 pc2 : ok=9 changed=3 unreachable=0 failed=0 skipped=3 rescued=0 ignored=0 pc3 : ok=9 changed=3 unreachable=0 failed=0 skipped=3 rescued=0 ignored=0 pc4 : ok=9 changed=3 unreachable=0 failed=0 skipped=3 rescued=0 ignored=0The same applies to this step as well. We can stop, start and reconfigure the test parameters in our template at any point using the appropriate ACTION value. Here are some outputs from the devices: (wsl)\u0026lt;\u0026lt;· netlab exec rc1 sh int e0/3 \\| i rate Connecting to rc1 using SSH port 22, executing sh int e0/3 | i rate Queueing strategy: fifo 5 minute input rate 32387000 bits/sec, 3279 packets/sec 5 minute output rate 16295000 bits/sec, 2494 packets/sec Connection to rc1 closed by remote host. (wsl)·\u0026gt;\u0026gt; netlab exec ce2 sh int e0/1 \\| i rate Connecting to ce2 using SSH port 22, executing sh int e0/1 | i rate Queueing strategy: fifo 5 minute input rate 21226000 bits/sec, 2532 packets/sec 5 minute output rate 22546000 bits/sec, 2651 packets/sec Connection to ce2 closed by remote host.And here is how it looks in Kentik Top Device by Average bits/s ","date":"2024-11-05","objectID":"/posts/iou-love/:5:5","tags":["netsim","netlab","containerlab","automation"],"title":"For those about to LAB...⚡","uri":"/posts/iou-love/#5-enable-traffic"},{"categories":["Art of Labbing"],"collections":null,"content":"6. Register Synthetics Agents Now for the last step, the ksynth agents, on their first run they will initialise and appear in the portal as Pending until they are activated. We need to activate them in the portal, assign them to the site we are using in the lab, choose the address families enabled, and also provide their private IP address they got from netlab. We can get the IP address with the netlab inspect command: (wsl)·\u0026gt;\u0026gt; for i in 1 2 3 4 ; do echo -n \u0026#34;ksynth$i \u0026#34;;\\ netlab inspect --node ksynth$i interfaces[0].ipv4 ; done ksynth1 172.16.4.6/24 ksynth2 172.16.5.7/24 ksynth3 172.16.6.8/24 ksynth4 172.16.7.9/24From this point onwards we can configure our tests in the portal. ","date":"2024-11-05","objectID":"/posts/iou-love/:5:6","tags":["netsim","netlab","containerlab","automation"],"title":"For those about to LAB...⚡","uri":"/posts/iou-love/#6-register-synthetics-agents"},{"categories":["Art of Labbing"],"collections":null,"content":"Outro Well I hope you reached the end of this long post and that by now you have realised how simple it is to bring up a topology in containerlab using netlab to take care of the initial configuration of the underneath connectivity details and protocols running on it. The topology we used here is very small and simple, indeed, but I think that what netlab has to offer does worth the effort of spending some time in getting to know it better and using it in your use cases or even pipelines. Let\u0026rsquo;s summarise here the key takeaways of this post: We have used netlab to bring up a simple network topology via containerlab and provision the initial configs on the devices based on the supported modules and features We have seen how to integrate additional containers interacting with this topology for serving our use case We have seen how to use netlabs\u0026rsquo; ansible inventory to run our own plays extending the usability of the lab We have seen how to create additional configurations using templates and provision them ad-hoc to our nodes Some of my thoughts on more back-blogs coming out of this could be: Building IOL containers out of the old images using i386 libraries and also licensing Using netlab tools for the service containers Using netlab plugins to extend the current functionality or even add a new one Handling of sensitive data, maybe ansible vault Use a wrapper on top to orchestrate the use case, like python invoke Use netlab validation tests Use WEmulate to artificially impair links for my Kentik use case, like this. ...till next time...have fun!!! ","date":"2024-11-05","objectID":"/posts/iou-love/:6:0","tags":["netsim","netlab","containerlab","automation"],"title":"For those about to LAB...⚡","uri":"/posts/iou-love/#outro"},{"categories":["Art of Labbing"],"collections":null,"content":"Influences and Reference netlab containerlab vrnetlab for containerlab srl-telemetry-lab Cisco IOL in CML My repo for this post ","date":"2024-11-05","objectID":"/posts/iou-love/:7:0","tags":["netsim","netlab","containerlab","automation"],"title":"For those about to LAB...⚡","uri":"/posts/iou-love/#influences-and-reference"},{"categories":["network monitoring"],"collections":null,"content":"Intro In this post we will go through a detailed how to on executing Ansible playbooks triggered by a webhook notification event raised in Kentik Portal. The use case is to have visibility into BGP Flowspec metrics of the devices during mitigations and see the relevant counters in the Portal under Kentik\u0026rsquo;s NMS Metrics Explorer. You may find all relevant files in my repo. ","date":"2024-06-01","objectID":"/posts/modern-nms/:1:0","tags":["network monitoring","telemetry","ansible","automation"],"title":"If this ain't a modern NMS, then what is?","uri":"/posts/modern-nms/#intro"},{"categories":["network monitoring"],"collections":null,"content":"The Intent For the webhook receiver part, we are going to use Kentik\u0026rsquo;s ansible_eda collection that will listen for a mitigation notification event. Once the event is received and a mitigation is started, EDA will trigger the execution of a playbook to handle the event and spin-up Telegraf in order to start polling the devices for BGP Flowspec streaming telemetry counters while reporting them back to the Portal in influx line format via HTTPS. Once the mitigation is over and EDA receives the respective cease notification, a check for any active mitigations on the devices is done via Kentik\u0026rsquo;s API and if there are no active mitigations on the devices, EDA will trigger Telegraf to stop. If there are still on-going mitigations, Telegraf will continue to poll and ship metrics till we receive the last mitigation\u0026rsquo;s notification. ","date":"2024-06-01","objectID":"/posts/modern-nms/:2:0","tags":["network monitoring","telemetry","ansible","automation"],"title":"If this ain't a modern NMS, then what is?","uri":"/posts/modern-nms/#the-intent"},{"categories":["network monitoring"],"collections":null,"content":"Lab Setup Here is how the lab has been set-up for this: Two Cisco IOS XRv 9000 devices defined in the portal with established BGP sessions and Flowspec family enabled. GRPC is configured and enabled on the devices. A mitigation Platform defined in the Portal that includes both devices, and two mitigation methods attached. One will block ICMP echo-requests and the other will Rate Limit SSH protocol A notification channel of type JSON defined in the portal and attached to both mitigation methods Note I chose not to use any Alerting Policies with this and trigger all mitigations manually in order to speed things up An Ubuntu VM (jammy) with docker installed EDA will run inside a docker container with Traefik proxying the webhook events Once an actionable event is received, EDA will control a Telegraf docker container accordingly through a playbook ","date":"2024-06-01","objectID":"/posts/modern-nms/:3:0","tags":["network monitoring","telemetry","ansible","automation"],"title":"If this ain't a modern NMS, then what is?","uri":"/posts/modern-nms/#lab-setup"},{"categories":["network monitoring"],"collections":null,"content":"Discovery We will start by exploring what we see on the devices during a running flowspec mitigation. We have triggered two manual mitigations in the Portal and those are currently active on the devices: RP/0/RP0/CPU0:ATH-POP1-XRV1#show flowspec afi-all detail AFI: IPv4 Flow :Dest:10.10.10.11/32,ICMPType:=8 Actions :Traffic-rate: 0 bps (bgp.1) Flow :Source:10.10.10.12/32,Proto:=6,DPort:=22 Actions :Traffic-rate: 160000 bps (bgp.1) XRv9k limitations Unfortunatelly the XRv has a limited data plane, so we do not get any action counters on the mitigations - this and also no netflow out of this virtual device \u0026#x1f622;. In case of a real XR device the output would be similar to this: REAL-XR#show flowspec afi-all detail AFI: IPv4 Flow :Dest:10.10.10.11/32,ICMPType:=8 Actions :Traffic-rate: 0 bps (bgp.1) Statisctics (packets/bytes) Matched : 10/640 Dropped : 10/640 So we have two mitigations as expected. Let\u0026rsquo;s find out which YANG model those are defined under. After trying some show commands for the xpath, there it is: RP/0/RP0/CPU0:ATH-POP1-XRV1#schema-describe \u0026#34;show flowspec afi-all detail\u0026#34; Action: get Path: RootCfg.ASNFormat Action: get_children Path: RootOper.FlowSpec.VRF({\u0026#39;VRFName\u0026#39;: \u0026#39;default\u0026#39;}).AF Action: get Path: RootOper.FlowSpec.VRF({\u0026#39;VRFName\u0026#39;: \u0026#39;default\u0026#39;}).AF({\u0026#39;AFName\u0026#39;: \u0026#39;IPv4\u0026#39;}).Flow RP/0/RP0/CPU0:ATH-POP1-XRV1#show telemetry internal xpath \u0026#34;show flowspec afi-all detail\u0026#34; Error: Invalid input RP/0/RP0/CPU0:ATH-POP1-XRV1#show telemetry internal xpath \u0026#34;show flowspec afi-all\u0026#34; Error: Invalid input RP/0/RP0/CPU0:ATH-POP1-XRV1#show telemetry internal xpath \u0026#34;show flowspec summary\u0026#34; Cisco-IOS-XR-flowspec-oper:flow-spec/summary RP/0/RP0/CPU0:ATH-POP1-XRV1#show telemetry internal json Cisco-IOS-XR-flowspec-oper:flow-spec | include path \u0026#34;encoding_path\u0026#34;: \u0026#34;Cisco-IOS-XR-flowspec-oper:flow-spec/vrfs/vrf/afs/af/flows/flow\u0026#34;, \u0026#34;encoding_path\u0026#34;: \u0026#34;Cisco-IOS-XR-flowspec-oper:flow-spec/vrfs/vrf/afs/af/nlris/nlri\u0026#34;, \u0026#34;encoding_path\u0026#34;: \u0026#34;Cisco-IOS-XR-flowspec-oper:flow-spec/vrfs/vrf/afs/af/table-summary\u0026#34;, \u0026#34;encoding_path\u0026#34;: \u0026#34;Cisco-IOS-XR-flowspec-oper:flow-spec/summary\u0026#34;, \u0026#34;encoding_path\u0026#34;: \u0026#34;Cisco-IOS-XR-flowspec-oper:flow-spec/clients/client\u0026#34;,Now we know the YANG model and the xpath, and we can get the metrics out of. We will test it with gnmic to see if it works and get the details of the tags/paths returned: ","date":"2024-06-01","objectID":"/posts/modern-nms/:4:0","tags":["network monitoring","telemetry","ansible","automation"],"title":"If this ain't a modern NMS, then what is?","uri":"/posts/modern-nms/#discovery"},{"categories":["network monitoring"],"collections":null,"content":"Kentik EDA We are going to start off by explaining the file and folder structure giving a bit of info of how this works. The idea behind this is to have EDA running as a docker container. Traefik will handle reverse proxying the webhooks to EDA and in case a new mitigation is happening, then a telegraf docker container will spin up and start collecting gnmi metrics from the devices and pushing them to the Portal. Once the mitigation is over, EDA will receive the event to stop the mitigation. Now, at this point, since more than one mitigations can be ongoing on the devices, we are going to focus on the specific mitigation Platform that will always include our devices. So any Method under the specific mitigation Platform will cause the start of the metrics collection. Upon receiving a stop event on this Platform, we just have to make sure that this event is the last one on the devices, and we can verify this via Kentik API requesting all active alerts in the portal of type Mitigation. If none exists, then we can assume that the event EDA received was the last one for the Platform. In summary, here are the features delivered: Traefik is handling the proxying of webhooks to the appropriate path that Kentik EDA is set up to receive Kentik EDA processes only the relevant mitigation events coming in excluding the rest The playbook will dynamically produce the telegraf configuration file according to our defined variables in order to ship the metrics in the Portal ","date":"2024-06-01","objectID":"/posts/modern-nms/:5:0","tags":["network monitoring","telemetry","ansible","automation"],"title":"If this ain't a modern NMS, then what is?","uri":"/posts/modern-nms/#kentik-eda"},{"categories":["network monitoring"],"collections":null,"content":"Directory Structure . ├── docker-compose.yaml \u0026gt;-- How to bring up our services ├── Dockerfile \u0026gt;-- How to build the EDA image ├── .env \u0026gt;-- Hidden file containing sensitive data to be passed as environment variables ├── .env.sample \u0026gt;-- Example env file └── eda \u0026gt;-- EDA config folder to be mounted on the container ├── ansible.cfg \u0026gt;-- Ansible configuration file picked up by default ├── ansible-inventory.yml \u0026gt;-- Simple inv file including only localhost ├── eda_vars.yml \u0026gt;-- Definition of the related variables used in the project ├── mitigation.yml \u0026gt;-- Playbook to handle events received ├── rules.yml \u0026gt;-- Rulebook defining our EDA logic └── telegraf ├── telegraf.conf.j2 \u0026gt;-- Jinja2 template producing the configuration dynamically └── telegraf.conf \u0026gt;-- Actual config to be used with the telegraf container ","date":"2024-06-01","objectID":"/posts/modern-nms/:5:1","tags":["network monitoring","telemetry","ansible","automation"],"title":"If this ain't a modern NMS, then what is?","uri":"/posts/modern-nms/#directory-structure"},{"categories":["network monitoring"],"collections":null,"content":"Dockerfile We are installing the dependencies and ansible needed collections for Kentik EDA to run. We are going to run everything under the /app directory inside the container FROM quay.io/centos/centos:stream9-development RUN dnf install -y java-17-openjdk-devel python3-pip gcc python3-devel postgresql-devel ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk RUN pip install -U pip \\ \u0026amp;\u0026amp; pip install ansible-core \\ ansible-rulebook \\ ansible-runner \\ psycopg requests \\ \u0026amp;\u0026amp; ansible-galaxy collection install kentik.ansible_eda community.docker ARG APP_DIR=${APP_DIR:-/app} WORKDIR $APP_DIR RUN chmod -R 0775 $APP_DIR","date":"2024-06-01","objectID":"/posts/modern-nms/:5:2","tags":["network monitoring","telemetry","ansible","automation"],"title":"If this ain't a modern NMS, then what is?","uri":"/posts/modern-nms/#dockerfile"},{"categories":["network monitoring"],"collections":null,"content":"Compose Our compose file will bring up two services. traefik: that will listen for HTTP and will forward everything received at the /eda path to the EDA container replacing the path to /alert, since this is the path that Kentik EDA webhook listens on, and port 8080. kentik-eda: Our eda folder is mounted at the /app path inside the container, and contains all the files needed to run our logic. We also mount the docker unix socket to the container so that we can control the containers from within the EDA one. We also pass our variables to the container environment and we instruct the container to start the rulebook passing also the variables files. Avoid using this in production!!! Security wise it is not the best to have a docker container controlling the host\u0026rsquo;s containers by exposing the unix socket to it. \u0026#x1f604; --- services: traefik: image: traefik container_name: traefik command: \u0026gt; --api.insecure=true --providers.docker --providers.docker.exposedbydefault=false --accesslog=true --entrypoints.eda.address=:80 #--log.level=DEBUG ports: - 80:80 - 8080:8080 volumes: - /var/run/docker.sock:/var/run/docker.sock:ro kentik-eda: #scale: 2 image: kentik-eda:0.1 container_name: kentik-eda depends_on: - traefik build: context: . volumes: - ${PWD}/eda:/app - /var/run/docker.sock:/var/run/docker.sock:ro env_file: - .env ports: - 8080 command: bash -c \u0026#34;ansible-rulebook --rulebook rules.yml -i ./ansible-inventory.yml --vars eda_vars.yml\u0026#34; labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.eda.rule=Host(`kentik.eda`) \u0026amp;\u0026amp; Path(`/eda`)\u0026#34; - \u0026#34;traefik.http.routers.eda.middlewares=eda-ratelimit,eda-path\u0026#34; - \u0026#34;traefik.http.middlewares.eda-path.replacepath.path=/alert\u0026#34; - \u0026#34;traefik.http.middlewares.eda-ratelimit.ratelimit.average=10\u0026#34; - \u0026#34;traefik.http.middlewares.eda-ratelimit.ratelimit.burst=50\u0026#34; - \u0026#34;traefik.http.routers.eda.entrypoints=eda\u0026#34;Here is the example .env file showing the variables needed to be exposed inside the EDA container environment: KENTIK_API_TOKEN=\u0026lt;your api token\u0026gt; KENTIK_API_EMAIL=\u0026lt;your portal email\u0026gt; KENTIK_API_ENDPOINT=\u0026#34;https://grpc.api.kentik.eu/kmetrics/v202207/metrics/api/v2/write?bucket=\u0026amp;org=\u0026amp;precision=ns\u0026#34; GRPC_USERNAME=\u0026lt;device username\u0026gt; GRPC_PASSWORD=\u0026lt;device password\u0026gt;We can bring everything up with docker compose up --build ❯ docker compose up --build --force-recreate --dry-run [+] Building 0.0s (0/0) docker:default [+] Running 5/0 ✔ DRY-RUN MODE - build service kentik-eda 0.0s ✔ DRY-RUN MODE - ==\u0026gt; ==\u0026gt; writing image dryRun-648b28d83dd200d4e7709ac3a5d522f8244467bd 0.0s ✔ DRY-RUN MODE - ==\u0026gt; ==\u0026gt; naming to kentik-eda:0.1 0.0s ✔ DRY-RUN MODE - Container traefik Recreated 0.0s ✔ DRY-RUN MODE - Container kentik-eda Recreated 0.0s end of \u0026#39;compose up\u0026#39; output, interactive run is not supported in dry-run mode","date":"2024-06-01","objectID":"/posts/modern-nms/:5:3","tags":["network monitoring","telemetry","ansible","automation"],"title":"If this ain't a modern NMS, then what is?","uri":"/posts/modern-nms/#compose"},{"categories":["network monitoring"],"collections":null,"content":"Rulebook rules Here is how we have configured our rules: --- - name: Listening for Webhook Events hosts: localhost sources: - kentik.ansible_eda.kentik_webhook: host: 0.0.0.0 port: 8080 rules: - name: R1 - New Event Received # If it is a valid start or stop mitigation event condition: event.payload is defined and event.payload.CompanyID == vars.CompanyID and event.payload.EventType == vars.EventType and event.payload.MitigationPlatformID == vars.MitigationPlatformID and event.payload.MitigationState in vars.ValidMitigationStates and event.payload.MitigationStateNew in vars.ValidMitigationStates actions: - debug: msg: | New {{event.payload.MitigationType}}/{{event.payload.MitigationState}} event received ID: {{event.payload.MitigationID}} Platform: {{event.payload.MitigationPlatformName}} Method: {{event.payload.MitigationMethodName}} IP: {{event.payload.MitigationAlertIP}} # DEBUG:Dump the event #- print_event: # pretty: true # Call the pb to handle the mitigation event - run_playbook: name: mitigation.yml # Catch and ignore the rest - name: R2 - Not taking action condition: event.meta is defined action: debug: msg: - \u0026#34;Ignoring {{event.payload.EventType}} event\u0026#34;eda/rules.yml and our variables: --- CompanyID: \u0026lt;REDACTED\u0026gt; EventType: \u0026#34;mitigation\u0026#34; MitigationPlatformID: \u0026#34;257\u0026#34; ValidMitigationStates: - \u0026#34;manualMitigating\u0026#34; - \u0026#34;mitigating\u0026#34; - \u0026#34;archived\u0026#34; Mitigation: devices: - name: \u0026#34;ath-pop1-xrv1\u0026#34; ip: \u0026#34;10.12.255.1\u0026#34; port: 57344 - name: \u0026#34;ath-pop1-xrv2\u0026#34; ip: \u0026#34;10.13.255.1\u0026#34; port: 57344eda/eda_vars.yml As a condition we check the event payload against certain values in order to process it further, e.g. the CompanyID , the EventType and the MitigationPlatformID must match to what we have configured and also the EventState must be one of the actionable states that we have defined in our dictionary. If we have a matching event then we pass control to the mitigation.yml playbook to handle the event further. ","date":"2024-06-01","objectID":"/posts/modern-nms/:5:4","tags":["network monitoring","telemetry","ansible","automation"],"title":"If this ain't a modern NMS, then what is?","uri":"/posts/modern-nms/#rulebook-rules"},{"categories":["network monitoring"],"collections":null,"content":"Ansible Playbook We could have had separate playbooks for the different states of the alerts received, i.e. separate conditions in the rules file, but we chose here to proceed with just one playbook to handle both cases. A brief pseudocode of the playbook could be: #IF event state is \u0026#39;mitigating\u0026#39; #IF Telegraf is not running - Generate telegraf config from template - Bring up the container #ELSE IF event state is \u0026#39;archived\u0026#39; #IF no active mitigation alerts in portal for PlatormID - Stop telegraf container Here is a sequence diagram showing the workflows sequenceDiagram autonumber participant Portal participant EDA Portal-\u003e\u003eEDA: Alerts loop Listen EDA-\u003e\u003eEDA: Check Platform and State (Start/Stop) create participant Playbook EDA-\u003e\u003ePlaybook: Process Valid Alert end alt Mitigation Started Playbook-\u003e\u003eDocker: Is Telegraf Running? Docker--\u003e\u003ePlaybook: State alt Telegraf Running Playbook-\u003e\u003ePlaybook: Do nothing else Telegraf Down Playbook-\u003e\u003ePlaybook: Create Telegraf config Playbook-\u003e\u003eDocker: Start Telegraf create participant Telegraf Docker-\u003e\u003eTelegraf: Run Container loop create participant Devices Telegraf-\u003e\u003eDevices: Fetch metrics Telegraf-\u003e\u003ePortal: Ship metrics end end else Mitigation Finished Playbook-\u003e\u003ePortal: Get active mitigations Portal--\u003e\u003ePlaybook: Active Mitigations alt Active Mitigations on Platform Playbook-\u003e\u003ePlaybook: Do nothing else No Active Mitigations on Platform Playbook-\u003e\u003eDocker: Stop Telegraf Docker-\u003e\u003eTelegraf: Container Down destroy Telegraf end end sequenceDiagram autonumber participant Portal participant EDA Portal-\u003e\u003eEDA: Alerts loop Listen EDA-\u003e\u003eEDA: Check Platform and State (Start/Stop) create participant Playbook EDA-\u003e\u003ePlaybook: Process Valid Alert end alt Mitigation Started Playbook-\u003e\u003eDocker: Is Telegraf Running? Docker--\u003e\u003ePlaybook: State alt Telegraf Running Playbook-\u003e\u003ePlaybook: Do nothing else Telegraf Down Playbook-\u003e\u003ePlaybook: Create Telegraf config Playbook-\u003e\u003eDocker: Start Telegraf create participant Telegraf Docker-\u003e\u003eTelegraf: Run Container loop create participant Devices Telegraf-\u003e\u003eDevices: Fetch metrics Telegraf-\u003e\u003ePortal: Ship metrics end end else Mitigation Finished Playbook-\u003e\u003ePortal: Get active mitigations Portal--\u003e\u003ePlaybook: Active Mitigations alt Active Mitigations on Platform Playbook-\u003e\u003ePlaybook: Do nothing else No Active Mitigations on Platform Playbook-\u003e\u003eDocker: Stop Telegraf Docker-\u003e\u003eTelegraf: Container Down destroy Telegraf end end sequenceDiagram autonumber participant Portal participant EDA Portal-\u003e\u003eEDA: Alerts loop Listen EDA-\u003e\u003eEDA: Check Platform and State (Start/Stop) create participant Playbook EDA-\u003e\u003ePlaybook: Process Valid Alert end alt Mitigation Started Playbook-\u003e\u003eDocker: Is Telegraf Running? Docker--\u003e\u003ePlaybook: State alt Telegraf Running Playbook-\u003e\u003ePlaybook: Do nothing else Telegraf Down Playbook-\u003e\u003ePlaybook: Create Telegraf config Playbook-\u003e\u003eDocker: Start Telegraf create participant Telegraf Docker-\u003e\u003eTelegraf: Run Container loop create participant Devices Telegraf-\u003e\u003eDevices: Fetch metrics Telegraf-\u003e\u003ePortal: Ship metrics end end else Mitigation Finished Playbook-\u003e\u003ePortal: Get active mitigations Portal--\u003e\u003ePlaybook: Active Mitigations alt Active Mitigations on Platform Playbook-\u003e\u003ePlaybook: Do nothing else No Active Mitigations on Platform Playbook-\u003e\u003eDocker: Stop Telegraf Docker-\u003e\u003eTelegraf: Container Down destroy Telegraf end end sequenceDiagram autonumber participant Portal participant EDA Portal-\u003e\u003eEDA: Alerts loop Listen EDA-\u003e\u003eEDA: Check Platform and State (Start/Stop) create participant Playbook EDA-\u003e\u003ePlaybook: Process Valid Alert end alt Mitigation Started Playbook-\u003e\u003eDocker: Is Telegraf Running? Docker--\u003e\u003ePlaybook: State alt Telegraf Running Playbook-\u003e\u003ePlaybook: Do nothing else Telegraf Down Playbook-\u003e\u003ePlaybook: Create Telegraf config Playbook-\u003e\u003eDocker: Start Telegraf create participant Telegraf Docker-\u003e\u003eTelegraf: Run Container loop create participant Devices Telegraf-\u003e\u003eDevices: Fetch metrics Telegraf-\u003e\u003ePortal: Ship metrics end end else Mitigation Finished Playbook-\u003e\u003ePortal","date":"2024-06-01","objectID":"/posts/modern-nms/:6:0","tags":["network monitoring","telemetry","ansible","automation"],"title":"If this ain't a modern NMS, then what is?","uri":"/posts/modern-nms/#ansible-playbook"},{"categories":["network monitoring"],"collections":null,"content":"Telegraf configuration When it comes to telegraf, we chose to have the config generated dynamically before the container starts. In this way we can: Specify the devices\u0026rsquo; attributes in the variables file and deduce the gnmi inputs from them Include the MitigationPlatformID as a tag along with all other event details Perform a lookup to include DEVICE_IP and DEVICE_NAME along with the tags. This was implemented via an inline starlark script referencing a lookup dictionary that is dynamically constructed from our devices dictionary. All sensitive data are passed from the EDA container environment and instantiated via docker when the container is brought up. Here is the template: [agent] omit_hostname = true debug = true quiet = false [global_tags] platform_id = \u0026#34;{{ MitigationPlatformID }}\u0026#34; [[inputs.gnmi]] addresses = [{% for device in Mitigation.devices %}\u0026#34;{{device.ip}}:{{device.port}}\u0026#34;{% if not loop.last %},{% endif %}{% endfor %}] username = \u0026#34;${GRPC_USERNAME}\u0026#34; password = \u0026#34;${GRPC_PASSWORD}\u0026#34; redial = \u0026#34;10s\u0026#34; encoding = \u0026#34;proto\u0026#34; tls_enable = true insecure_skip_verify = true [[inputs.gnmi.subscription]] name = \u0026#34;/devices/xrv9000/flowspec\u0026#34; origin = \u0026#34;Cisco-IOS-XR-flowspec-oper\u0026#34; path = \u0026#34;/flow-spec/vrfs/vrf/afs/af/flows/flow/flow-statistics/\u0026#34; subscription_mode = \u0026#34;sample\u0026#34; sample_interval = \u0026#34;30s\u0026#34; [[processors.rename]] [[processors.rename.replace]] tag = \u0026#34;source\u0026#34; dest = \u0026#34;device_ip\u0026#34; [[processors.override]] [processors.override.tagpass] vrf_name = [\u0026#34;\u0026#34;] [processors.override.tags] vrf_name = \u0026#34;default\u0026#34; [[processors.starlark]] source=\u0026#39;\u0026#39;\u0026#39; lookup = { {% for device in Mitigation.devices %} \u0026#34;{{ device.ip }}\u0026#34;: \u0026#34;{{ device.name }}\u0026#34;, {% endfor %}} def apply(metric): if metric.tags[\u0026#39;device_ip\u0026#39;] and metric.tags[\u0026#39;device_ip\u0026#39;] in lookup: metric.tags[\u0026#39;device_name\u0026#39;] = lookup[metric.tags[\u0026#39;device_ip\u0026#39;]] return metric \u0026#39;\u0026#39;\u0026#39; [[outputs.file]] files = [\u0026#34;stdout\u0026#34;] data_format = \u0026#34;influx\u0026#34; influx_sort_fields = false tagexclude = [\u0026#34;path\u0026#34;] [[outputs.http]] url = \u0026#34;{{ lookup(\u0026#39;env\u0026#39;, \u0026#39;KENTIK_API_ENDPOINT\u0026#39;)}}\u0026#34; data_format = \u0026#34;influx\u0026#34; influx_sort_fields = false tagexclude = [\u0026#34;path\u0026#34;] [outputs.http.headers] Content-Type = \u0026#34;application/influx\u0026#34; X-CH-Auth-Email = \u0026#34;${KENTIK_API_EMAIL}\u0026#34; X-CH-Auth-API-Token = \u0026#34;${KENTIK_API_TOKEN}\u0026#34;eda/telegraf/telegraf.conf.j2 And the respective config file produced to be picked up by docker: [agent] omit_hostname = true debug = true quiet = false [global_tags] platform_id = \u0026#34;257\u0026#34; [[inputs.gnmi]] addresses = [\u0026#34;10.12.255.1:57344\u0026#34;,\u0026#34;10.13.255.1:57344\u0026#34;] username = \u0026#34;${GRPC_USERNAME}\u0026#34; password = \u0026#34;${GRPC_PASSWORD}\u0026#34; redial = \u0026#34;10s\u0026#34; encoding = \u0026#34;proto\u0026#34; tls_enable = true insecure_skip_verify = true [[inputs.gnmi.subscription]] name = \u0026#34;/devices/xrv9000/flowspec\u0026#34; origin = \u0026#34;Cisco-IOS-XR-flowspec-oper\u0026#34; path = \u0026#34;/flow-spec/vrfs/vrf/afs/af/flows/flow/flow-statistics/\u0026#34; subscription_mode = \u0026#34;sample\u0026#34; sample_interval = \u0026#34;30s\u0026#34; [[processors.rename]] [[processors.rename.replace]] tag = \u0026#34;source\u0026#34; dest = \u0026#34;device_ip\u0026#34; [[processors.override]] [processors.override.tagpass] vrf_name = [\u0026#34;\u0026#34;] [processors.override.tags] vrf_name = \u0026#34;default\u0026#34; [[processors.starlark]] source=\u0026#39;\u0026#39;\u0026#39; lookup = { \u0026#34;10.12.255.1\u0026#34;: \u0026#34;ath-pop1-xrv1\u0026#34;, \u0026#34;10.13.255.1\u0026#34;: \u0026#34;ath-pop1-xrv2\u0026#34;, } def apply(metric): if metric.tags[\u0026#39;device_ip\u0026#39;] and metric.tags[\u0026#39;device_ip\u0026#39;] in lookup: metric.tags[\u0026#39;device_name\u0026#39;] = lookup[metric.tags[\u0026#39;device_ip\u0026#39;]] return metric \u0026#39;\u0026#39;\u0026#39; [[outputs.file]] files = [\u0026#34;stdout\u0026#34;] data_format = \u0026#34;influx\u0026#34; influx_sort_fields = false tagexclude = [\u0026#34;path\u0026#34;] [[outputs.http]] url = \u0026#34;https://grpc.api.kentik.eu/kmetrics/v","date":"2024-06-01","objectID":"/posts/modern-nms/:6:1","tags":["network monitoring","telemetry","ansible","automation"],"title":"If this ain't a modern NMS, then what is?","uri":"/posts/modern-nms/#telegraf-configuration"},{"categories":["network monitoring"],"collections":null,"content":"Moment of Truth After starting some manual mitigations, telegraf will ship the gnmi metrics to the Portal and those will be available under the /devices/xrv9000/flowspec custom schema path based on our configuration in the telegraf.conffile. Here is how it looks like in the Metrics Explorer Flowspec metrics in the Portal Here are some console outputs from EDA running: ** 2024-05-26 16:00:04.559137 [debug] ****************************************** Ignoring mitigation event ******************************************************************************** ** 2024-05-26 16:00:18.406938 [debug] ****************************************** New manual/manualMitigating event received ID: 10108 Platform: XR-FlowSpec Method: Discard ICMP echo-request IP: 10.10.10.11/32 ******************************************************************************** PLAY [::HANDLE MITIGATION EVENT::] ********************************************* TASK [START::IS TELEGRAF RUNNING] ********************************************** ok: [localhost] TASK [START::GENERATE TELEGRAF CONFIG] ***************************************** ok: [localhost] TASK [START::BRING UP CONTAINER] *********************************************** changed: [localhost] TASK [STOP::GET ACTIVE ALERTS FROM KENTIK] ************************************* skipping: [localhost] TASK [STOP::CHECK KENTIK FOR ACTIVE MITIGATIONS] ******************************* skipping: [localhost] TASK [STOP::REMOVE TELEGRAF CONTAINER] ***************************************** skipping: [localhost] PLAY RECAP ********************************************************************* localhost : ok=3 changed=1 unreachable=0 failed=0 skipped=3 rescued=0 ignored=0New mitigation ** 2024-05-26 16:10:20.682550 [debug] ****************************************** New manual/archived event received ID: 10111 Platform: XR-FlowSpec Method: Rate Limit 20k TCP SSH IP: 10.10.10.15/32 ******************************************************************************** PLAY [::HANDLE MITIGATION EVENT::] ********************************************* TASK [START::IS TELEGRAF RUNNING] ********************************************** skipping: [localhost] TASK [START::GENERATE TELEGRAF CONFIG] ***************************************** skipping: [localhost] TASK [START::BRING UP CONTAINER] *********************************************** skipping: [localhost] TASK [STOP::GET ACTIVE ALERTS FROM KENTIK] ************************************* ok: [localhost] TASK [STOP::CHECK KENTIK FOR ACTIVE MITIGATIONS] ******************************* ok: [localhost] =\u0026gt; { \u0026#34;msg\u0026#34;: [ \u0026#34;Current Active Mitigations on Platform ID#257: 3\u0026#34; ] } TASK [STOP::REMOVE TELEGRAF CONTAINER] ***************************************** skipping: [localhost] PLAY RECAP ********************************************************************* localhost : ok=2 changed=0 unreachable=0 failed=0 skipped=4 rescued=0 ignored=0Stop mitigation while others are running ","date":"2024-06-01","objectID":"/posts/modern-nms/:7:0","tags":["network monitoring","telemetry","ansible","automation"],"title":"If this ain't a modern NMS, then what is?","uri":"/posts/modern-nms/#moment-of-truth"},{"categories":["network monitoring"],"collections":null,"content":"Outro Well I hope this post was interesting enough and can be used as a reference while exploring the use cases covered: How to leverage Kentik EDA to receive Portal notifications and execute ansible playbooks How to use telegraf to send custom ST metrics to Kentik NMS Generating telegraf configuration dynamically ...till next time...have fun!!! ","date":"2024-06-01","objectID":"/posts/modern-nms/:8:0","tags":["network monitoring","telemetry","ansible","automation"],"title":"If this ain't a modern NMS, then what is?","uri":"/posts/modern-nms/#outro"},{"categories":["network monitoring"],"collections":null,"content":"Influences and Reference Driving Network Automation Innovation: Kentik and Red Hat Launch Integration EDA Quickstart Using Telegraf to Feed API JSON Data into Kentik NMS Kentik\u0026rsquo;s ansible_eda Repo for this post ","date":"2024-06-01","objectID":"/posts/modern-nms/:9:0","tags":["network monitoring","telemetry","ansible","automation"],"title":"If this ain't a modern NMS, then what is?","uri":"/posts/modern-nms/#influences-and-reference"},{"categories":["network monitoring"],"collections":null,"content":"Intro Since the launch of Kentik NMS, it it is possible to ingest and display custom metrics in the Portal. In this post we will be going over a walkthrough of how we can enable Kentik\u0026rsquo;s Universal Agent to poll custom SNMP and Streaming Telemetry metrics from devices and report them back to the Portal. The use case is about having visibility into BGP Flowspec counters to see what our mitigations are reporting on the devices. We are going to address this through SNMP as well as ST for both Junos and IOS-XR devices. For Junos, the flowspec rules are translated into firewall filters which have counters and policers depending on the action. We have two possible ways to extract those, either via SNMP or via streaming telemetry and we are going to use both. For IOS-XR, on the other-hand, there are no SNMP flowspec related OIDs, but there is a dedicated flowspec YANG model we can use to extract the counters via streaming telemetry. The various tools and device versions used throughout the use case are listed below: Kentik kagent docker container snmpwalk and snmptable gnmic Junos vMX on 21.3R1.9 Cisco IOS-XRv 9000 on 7.9.1 You may find all relevant files in my repo. ","date":"2024-05-04","objectID":"/posts/kustom-metrics/:1:0","tags":["network","monitoring","telemetry","snmp"],"title":"K~ustom Metrics in Kentik NMS","uri":"/posts/kustom-metrics/#intro"},{"categories":["network monitoring"],"collections":null,"content":"Kentik Universal Agent (kagent) In order to use the custom metrics feature on the kagent we need some local configuration to be picked up by the agent on startup so it can be processed. In summary, we need three configuration files to exist localy in a specific directory structure that we can call the override directory. Below is the structure along with a brief explanation on the purpose of each one. /opt/kentik/components/ranger/local/ └── config ├── profiles # Holds the configuration yaml files for \u0026#39;binding\u0026#39; the sources to specific \u0026#39;device\u0026#39; types ├── reports # Holds the configuration yaml files for \u0026#39;how\u0026#39; we want to report what is collected from the sources in the portal └── sources # Holds the configuration yaml files for \u0026#39;what\u0026#39; we want to poll from the deviceSo once kagent starts it looks for the local folder and it processes those files in the structure. For example, if we want to poll an SNMP OID for a device, we would need to create a configuration file under the sources directory specifying which OID to poll and how often. Then we need to bind this source to only get polled if say the device is of a specific sysObjectID, and we do that in the profiles directory. Lastly, in the reports directory we define the path to present the data in the portal, how the data is represented and polled from the MIB, i.e. metrics or dimensions, and how often to update the values in the database. Additionally, in the reports directory we can use starlark scripts to add additional logic to our reporting capabilities. ","date":"2024-05-04","objectID":"/posts/kustom-metrics/:2:0","tags":["network","monitoring","telemetry","snmp"],"title":"K~ustom Metrics in Kentik NMS","uri":"/posts/kustom-metrics/#kentik-universal-agent-kagent"},{"categories":["network monitoring"],"collections":null,"content":"Kagent container We are going to be running our kagent instance as a container through compose. Here is how our directory structure looks like: /opt/dev/kentik/ ├── docker-compose.yml # How to bring up the container ├── kagent-data # Local dir mount so kagent data persists -\u0026gt; /opt/kentik in the container └── override-data # Local overrides directory with custom definitions -\u0026gt; /opt/kentik/components/ranger/local in the container └── config ├── profiles ├── reports └── sourcesAnd here is the docker compose file: --- services: kagent: hostname: kagent03 image: kentik/kagent:latest restart: unless-stopped pull_policy: always cap_add: - NET_RAW environment: - K_COMPANY_ID=\u0026lt;REDACTED\u0026gt; - K_API_ROOT=grpc.api.kentik.eu:443 #- K_LOG_LEVEL=debug volumes: - /opt/dev/kentik/kagent-data:/opt/kentik - /opt/dev/kentik/override-data:/opt/kentik/components/ranger/local/ ","date":"2024-05-04","objectID":"/posts/kustom-metrics/:2:1","tags":["network","monitoring","telemetry","snmp"],"title":"K~ustom Metrics in Kentik NMS","uri":"/posts/kustom-metrics/#kagent-container"},{"categories":["network monitoring"],"collections":null,"content":"Junos Firewall Filters In Junos the flowspec rules create relevant ad-hoc firewall filters and policers that are applied to the linecards and can be examined via the show firewall detail command. Here is what we get with nothing configured: netops@ATH-POP1-VMX1\u0026gt; show configuration firewall netops@ATH-POP1-VMX1\u0026gt; show firewall detail Filter: __default_bpdu_filter__And here is what we get if we configure a filter to count SSH packets destined to the management IP of the device and two manual mitigations from the portal, one to block traffic and one to rate limit: netops@ATH-POP1-VMX1\u0026gt; show firewall detail Filter: __default_bpdu_filter__ Filter: TEST-FIREWALL Counters: Name Bytes Packets SSH-PACKETS 74932 1153 Filter: __flowspec_default_inet__ Counters: Name Bytes Packets 10.11.10.10,*,icmp-type=8 41496 30 10.11.10.10,*,proto=6,dstport=5201 172233 184 Policers: Name Bytes Packets 40K_10.11.10.10,*,proto=6,dstport=5201 402000 268So any flowpsec rule received is reported under the __flowspec_default_inet__ filter. The one that blocks ICMP traffic is a counter and the one that limits IPERF is both a counter and a policer. ","date":"2024-05-04","objectID":"/posts/kustom-metrics/:3:0","tags":["network","monitoring","telemetry","snmp"],"title":"K~ustom Metrics in Kentik NMS","uri":"/posts/kustom-metrics/#junos-firewall-filters"},{"categories":["network monitoring"],"collections":null,"content":"SNMP It seems that Juniper has a dedicated MIB for reporting those firewall metrics, the JUNOS-FIREWALL-MIB and after downloading all juniper MIBs in my ~/.snmp/mibs directory and grepping for Firewall I got this: jnxFirewallsTable OBJECT-TYPE SYNTAX SEQUENCE OF JnxFirewallsEntry MAX-ACCESS not-accessible STATUS deprecated DESCRIPTION \u0026#34;A list of firewalls entries. NOTE: This table is deprecated and exists for backward compatibility. The user is encouraged to use jnxFirewallCounterTable. This table does not handle: 1) counter and filter names greater than 24 characters 2) counters with same names but different types (the first duplicate is returned only)\u0026#34; ::= { jnxFirewalls 1 }So the interesting OID to be polled is the jnxFirewallCounterTable at 1.3.6.1.4.1.2636.3.5.2. Let’s use snmptable to see what we get: $ snmptable -v2c -c kentik -m all 10.11.255.1 jnxFirewallCounterTable SNMP table: JUNIPER-FIREWALL-MIB::jnxFirewallCounterTable jnxFWCounterPacketCount jnxFWCounterByteCount jnxFWCounterDisplayFilterName jnxFWCounterDisplayName jnxFWCounterDisplayType 1118 72508 TEST-FIREWALL SSH-PACKETS counter 0 0 __default_arp_policer__ __default_arp_policer__ policer 30 41496 __flowspec_default_inet__ 10.11.10.10,*,icmp-type=8 counter 184 172233 __flowspec_default_inet__ 10.11.10.10,*,proto=6,dstport=5201 counter 268 402000 __flowspec_default_inet__ 40K_10.11.10.10,*,proto=6,dstport=5201 policerLocal Configuration From this point onward we can define our custom metrics in the relevant overrides sub-directories Sources file We are going to poll the table every 1 minute. --- version: 1 metadata: name: junos-fw-snmp kind: sources sources: junos-fw-snmp: !snmp table: 1.3.6.1.4.1.2636.3.5.2 interval: 60sProfiles file We will bind the source to the vMX sysObjectId --- version: 1 metadata: name: junos-fw-snmp kind: profile profile: match: sysobjectid: - 1.3.6.1.4.1.2636.1.1.1.2.108 reports: - junos-fw-snmp include: - device_name_ip sources: - junos-fw-snmpReports file We are going to report those under the /junos/firewall/snmp schema path in the Portal and we define the mapping on the table fields identifying the metrics: --- version: 1 metadata: name: junos-fw-snmp kind: reports reports: /junos/firewall/snmp: combine: table0: !snmp table: 1.3.6.1.4.1.2636.3.5.2 index: $index0 fields: packets: !snmp table: 1.3.6.1.4.1.2636.3.5.2 value: 1.3.6.1.4.1.2636.3.5.2.1.4 metric: true bytes: !snmp table: 1.3.6.1.4.1.2636.3.5.2 value: 1.3.6.1.4.1.2636.3.5.2.1.5 metric: true filter_name: !snmp table: 1.3.6.1.4.1.2636.3.5.2 value: 1.3.6.1.4.1.2636.3.5.2.1.6 metric: false counter_name: !snmp table: 1.3.6.1.4.1.2636.3.5.2 value: 1.3.6.1.4.1.2636.3.5.2.1.7 metric: false counter_type: !snmp table: 1.3.6.1.4.1.2636.3.5.2 value: 1.3.6.1.4.1.2636.3.5.2.1.8 metric: false tweak: !enum 1: other 2: counter 3: policer interval: 60sHere is how our overrides directory structure looks like: override-data/ └── config ├── profiles │ ├── junos-fw-snmp.yml ├── reports │ ├── junos-fw-snmp.yml └── sources └── junos-fw-snmp.ymlI\u0026rsquo;ve used the same file name for these but it could be anything. Resources are referenced via the metadata name key and not by the filename. Kagent Bring Up Next step is to bring up the container and see what we get, but it seems there is an error: {\u0026#34;level\u0026#34;:\u0026#34;error\u0026#34;,\u0026#34;error\u0026#34;:\u0026#34;OID 1.3.6.1.4.1.2636.3.5.2 in table source jnxFirewallCounterTable loaded from config/sources/snmp/juniper.yml also appears in table source junos-fw-snmp loaded from sources/junos-fw-snmp.yml\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2024-05-03T16:51:41Z\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;invalid config\u0026#34;}This means that kagent is already configured to poll this OID from the file mentioned. Kagent Default Config Kagent is pulling its default configuration each time is brought up and this is in the /opt/kentik/components/ranger/current/LATEST.zip file So after unziping the file and looking into the file mentioned we see that the definition is there and the table is ","date":"2024-05-04","objectID":"/posts/kustom-metrics/:3:1","tags":["network","monitoring","telemetry","snmp"],"title":"K~ustom Metrics in Kentik NMS","uri":"/posts/kustom-metrics/#snmp"},{"categories":["network monitoring"],"collections":null,"content":"SNMP It seems that Juniper has a dedicated MIB for reporting those firewall metrics, the JUNOS-FIREWALL-MIB and after downloading all juniper MIBs in my ~/.snmp/mibs directory and grepping for Firewall I got this: jnxFirewallsTable OBJECT-TYPE SYNTAX SEQUENCE OF JnxFirewallsEntry MAX-ACCESS not-accessible STATUS deprecated DESCRIPTION \u0026#34;A list of firewalls entries. NOTE: This table is deprecated and exists for backward compatibility. The user is encouraged to use jnxFirewallCounterTable. This table does not handle: 1) counter and filter names greater than 24 characters 2) counters with same names but different types (the first duplicate is returned only)\u0026#34; ::= { jnxFirewalls 1 }So the interesting OID to be polled is the jnxFirewallCounterTable at 1.3.6.1.4.1.2636.3.5.2. Let’s use snmptable to see what we get: $ snmptable -v2c -c kentik -m all 10.11.255.1 jnxFirewallCounterTable SNMP table: JUNIPER-FIREWALL-MIB::jnxFirewallCounterTable jnxFWCounterPacketCount jnxFWCounterByteCount jnxFWCounterDisplayFilterName jnxFWCounterDisplayName jnxFWCounterDisplayType 1118 72508 TEST-FIREWALL SSH-PACKETS counter 0 0 __default_arp_policer__ __default_arp_policer__ policer 30 41496 __flowspec_default_inet__ 10.11.10.10,*,icmp-type=8 counter 184 172233 __flowspec_default_inet__ 10.11.10.10,*,proto=6,dstport=5201 counter 268 402000 __flowspec_default_inet__ 40K_10.11.10.10,*,proto=6,dstport=5201 policerLocal Configuration From this point onward we can define our custom metrics in the relevant overrides sub-directories Sources file We are going to poll the table every 1 minute. --- version: 1 metadata: name: junos-fw-snmp kind: sources sources: junos-fw-snmp: !snmp table: 1.3.6.1.4.1.2636.3.5.2 interval: 60sProfiles file We will bind the source to the vMX sysObjectId --- version: 1 metadata: name: junos-fw-snmp kind: profile profile: match: sysobjectid: - 1.3.6.1.4.1.2636.1.1.1.2.108 reports: - junos-fw-snmp include: - device_name_ip sources: - junos-fw-snmpReports file We are going to report those under the /junos/firewall/snmp schema path in the Portal and we define the mapping on the table fields identifying the metrics: --- version: 1 metadata: name: junos-fw-snmp kind: reports reports: /junos/firewall/snmp: combine: table0: !snmp table: 1.3.6.1.4.1.2636.3.5.2 index: $index0 fields: packets: !snmp table: 1.3.6.1.4.1.2636.3.5.2 value: 1.3.6.1.4.1.2636.3.5.2.1.4 metric: true bytes: !snmp table: 1.3.6.1.4.1.2636.3.5.2 value: 1.3.6.1.4.1.2636.3.5.2.1.5 metric: true filter_name: !snmp table: 1.3.6.1.4.1.2636.3.5.2 value: 1.3.6.1.4.1.2636.3.5.2.1.6 metric: false counter_name: !snmp table: 1.3.6.1.4.1.2636.3.5.2 value: 1.3.6.1.4.1.2636.3.5.2.1.7 metric: false counter_type: !snmp table: 1.3.6.1.4.1.2636.3.5.2 value: 1.3.6.1.4.1.2636.3.5.2.1.8 metric: false tweak: !enum 1: other 2: counter 3: policer interval: 60sHere is how our overrides directory structure looks like: override-data/ └── config ├── profiles │ ├── junos-fw-snmp.yml ├── reports │ ├── junos-fw-snmp.yml └── sources └── junos-fw-snmp.ymlI\u0026rsquo;ve used the same file name for these but it could be anything. Resources are referenced via the metadata name key and not by the filename. Kagent Bring Up Next step is to bring up the container and see what we get, but it seems there is an error: {\u0026#34;level\u0026#34;:\u0026#34;error\u0026#34;,\u0026#34;error\u0026#34;:\u0026#34;OID 1.3.6.1.4.1.2636.3.5.2 in table source jnxFirewallCounterTable loaded from config/sources/snmp/juniper.yml also appears in table source junos-fw-snmp loaded from sources/junos-fw-snmp.yml\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2024-05-03T16:51:41Z\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;invalid config\u0026#34;}This means that kagent is already configured to poll this OID from the file mentioned. Kagent Default Config Kagent is pulling its default configuration each time is brought up and this is in the /opt/kentik/components/ranger/current/LATEST.zip file So after unziping the file and looking into the file mentioned we see that the definition is there and the table is ","date":"2024-05-04","objectID":"/posts/kustom-metrics/:3:1","tags":["network","monitoring","telemetry","snmp"],"title":"K~ustom Metrics in Kentik NMS","uri":"/posts/kustom-metrics/#local-configuration"},{"categories":["network monitoring"],"collections":null,"content":"SNMP It seems that Juniper has a dedicated MIB for reporting those firewall metrics, the JUNOS-FIREWALL-MIB and after downloading all juniper MIBs in my ~/.snmp/mibs directory and grepping for Firewall I got this: jnxFirewallsTable OBJECT-TYPE SYNTAX SEQUENCE OF JnxFirewallsEntry MAX-ACCESS not-accessible STATUS deprecated DESCRIPTION \u0026#34;A list of firewalls entries. NOTE: This table is deprecated and exists for backward compatibility. The user is encouraged to use jnxFirewallCounterTable. This table does not handle: 1) counter and filter names greater than 24 characters 2) counters with same names but different types (the first duplicate is returned only)\u0026#34; ::= { jnxFirewalls 1 }So the interesting OID to be polled is the jnxFirewallCounterTable at 1.3.6.1.4.1.2636.3.5.2. Let’s use snmptable to see what we get: $ snmptable -v2c -c kentik -m all 10.11.255.1 jnxFirewallCounterTable SNMP table: JUNIPER-FIREWALL-MIB::jnxFirewallCounterTable jnxFWCounterPacketCount jnxFWCounterByteCount jnxFWCounterDisplayFilterName jnxFWCounterDisplayName jnxFWCounterDisplayType 1118 72508 TEST-FIREWALL SSH-PACKETS counter 0 0 __default_arp_policer__ __default_arp_policer__ policer 30 41496 __flowspec_default_inet__ 10.11.10.10,*,icmp-type=8 counter 184 172233 __flowspec_default_inet__ 10.11.10.10,*,proto=6,dstport=5201 counter 268 402000 __flowspec_default_inet__ 40K_10.11.10.10,*,proto=6,dstport=5201 policerLocal Configuration From this point onward we can define our custom metrics in the relevant overrides sub-directories Sources file We are going to poll the table every 1 minute. --- version: 1 metadata: name: junos-fw-snmp kind: sources sources: junos-fw-snmp: !snmp table: 1.3.6.1.4.1.2636.3.5.2 interval: 60sProfiles file We will bind the source to the vMX sysObjectId --- version: 1 metadata: name: junos-fw-snmp kind: profile profile: match: sysobjectid: - 1.3.6.1.4.1.2636.1.1.1.2.108 reports: - junos-fw-snmp include: - device_name_ip sources: - junos-fw-snmpReports file We are going to report those under the /junos/firewall/snmp schema path in the Portal and we define the mapping on the table fields identifying the metrics: --- version: 1 metadata: name: junos-fw-snmp kind: reports reports: /junos/firewall/snmp: combine: table0: !snmp table: 1.3.6.1.4.1.2636.3.5.2 index: $index0 fields: packets: !snmp table: 1.3.6.1.4.1.2636.3.5.2 value: 1.3.6.1.4.1.2636.3.5.2.1.4 metric: true bytes: !snmp table: 1.3.6.1.4.1.2636.3.5.2 value: 1.3.6.1.4.1.2636.3.5.2.1.5 metric: true filter_name: !snmp table: 1.3.6.1.4.1.2636.3.5.2 value: 1.3.6.1.4.1.2636.3.5.2.1.6 metric: false counter_name: !snmp table: 1.3.6.1.4.1.2636.3.5.2 value: 1.3.6.1.4.1.2636.3.5.2.1.7 metric: false counter_type: !snmp table: 1.3.6.1.4.1.2636.3.5.2 value: 1.3.6.1.4.1.2636.3.5.2.1.8 metric: false tweak: !enum 1: other 2: counter 3: policer interval: 60sHere is how our overrides directory structure looks like: override-data/ └── config ├── profiles │ ├── junos-fw-snmp.yml ├── reports │ ├── junos-fw-snmp.yml └── sources └── junos-fw-snmp.ymlI\u0026rsquo;ve used the same file name for these but it could be anything. Resources are referenced via the metadata name key and not by the filename. Kagent Bring Up Next step is to bring up the container and see what we get, but it seems there is an error: {\u0026#34;level\u0026#34;:\u0026#34;error\u0026#34;,\u0026#34;error\u0026#34;:\u0026#34;OID 1.3.6.1.4.1.2636.3.5.2 in table source jnxFirewallCounterTable loaded from config/sources/snmp/juniper.yml also appears in table source junos-fw-snmp loaded from sources/junos-fw-snmp.yml\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2024-05-03T16:51:41Z\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;invalid config\u0026#34;}This means that kagent is already configured to poll this OID from the file mentioned. Kagent Default Config Kagent is pulling its default configuration each time is brought up and this is in the /opt/kentik/components/ranger/current/LATEST.zip file So after unziping the file and looking into the file mentioned we see that the definition is there and the table is ","date":"2024-05-04","objectID":"/posts/kustom-metrics/:3:1","tags":["network","monitoring","telemetry","snmp"],"title":"K~ustom Metrics in Kentik NMS","uri":"/posts/kustom-metrics/#kagent-bring-up"},{"categories":["network monitoring"],"collections":null,"content":"SNMP It seems that Juniper has a dedicated MIB for reporting those firewall metrics, the JUNOS-FIREWALL-MIB and after downloading all juniper MIBs in my ~/.snmp/mibs directory and grepping for Firewall I got this: jnxFirewallsTable OBJECT-TYPE SYNTAX SEQUENCE OF JnxFirewallsEntry MAX-ACCESS not-accessible STATUS deprecated DESCRIPTION \u0026#34;A list of firewalls entries. NOTE: This table is deprecated and exists for backward compatibility. The user is encouraged to use jnxFirewallCounterTable. This table does not handle: 1) counter and filter names greater than 24 characters 2) counters with same names but different types (the first duplicate is returned only)\u0026#34; ::= { jnxFirewalls 1 }So the interesting OID to be polled is the jnxFirewallCounterTable at 1.3.6.1.4.1.2636.3.5.2. Let’s use snmptable to see what we get: $ snmptable -v2c -c kentik -m all 10.11.255.1 jnxFirewallCounterTable SNMP table: JUNIPER-FIREWALL-MIB::jnxFirewallCounterTable jnxFWCounterPacketCount jnxFWCounterByteCount jnxFWCounterDisplayFilterName jnxFWCounterDisplayName jnxFWCounterDisplayType 1118 72508 TEST-FIREWALL SSH-PACKETS counter 0 0 __default_arp_policer__ __default_arp_policer__ policer 30 41496 __flowspec_default_inet__ 10.11.10.10,*,icmp-type=8 counter 184 172233 __flowspec_default_inet__ 10.11.10.10,*,proto=6,dstport=5201 counter 268 402000 __flowspec_default_inet__ 40K_10.11.10.10,*,proto=6,dstport=5201 policerLocal Configuration From this point onward we can define our custom metrics in the relevant overrides sub-directories Sources file We are going to poll the table every 1 minute. --- version: 1 metadata: name: junos-fw-snmp kind: sources sources: junos-fw-snmp: !snmp table: 1.3.6.1.4.1.2636.3.5.2 interval: 60sProfiles file We will bind the source to the vMX sysObjectId --- version: 1 metadata: name: junos-fw-snmp kind: profile profile: match: sysobjectid: - 1.3.6.1.4.1.2636.1.1.1.2.108 reports: - junos-fw-snmp include: - device_name_ip sources: - junos-fw-snmpReports file We are going to report those under the /junos/firewall/snmp schema path in the Portal and we define the mapping on the table fields identifying the metrics: --- version: 1 metadata: name: junos-fw-snmp kind: reports reports: /junos/firewall/snmp: combine: table0: !snmp table: 1.3.6.1.4.1.2636.3.5.2 index: $index0 fields: packets: !snmp table: 1.3.6.1.4.1.2636.3.5.2 value: 1.3.6.1.4.1.2636.3.5.2.1.4 metric: true bytes: !snmp table: 1.3.6.1.4.1.2636.3.5.2 value: 1.3.6.1.4.1.2636.3.5.2.1.5 metric: true filter_name: !snmp table: 1.3.6.1.4.1.2636.3.5.2 value: 1.3.6.1.4.1.2636.3.5.2.1.6 metric: false counter_name: !snmp table: 1.3.6.1.4.1.2636.3.5.2 value: 1.3.6.1.4.1.2636.3.5.2.1.7 metric: false counter_type: !snmp table: 1.3.6.1.4.1.2636.3.5.2 value: 1.3.6.1.4.1.2636.3.5.2.1.8 metric: false tweak: !enum 1: other 2: counter 3: policer interval: 60sHere is how our overrides directory structure looks like: override-data/ └── config ├── profiles │ ├── junos-fw-snmp.yml ├── reports │ ├── junos-fw-snmp.yml └── sources └── junos-fw-snmp.ymlI\u0026rsquo;ve used the same file name for these but it could be anything. Resources are referenced via the metadata name key and not by the filename. Kagent Bring Up Next step is to bring up the container and see what we get, but it seems there is an error: {\u0026#34;level\u0026#34;:\u0026#34;error\u0026#34;,\u0026#34;error\u0026#34;:\u0026#34;OID 1.3.6.1.4.1.2636.3.5.2 in table source jnxFirewallCounterTable loaded from config/sources/snmp/juniper.yml also appears in table source junos-fw-snmp loaded from sources/junos-fw-snmp.yml\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2024-05-03T16:51:41Z\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;invalid config\u0026#34;}This means that kagent is already configured to poll this OID from the file mentioned. Kagent Default Config Kagent is pulling its default configuration each time is brought up and this is in the /opt/kentik/components/ranger/current/LATEST.zip file So after unziping the file and looking into the file mentioned we see that the definition is there and the table is ","date":"2024-05-04","objectID":"/posts/kustom-metrics/:3:1","tags":["network","monitoring","telemetry","snmp"],"title":"K~ustom Metrics in Kentik NMS","uri":"/posts/kustom-metrics/#kentik-portal"},{"categories":["network monitoring"],"collections":null,"content":"GRPC Let’s try now to get the same via streaming telemetry and GRPC. According to this we can get those metrics under the /junos/system/linecard/firewall path. Using gnmic (I could only get data via subscriptions and not get rpc calls): $ gnmic -a 10.11.255.1:32767 -u \u0026lt;username\u0026gt;-p \u0026lt;password\u0026gt; --skip-verify sub --mode once --path /junos/system/linecard/firewall { \u0026#34;source\u0026#34;: \u0026#34;10.11.255.1:32767\u0026#34;, \u0026#34;subscription-name\u0026#34;: \u0026#34;default-1714758367\u0026#34;, \u0026#34;timestamp\u0026#34;: 1714758371632000000, \u0026#34;time\u0026#34;: \u0026#34;2024-05-03T20:46:11.632+03:00\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;junos/firewall[name=__default_bpdu_filter__]/state\u0026#34;, \u0026#34;updates\u0026#34;: [ { \u0026#34;Path\u0026#34;: \u0026#34;timestamp\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;timestamp\u0026#34;: 1714746658 } }, { \u0026#34;Path\u0026#34;: \u0026#34;memory-usage[name=HEAP]/allocated\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;memory-usage/allocated\u0026#34;: 2596 } } ] } { \u0026#34;source\u0026#34;: \u0026#34;10.11.255.1:32767\u0026#34;, \u0026#34;subscription-name\u0026#34;: \u0026#34;default-1714758367\u0026#34;, \u0026#34;timestamp\u0026#34;: 1714758371632000000, \u0026#34;time\u0026#34;: \u0026#34;2024-05-03T20:46:11.632+03:00\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;junos/firewall[name=TEST-FIREWALL]/state\u0026#34;, \u0026#34;updates\u0026#34;: [ { \u0026#34;Path\u0026#34;: \u0026#34;timestamp\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;timestamp\u0026#34;: 1714746658 } }, { \u0026#34;Path\u0026#34;: \u0026#34;memory-usage[name=HEAP]/allocated\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;memory-usage/allocated\u0026#34;: 4004 } }, { \u0026#34;Path\u0026#34;: \u0026#34;counter[name=SSH-PACKETS]/packets\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;counter/packets\u0026#34;: 1162 } }, { \u0026#34;Path\u0026#34;: \u0026#34;counter[name=SSH-PACKETS]/bytes\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;counter/bytes\u0026#34;: 75400 } } ] } { \u0026#34;source\u0026#34;: \u0026#34;10.11.255.1:32767\u0026#34;, \u0026#34;subscription-name\u0026#34;: \u0026#34;default-1714758367\u0026#34;, \u0026#34;timestamp\u0026#34;: 1714758371632000000, \u0026#34;time\u0026#34;: \u0026#34;2024-05-03T20:46:11.632+03:00\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;junos/firewall[name=__default_arp_policer__]/state\u0026#34;, \u0026#34;updates\u0026#34;: [ { \u0026#34;Path\u0026#34;: \u0026#34;timestamp\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;timestamp\u0026#34;: 1714403887 } }, { \u0026#34;Path\u0026#34;: \u0026#34;memory-usage[name=HEAP]/allocated\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;memory-usage/allocated\u0026#34;: 1652 } }, { \u0026#34;Path\u0026#34;: \u0026#34;policer[name=__default_arp_policer__]/out-of-spec-packets\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;policer/out-of-spec-packets\u0026#34;: 0 } }, { \u0026#34;Path\u0026#34;: \u0026#34;policer[name=__default_arp_policer__]/out-of-spec-bytes\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;policer/out-of-spec-bytes\u0026#34;: 0 } }, { \u0026#34;Path\u0026#34;: \u0026#34;policer[name=__default_arp_policer__]/offered-packets\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;policer/offered-packets\u0026#34;: 0 } }, { \u0026#34;Path\u0026#34;: \u0026#34;policer[name=__default_arp_policer__]/offered-bytes\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;policer/offered-bytes\u0026#34;: 0 } }, { \u0026#34;Path\u0026#34;: \u0026#34;policer[name=__default_arp_policer__]/transmitted-packets\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;policer/transmitted-packets\u0026#34;: 0 } }, { \u0026#34;Path\u0026#34;: \u0026#34;policer[name=__default_arp_policer__]/transmitted-bytes\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;policer/transmitted-bytes\u0026#34;: 0 } } ] } { \u0026#34;source\u0026#34;: \u0026#34;10.11.255.1:32767\u0026#34;, \u0026#34;subscription-name\u0026#34;: \u0026#34;default-1714758367\u0026#34;, \u0026#34;timestamp\u0026#34;: 1714758371632000000, \u0026#34;time\u0026#34;: \u0026#34;2024-05-03T20:46:11.632+03:00\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;junos/firewall[name=__flowspec_default_inet__]/state\u0026#34;, \u0026#34;updates\u0026#34;: [ { \u0026#34;Path\u0026#34;: \u0026#34;timestamp\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;timestamp\u0026#34;: 1714753321 } }, { \u0026#34;Path\u0026#34;: \u0026#34;memory-usage[name=HEAP]/allocated\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;memory-usage/allocated\u0026#34;: 7172 } }, { \u0026#34;Path\u0026#34;: \u0026#34;counter[name=10.11.10.10,*,icmp-type=8]/packets\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;counter/packets\u0026#34;: 1315 } }, { \u0026#34;Path\u0026#34;: \u0026#34;counter[name=10.11.10.10,*,icmp-type=8]/bytes\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;counter/bytes\u0026#34;: 1876476 } }, { \u0026#34;Path\u0026#34;: \u0026#34;counter[name=10.11.10.10,*,proto=6,dstport=5201]/packets\u0026#34;, \u0026#34;values\u0026#34;: { \u0026","date":"2024-05-04","objectID":"/posts/kustom-metrics/:3:2","tags":["network","monitoring","telemetry","snmp"],"title":"K~ustom Metrics in Kentik NMS","uri":"/posts/kustom-metrics/#grpc"},{"categories":["network monitoring"],"collections":null,"content":"GRPC Let’s try now to get the same via streaming telemetry and GRPC. According to this we can get those metrics under the /junos/system/linecard/firewall path. Using gnmic (I could only get data via subscriptions and not get rpc calls): $ gnmic -a 10.11.255.1:32767 -u \u0026lt;username\u0026gt;-p \u0026lt;password\u0026gt; --skip-verify sub --mode once --path /junos/system/linecard/firewall { \u0026#34;source\u0026#34;: \u0026#34;10.11.255.1:32767\u0026#34;, \u0026#34;subscription-name\u0026#34;: \u0026#34;default-1714758367\u0026#34;, \u0026#34;timestamp\u0026#34;: 1714758371632000000, \u0026#34;time\u0026#34;: \u0026#34;2024-05-03T20:46:11.632+03:00\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;junos/firewall[name=__default_bpdu_filter__]/state\u0026#34;, \u0026#34;updates\u0026#34;: [ { \u0026#34;Path\u0026#34;: \u0026#34;timestamp\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;timestamp\u0026#34;: 1714746658 } }, { \u0026#34;Path\u0026#34;: \u0026#34;memory-usage[name=HEAP]/allocated\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;memory-usage/allocated\u0026#34;: 2596 } } ] } { \u0026#34;source\u0026#34;: \u0026#34;10.11.255.1:32767\u0026#34;, \u0026#34;subscription-name\u0026#34;: \u0026#34;default-1714758367\u0026#34;, \u0026#34;timestamp\u0026#34;: 1714758371632000000, \u0026#34;time\u0026#34;: \u0026#34;2024-05-03T20:46:11.632+03:00\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;junos/firewall[name=TEST-FIREWALL]/state\u0026#34;, \u0026#34;updates\u0026#34;: [ { \u0026#34;Path\u0026#34;: \u0026#34;timestamp\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;timestamp\u0026#34;: 1714746658 } }, { \u0026#34;Path\u0026#34;: \u0026#34;memory-usage[name=HEAP]/allocated\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;memory-usage/allocated\u0026#34;: 4004 } }, { \u0026#34;Path\u0026#34;: \u0026#34;counter[name=SSH-PACKETS]/packets\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;counter/packets\u0026#34;: 1162 } }, { \u0026#34;Path\u0026#34;: \u0026#34;counter[name=SSH-PACKETS]/bytes\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;counter/bytes\u0026#34;: 75400 } } ] } { \u0026#34;source\u0026#34;: \u0026#34;10.11.255.1:32767\u0026#34;, \u0026#34;subscription-name\u0026#34;: \u0026#34;default-1714758367\u0026#34;, \u0026#34;timestamp\u0026#34;: 1714758371632000000, \u0026#34;time\u0026#34;: \u0026#34;2024-05-03T20:46:11.632+03:00\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;junos/firewall[name=__default_arp_policer__]/state\u0026#34;, \u0026#34;updates\u0026#34;: [ { \u0026#34;Path\u0026#34;: \u0026#34;timestamp\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;timestamp\u0026#34;: 1714403887 } }, { \u0026#34;Path\u0026#34;: \u0026#34;memory-usage[name=HEAP]/allocated\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;memory-usage/allocated\u0026#34;: 1652 } }, { \u0026#34;Path\u0026#34;: \u0026#34;policer[name=__default_arp_policer__]/out-of-spec-packets\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;policer/out-of-spec-packets\u0026#34;: 0 } }, { \u0026#34;Path\u0026#34;: \u0026#34;policer[name=__default_arp_policer__]/out-of-spec-bytes\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;policer/out-of-spec-bytes\u0026#34;: 0 } }, { \u0026#34;Path\u0026#34;: \u0026#34;policer[name=__default_arp_policer__]/offered-packets\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;policer/offered-packets\u0026#34;: 0 } }, { \u0026#34;Path\u0026#34;: \u0026#34;policer[name=__default_arp_policer__]/offered-bytes\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;policer/offered-bytes\u0026#34;: 0 } }, { \u0026#34;Path\u0026#34;: \u0026#34;policer[name=__default_arp_policer__]/transmitted-packets\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;policer/transmitted-packets\u0026#34;: 0 } }, { \u0026#34;Path\u0026#34;: \u0026#34;policer[name=__default_arp_policer__]/transmitted-bytes\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;policer/transmitted-bytes\u0026#34;: 0 } } ] } { \u0026#34;source\u0026#34;: \u0026#34;10.11.255.1:32767\u0026#34;, \u0026#34;subscription-name\u0026#34;: \u0026#34;default-1714758367\u0026#34;, \u0026#34;timestamp\u0026#34;: 1714758371632000000, \u0026#34;time\u0026#34;: \u0026#34;2024-05-03T20:46:11.632+03:00\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;junos/firewall[name=__flowspec_default_inet__]/state\u0026#34;, \u0026#34;updates\u0026#34;: [ { \u0026#34;Path\u0026#34;: \u0026#34;timestamp\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;timestamp\u0026#34;: 1714753321 } }, { \u0026#34;Path\u0026#34;: \u0026#34;memory-usage[name=HEAP]/allocated\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;memory-usage/allocated\u0026#34;: 7172 } }, { \u0026#34;Path\u0026#34;: \u0026#34;counter[name=10.11.10.10,*,icmp-type=8]/packets\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;counter/packets\u0026#34;: 1315 } }, { \u0026#34;Path\u0026#34;: \u0026#34;counter[name=10.11.10.10,*,icmp-type=8]/bytes\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;counter/bytes\u0026#34;: 1876476 } }, { \u0026#34;Path\u0026#34;: \u0026#34;counter[name=10.11.10.10,*,proto=6,dstport=5201]/packets\u0026#34;, \u0026#34;values\u0026#34;: { \u0026","date":"2024-05-04","objectID":"/posts/kustom-metrics/:3:2","tags":["network","monitoring","telemetry","snmp"],"title":"K~ustom Metrics in Kentik NMS","uri":"/posts/kustom-metrics/#local-configuration-take-1"},{"categories":["network monitoring"],"collections":null,"content":"GRPC Let’s try now to get the same via streaming telemetry and GRPC. According to this we can get those metrics under the /junos/system/linecard/firewall path. Using gnmic (I could only get data via subscriptions and not get rpc calls): $ gnmic -a 10.11.255.1:32767 -u \u0026lt;username\u0026gt;-p \u0026lt;password\u0026gt; --skip-verify sub --mode once --path /junos/system/linecard/firewall { \u0026#34;source\u0026#34;: \u0026#34;10.11.255.1:32767\u0026#34;, \u0026#34;subscription-name\u0026#34;: \u0026#34;default-1714758367\u0026#34;, \u0026#34;timestamp\u0026#34;: 1714758371632000000, \u0026#34;time\u0026#34;: \u0026#34;2024-05-03T20:46:11.632+03:00\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;junos/firewall[name=__default_bpdu_filter__]/state\u0026#34;, \u0026#34;updates\u0026#34;: [ { \u0026#34;Path\u0026#34;: \u0026#34;timestamp\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;timestamp\u0026#34;: 1714746658 } }, { \u0026#34;Path\u0026#34;: \u0026#34;memory-usage[name=HEAP]/allocated\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;memory-usage/allocated\u0026#34;: 2596 } } ] } { \u0026#34;source\u0026#34;: \u0026#34;10.11.255.1:32767\u0026#34;, \u0026#34;subscription-name\u0026#34;: \u0026#34;default-1714758367\u0026#34;, \u0026#34;timestamp\u0026#34;: 1714758371632000000, \u0026#34;time\u0026#34;: \u0026#34;2024-05-03T20:46:11.632+03:00\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;junos/firewall[name=TEST-FIREWALL]/state\u0026#34;, \u0026#34;updates\u0026#34;: [ { \u0026#34;Path\u0026#34;: \u0026#34;timestamp\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;timestamp\u0026#34;: 1714746658 } }, { \u0026#34;Path\u0026#34;: \u0026#34;memory-usage[name=HEAP]/allocated\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;memory-usage/allocated\u0026#34;: 4004 } }, { \u0026#34;Path\u0026#34;: \u0026#34;counter[name=SSH-PACKETS]/packets\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;counter/packets\u0026#34;: 1162 } }, { \u0026#34;Path\u0026#34;: \u0026#34;counter[name=SSH-PACKETS]/bytes\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;counter/bytes\u0026#34;: 75400 } } ] } { \u0026#34;source\u0026#34;: \u0026#34;10.11.255.1:32767\u0026#34;, \u0026#34;subscription-name\u0026#34;: \u0026#34;default-1714758367\u0026#34;, \u0026#34;timestamp\u0026#34;: 1714758371632000000, \u0026#34;time\u0026#34;: \u0026#34;2024-05-03T20:46:11.632+03:00\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;junos/firewall[name=__default_arp_policer__]/state\u0026#34;, \u0026#34;updates\u0026#34;: [ { \u0026#34;Path\u0026#34;: \u0026#34;timestamp\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;timestamp\u0026#34;: 1714403887 } }, { \u0026#34;Path\u0026#34;: \u0026#34;memory-usage[name=HEAP]/allocated\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;memory-usage/allocated\u0026#34;: 1652 } }, { \u0026#34;Path\u0026#34;: \u0026#34;policer[name=__default_arp_policer__]/out-of-spec-packets\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;policer/out-of-spec-packets\u0026#34;: 0 } }, { \u0026#34;Path\u0026#34;: \u0026#34;policer[name=__default_arp_policer__]/out-of-spec-bytes\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;policer/out-of-spec-bytes\u0026#34;: 0 } }, { \u0026#34;Path\u0026#34;: \u0026#34;policer[name=__default_arp_policer__]/offered-packets\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;policer/offered-packets\u0026#34;: 0 } }, { \u0026#34;Path\u0026#34;: \u0026#34;policer[name=__default_arp_policer__]/offered-bytes\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;policer/offered-bytes\u0026#34;: 0 } }, { \u0026#34;Path\u0026#34;: \u0026#34;policer[name=__default_arp_policer__]/transmitted-packets\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;policer/transmitted-packets\u0026#34;: 0 } }, { \u0026#34;Path\u0026#34;: \u0026#34;policer[name=__default_arp_policer__]/transmitted-bytes\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;policer/transmitted-bytes\u0026#34;: 0 } } ] } { \u0026#34;source\u0026#34;: \u0026#34;10.11.255.1:32767\u0026#34;, \u0026#34;subscription-name\u0026#34;: \u0026#34;default-1714758367\u0026#34;, \u0026#34;timestamp\u0026#34;: 1714758371632000000, \u0026#34;time\u0026#34;: \u0026#34;2024-05-03T20:46:11.632+03:00\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;junos/firewall[name=__flowspec_default_inet__]/state\u0026#34;, \u0026#34;updates\u0026#34;: [ { \u0026#34;Path\u0026#34;: \u0026#34;timestamp\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;timestamp\u0026#34;: 1714753321 } }, { \u0026#34;Path\u0026#34;: \u0026#34;memory-usage[name=HEAP]/allocated\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;memory-usage/allocated\u0026#34;: 7172 } }, { \u0026#34;Path\u0026#34;: \u0026#34;counter[name=10.11.10.10,*,icmp-type=8]/packets\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;counter/packets\u0026#34;: 1315 } }, { \u0026#34;Path\u0026#34;: \u0026#34;counter[name=10.11.10.10,*,icmp-type=8]/bytes\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;counter/bytes\u0026#34;: 1876476 } }, { \u0026#34;Path\u0026#34;: \u0026#34;counter[name=10.11.10.10,*,proto=6,dstport=5201]/packets\u0026#34;, \u0026#34;values\u0026#34;: { \u0026","date":"2024-05-04","objectID":"/posts/kustom-metrics/:3:2","tags":["network","monitoring","telemetry","snmp"],"title":"K~ustom Metrics in Kentik NMS","uri":"/posts/kustom-metrics/#local-configuration-take-2"},{"categories":["network monitoring"],"collections":null,"content":"GRPC Let’s try now to get the same via streaming telemetry and GRPC. According to this we can get those metrics under the /junos/system/linecard/firewall path. Using gnmic (I could only get data via subscriptions and not get rpc calls): $ gnmic -a 10.11.255.1:32767 -u \u0026lt;username\u0026gt;-p \u0026lt;password\u0026gt; --skip-verify sub --mode once --path /junos/system/linecard/firewall { \u0026#34;source\u0026#34;: \u0026#34;10.11.255.1:32767\u0026#34;, \u0026#34;subscription-name\u0026#34;: \u0026#34;default-1714758367\u0026#34;, \u0026#34;timestamp\u0026#34;: 1714758371632000000, \u0026#34;time\u0026#34;: \u0026#34;2024-05-03T20:46:11.632+03:00\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;junos/firewall[name=__default_bpdu_filter__]/state\u0026#34;, \u0026#34;updates\u0026#34;: [ { \u0026#34;Path\u0026#34;: \u0026#34;timestamp\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;timestamp\u0026#34;: 1714746658 } }, { \u0026#34;Path\u0026#34;: \u0026#34;memory-usage[name=HEAP]/allocated\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;memory-usage/allocated\u0026#34;: 2596 } } ] } { \u0026#34;source\u0026#34;: \u0026#34;10.11.255.1:32767\u0026#34;, \u0026#34;subscription-name\u0026#34;: \u0026#34;default-1714758367\u0026#34;, \u0026#34;timestamp\u0026#34;: 1714758371632000000, \u0026#34;time\u0026#34;: \u0026#34;2024-05-03T20:46:11.632+03:00\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;junos/firewall[name=TEST-FIREWALL]/state\u0026#34;, \u0026#34;updates\u0026#34;: [ { \u0026#34;Path\u0026#34;: \u0026#34;timestamp\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;timestamp\u0026#34;: 1714746658 } }, { \u0026#34;Path\u0026#34;: \u0026#34;memory-usage[name=HEAP]/allocated\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;memory-usage/allocated\u0026#34;: 4004 } }, { \u0026#34;Path\u0026#34;: \u0026#34;counter[name=SSH-PACKETS]/packets\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;counter/packets\u0026#34;: 1162 } }, { \u0026#34;Path\u0026#34;: \u0026#34;counter[name=SSH-PACKETS]/bytes\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;counter/bytes\u0026#34;: 75400 } } ] } { \u0026#34;source\u0026#34;: \u0026#34;10.11.255.1:32767\u0026#34;, \u0026#34;subscription-name\u0026#34;: \u0026#34;default-1714758367\u0026#34;, \u0026#34;timestamp\u0026#34;: 1714758371632000000, \u0026#34;time\u0026#34;: \u0026#34;2024-05-03T20:46:11.632+03:00\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;junos/firewall[name=__default_arp_policer__]/state\u0026#34;, \u0026#34;updates\u0026#34;: [ { \u0026#34;Path\u0026#34;: \u0026#34;timestamp\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;timestamp\u0026#34;: 1714403887 } }, { \u0026#34;Path\u0026#34;: \u0026#34;memory-usage[name=HEAP]/allocated\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;memory-usage/allocated\u0026#34;: 1652 } }, { \u0026#34;Path\u0026#34;: \u0026#34;policer[name=__default_arp_policer__]/out-of-spec-packets\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;policer/out-of-spec-packets\u0026#34;: 0 } }, { \u0026#34;Path\u0026#34;: \u0026#34;policer[name=__default_arp_policer__]/out-of-spec-bytes\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;policer/out-of-spec-bytes\u0026#34;: 0 } }, { \u0026#34;Path\u0026#34;: \u0026#34;policer[name=__default_arp_policer__]/offered-packets\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;policer/offered-packets\u0026#34;: 0 } }, { \u0026#34;Path\u0026#34;: \u0026#34;policer[name=__default_arp_policer__]/offered-bytes\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;policer/offered-bytes\u0026#34;: 0 } }, { \u0026#34;Path\u0026#34;: \u0026#34;policer[name=__default_arp_policer__]/transmitted-packets\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;policer/transmitted-packets\u0026#34;: 0 } }, { \u0026#34;Path\u0026#34;: \u0026#34;policer[name=__default_arp_policer__]/transmitted-bytes\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;policer/transmitted-bytes\u0026#34;: 0 } } ] } { \u0026#34;source\u0026#34;: \u0026#34;10.11.255.1:32767\u0026#34;, \u0026#34;subscription-name\u0026#34;: \u0026#34;default-1714758367\u0026#34;, \u0026#34;timestamp\u0026#34;: 1714758371632000000, \u0026#34;time\u0026#34;: \u0026#34;2024-05-03T20:46:11.632+03:00\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;junos/firewall[name=__flowspec_default_inet__]/state\u0026#34;, \u0026#34;updates\u0026#34;: [ { \u0026#34;Path\u0026#34;: \u0026#34;timestamp\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;timestamp\u0026#34;: 1714753321 } }, { \u0026#34;Path\u0026#34;: \u0026#34;memory-usage[name=HEAP]/allocated\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;memory-usage/allocated\u0026#34;: 7172 } }, { \u0026#34;Path\u0026#34;: \u0026#34;counter[name=10.11.10.10,*,icmp-type=8]/packets\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;counter/packets\u0026#34;: 1315 } }, { \u0026#34;Path\u0026#34;: \u0026#34;counter[name=10.11.10.10,*,icmp-type=8]/bytes\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;counter/bytes\u0026#34;: 1876476 } }, { \u0026#34;Path\u0026#34;: \u0026#34;counter[name=10.11.10.10,*,proto=6,dstport=5201]/packets\u0026#34;, \u0026#34;values\u0026#34;: { \u0026","date":"2024-05-04","objectID":"/posts/kustom-metrics/:3:2","tags":["network","monitoring","telemetry","snmp"],"title":"K~ustom Metrics in Kentik NMS","uri":"/posts/kustom-metrics/#kentik-portal-1"},{"categories":["network monitoring"],"collections":null,"content":"IOS-XR Flowspec Let\u0026rsquo;s do the same now for the XRs. Since there is no SNMP for flowspec but there is a dedicated yang model for it let’s see what gnmic has to say: get -e json_ietf --path Cisco-IOS-XR-flowspec-oper:flow-spec/vrfs/vrf/afs/af/flows/flow/ [ { \u0026#34;source\u0026#34;: \u0026#34;10.12.255.1\u0026#34;, \u0026#34;timestamp\u0026#34;: 1714928869517180944, \u0026#34;time\u0026#34;: \u0026#34;2024-05-05T20:07:49.517180944+03:00\u0026#34;, \u0026#34;updates\u0026#34;: [ { \u0026#34;Path\u0026#34;: \u0026#34;Cisco-IOS-XR-flowspec-oper:flow-spec/vrfs/vrf[vrf-name=default]/afs/af[af-name=IPv4]/flows/flow[flow-notation=Dest:10.11.10.10/32,Proto:=6,DPort:=5201]\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;flow-spec/vrfs/vrf/afs/af/flows/flow\u0026#34;: { \u0026#34;active-flow-client\u0026#34;: { \u0026#34;action\u0026#34;: [ { \u0026#34;dscp\u0026#34;: 0, \u0026#34;ipv4-nh\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;ipv6-nh\u0026#34;: \u0026#34;::\u0026#34;, \u0026#34;rate\u0026#34;: \u0026#34;40000\u0026#34; } ] }, \u0026#34;flow-notation\u0026#34;: \u0026#34;Dest:10.11.10.10/32,Proto:=6,DPort:=5201\u0026#34;, \u0026#34;flow-statistics\u0026#34;: { \u0026#34;classified\u0026#34;: { \u0026#34;bytes\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;packets\u0026#34;: \u0026#34;0\u0026#34; }, \u0026#34;dropped\u0026#34;: { \u0026#34;bytes\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;packets\u0026#34;: \u0026#34;0\u0026#34; } }, \u0026#34;matches\u0026#34;: { \u0026#34;destination-port\u0026#34;: { \u0026#34;uint16_rng_array\u0026#34;: [ { \u0026#34;max\u0026#34;: 5201, \u0026#34;min\u0026#34;: 5201 } ] }, \u0026#34;destination-prefix-ipv4\u0026#34;: { \u0026#34;mask\u0026#34;: \u0026#34;255.255.255.255\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;10.11.10.10\u0026#34; }, \u0026#34;destination-prefix-ipv6\u0026#34;: { \u0026#34;mask\u0026#34;: 0, \u0026#34;prefix\u0026#34;: \u0026#34;::\u0026#34; }, \u0026#34;icmp\u0026#34;: { \u0026#34;code\u0026#34;: 0, \u0026#34;type\u0026#34;: 0 }, \u0026#34;ip-protocol\u0026#34;: { \u0026#34;uint16_rng_array\u0026#34;: [ { \u0026#34;max\u0026#34;: 6, \u0026#34;min\u0026#34;: 6 } ] }, \u0026#34;source-prefix-ipv4\u0026#34;: { \u0026#34;mask\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;0.0.0.0\u0026#34; }, \u0026#34;source-prefix-ipv6\u0026#34;: { \u0026#34;mask\u0026#34;: 0, \u0026#34;prefix\u0026#34;: \u0026#34;::\u0026#34; }, \u0026#34;tcp-flag\u0026#34;: { \u0026#34;match-any\u0026#34;: false, \u0026#34;value\u0026#34;: 0 } } } } }, { \u0026#34;Path\u0026#34;: \u0026#34;Cisco-IOS-XR-flowspec-oper:flow-spec/vrfs/vrf[vrf-name=default]/afs/af[af-name=IPv4]/flows/flow[flow-notation=Dest:10.11.10.10/32,ICMPType:=8]\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;flow-spec/vrfs/vrf/afs/af/flows/flow\u0026#34;: { \u0026#34;active-flow-client\u0026#34;: { \u0026#34;action\u0026#34;: [ { \u0026#34;dscp\u0026#34;: 0, \u0026#34;ipv4-nh\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;ipv6-nh\u0026#34;: \u0026#34;::\u0026#34;, \u0026#34;rate\u0026#34;: \u0026#34;0\u0026#34; } ] }, \u0026#34;flow-notation\u0026#34;: \u0026#34;Dest:10.11.10.10/32,ICMPType:=8\u0026#34;, \u0026#34;flow-statistics\u0026#34;: { \u0026#34;classified\u0026#34;: { \u0026#34;bytes\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;packets\u0026#34;: \u0026#34;0\u0026#34; }, \u0026#34;dropped\u0026#34;: { \u0026#34;bytes\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;packets\u0026#34;: \u0026#34;0\u0026#34; } }, \u0026#34;matches\u0026#34;: { \u0026#34;destination-prefix-ipv4\u0026#34;: { \u0026#34;mask\u0026#34;: \u0026#34;255.255.255.255\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;10.11.10.10\u0026#34; }, \u0026#34;destination-prefix-ipv6\u0026#34;: { \u0026#34;mask\u0026#34;: 0, \u0026#34;prefix\u0026#34;: \u0026#34;::\u0026#34; }, \u0026#34;icmp\u0026#34;: { \u0026#34;code\u0026#34;: 255, \u0026#34;type\u0026#34;: 8 }, \u0026#34;source-prefix-ipv4\u0026#34;: { \u0026#34;mask\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;0.0.0.0\u0026#34; }, \u0026#34;source-prefix-ipv6\u0026#34;: { \u0026#34;mask\u0026#34;: 0, \u0026#34;prefix\u0026#34;: \u0026#34;::\u0026#34; }, \u0026#34;tcp-flag\u0026#34;: { \u0026#34;match-any\u0026#34;: false, \u0026#34;value\u0026#34;: 0 } } } } } ] } ]And the equivalent list of paths: sub --mode once -e json_ietf --path Cisco-IOS-XR-flowspec-oper:flow-spec/vrfs/vrf/afs/af/flows/flow/ --format event [ { \u0026#34;name\u0026#34;: \u0026#34;default-1714934425\u0026#34;, \u0026#34;timestamp\u0026#34;: 1714934426719000000, \u0026#34;tags\u0026#34;: { \u0026#34;af_af-name\u0026#34;: \u0026#34;IPv4\u0026#34;, \u0026#34;flow_flow-notation\u0026#34;: \u0026#34;Dest:10.11.10.10/32,Proto:=6,DPort:=5201\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;10.12.255.1:57344\u0026#34;, \u0026#34;subscription-name\u0026#34;: \u0026#34;default-1714934425\u0026#34;, \u0026#34;vrf_vrf-name\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;values\u0026#34;: { \u0026#34;Cisco-IOS-XR-flowspec-oper:/flow-spec/vrfs/vrf/afs/af/flows/flow/active-flow-client/action.0/dscp\u0026#34;: 0, \u0026#34;Cisco-IOS-XR-flowspec-oper:/flow-spec/vrfs/vrf/afs/af/flows/flow/active-flow-client/action.0/ipv4-nh\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;Cisco-IOS-XR-flowspec-oper:/fl","date":"2024-05-04","objectID":"/posts/kustom-metrics/:4:0","tags":["network","monitoring","telemetry","snmp"],"title":"K~ustom Metrics in Kentik NMS","uri":"/posts/kustom-metrics/#ios-xr-flowspec"},{"categories":["network monitoring"],"collections":null,"content":"IOS-XR Flowspec Let\u0026rsquo;s do the same now for the XRs. Since there is no SNMP for flowspec but there is a dedicated yang model for it let’s see what gnmic has to say: get -e json_ietf --path Cisco-IOS-XR-flowspec-oper:flow-spec/vrfs/vrf/afs/af/flows/flow/ [ { \u0026#34;source\u0026#34;: \u0026#34;10.12.255.1\u0026#34;, \u0026#34;timestamp\u0026#34;: 1714928869517180944, \u0026#34;time\u0026#34;: \u0026#34;2024-05-05T20:07:49.517180944+03:00\u0026#34;, \u0026#34;updates\u0026#34;: [ { \u0026#34;Path\u0026#34;: \u0026#34;Cisco-IOS-XR-flowspec-oper:flow-spec/vrfs/vrf[vrf-name=default]/afs/af[af-name=IPv4]/flows/flow[flow-notation=Dest:10.11.10.10/32,Proto:=6,DPort:=5201]\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;flow-spec/vrfs/vrf/afs/af/flows/flow\u0026#34;: { \u0026#34;active-flow-client\u0026#34;: { \u0026#34;action\u0026#34;: [ { \u0026#34;dscp\u0026#34;: 0, \u0026#34;ipv4-nh\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;ipv6-nh\u0026#34;: \u0026#34;::\u0026#34;, \u0026#34;rate\u0026#34;: \u0026#34;40000\u0026#34; } ] }, \u0026#34;flow-notation\u0026#34;: \u0026#34;Dest:10.11.10.10/32,Proto:=6,DPort:=5201\u0026#34;, \u0026#34;flow-statistics\u0026#34;: { \u0026#34;classified\u0026#34;: { \u0026#34;bytes\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;packets\u0026#34;: \u0026#34;0\u0026#34; }, \u0026#34;dropped\u0026#34;: { \u0026#34;bytes\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;packets\u0026#34;: \u0026#34;0\u0026#34; } }, \u0026#34;matches\u0026#34;: { \u0026#34;destination-port\u0026#34;: { \u0026#34;uint16_rng_array\u0026#34;: [ { \u0026#34;max\u0026#34;: 5201, \u0026#34;min\u0026#34;: 5201 } ] }, \u0026#34;destination-prefix-ipv4\u0026#34;: { \u0026#34;mask\u0026#34;: \u0026#34;255.255.255.255\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;10.11.10.10\u0026#34; }, \u0026#34;destination-prefix-ipv6\u0026#34;: { \u0026#34;mask\u0026#34;: 0, \u0026#34;prefix\u0026#34;: \u0026#34;::\u0026#34; }, \u0026#34;icmp\u0026#34;: { \u0026#34;code\u0026#34;: 0, \u0026#34;type\u0026#34;: 0 }, \u0026#34;ip-protocol\u0026#34;: { \u0026#34;uint16_rng_array\u0026#34;: [ { \u0026#34;max\u0026#34;: 6, \u0026#34;min\u0026#34;: 6 } ] }, \u0026#34;source-prefix-ipv4\u0026#34;: { \u0026#34;mask\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;0.0.0.0\u0026#34; }, \u0026#34;source-prefix-ipv6\u0026#34;: { \u0026#34;mask\u0026#34;: 0, \u0026#34;prefix\u0026#34;: \u0026#34;::\u0026#34; }, \u0026#34;tcp-flag\u0026#34;: { \u0026#34;match-any\u0026#34;: false, \u0026#34;value\u0026#34;: 0 } } } } }, { \u0026#34;Path\u0026#34;: \u0026#34;Cisco-IOS-XR-flowspec-oper:flow-spec/vrfs/vrf[vrf-name=default]/afs/af[af-name=IPv4]/flows/flow[flow-notation=Dest:10.11.10.10/32,ICMPType:=8]\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;flow-spec/vrfs/vrf/afs/af/flows/flow\u0026#34;: { \u0026#34;active-flow-client\u0026#34;: { \u0026#34;action\u0026#34;: [ { \u0026#34;dscp\u0026#34;: 0, \u0026#34;ipv4-nh\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;ipv6-nh\u0026#34;: \u0026#34;::\u0026#34;, \u0026#34;rate\u0026#34;: \u0026#34;0\u0026#34; } ] }, \u0026#34;flow-notation\u0026#34;: \u0026#34;Dest:10.11.10.10/32,ICMPType:=8\u0026#34;, \u0026#34;flow-statistics\u0026#34;: { \u0026#34;classified\u0026#34;: { \u0026#34;bytes\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;packets\u0026#34;: \u0026#34;0\u0026#34; }, \u0026#34;dropped\u0026#34;: { \u0026#34;bytes\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;packets\u0026#34;: \u0026#34;0\u0026#34; } }, \u0026#34;matches\u0026#34;: { \u0026#34;destination-prefix-ipv4\u0026#34;: { \u0026#34;mask\u0026#34;: \u0026#34;255.255.255.255\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;10.11.10.10\u0026#34; }, \u0026#34;destination-prefix-ipv6\u0026#34;: { \u0026#34;mask\u0026#34;: 0, \u0026#34;prefix\u0026#34;: \u0026#34;::\u0026#34; }, \u0026#34;icmp\u0026#34;: { \u0026#34;code\u0026#34;: 255, \u0026#34;type\u0026#34;: 8 }, \u0026#34;source-prefix-ipv4\u0026#34;: { \u0026#34;mask\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;0.0.0.0\u0026#34; }, \u0026#34;source-prefix-ipv6\u0026#34;: { \u0026#34;mask\u0026#34;: 0, \u0026#34;prefix\u0026#34;: \u0026#34;::\u0026#34; }, \u0026#34;tcp-flag\u0026#34;: { \u0026#34;match-any\u0026#34;: false, \u0026#34;value\u0026#34;: 0 } } } } } ] } ]And the equivalent list of paths: sub --mode once -e json_ietf --path Cisco-IOS-XR-flowspec-oper:flow-spec/vrfs/vrf/afs/af/flows/flow/ --format event [ { \u0026#34;name\u0026#34;: \u0026#34;default-1714934425\u0026#34;, \u0026#34;timestamp\u0026#34;: 1714934426719000000, \u0026#34;tags\u0026#34;: { \u0026#34;af_af-name\u0026#34;: \u0026#34;IPv4\u0026#34;, \u0026#34;flow_flow-notation\u0026#34;: \u0026#34;Dest:10.11.10.10/32,Proto:=6,DPort:=5201\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;10.12.255.1:57344\u0026#34;, \u0026#34;subscription-name\u0026#34;: \u0026#34;default-1714934425\u0026#34;, \u0026#34;vrf_vrf-name\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;values\u0026#34;: { \u0026#34;Cisco-IOS-XR-flowspec-oper:/flow-spec/vrfs/vrf/afs/af/flows/flow/active-flow-client/action.0/dscp\u0026#34;: 0, \u0026#34;Cisco-IOS-XR-flowspec-oper:/flow-spec/vrfs/vrf/afs/af/flows/flow/active-flow-client/action.0/ipv4-nh\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;Cisco-IOS-XR-flowspec-oper:/fl","date":"2024-05-04","objectID":"/posts/kustom-metrics/:4:0","tags":["network","monitoring","telemetry","snmp"],"title":"K~ustom Metrics in Kentik NMS","uri":"/posts/kustom-metrics/#local-configuration-1"},{"categories":["network monitoring"],"collections":null,"content":"IOS-XR Flowspec Let\u0026rsquo;s do the same now for the XRs. Since there is no SNMP for flowspec but there is a dedicated yang model for it let’s see what gnmic has to say: get -e json_ietf --path Cisco-IOS-XR-flowspec-oper:flow-spec/vrfs/vrf/afs/af/flows/flow/ [ { \u0026#34;source\u0026#34;: \u0026#34;10.12.255.1\u0026#34;, \u0026#34;timestamp\u0026#34;: 1714928869517180944, \u0026#34;time\u0026#34;: \u0026#34;2024-05-05T20:07:49.517180944+03:00\u0026#34;, \u0026#34;updates\u0026#34;: [ { \u0026#34;Path\u0026#34;: \u0026#34;Cisco-IOS-XR-flowspec-oper:flow-spec/vrfs/vrf[vrf-name=default]/afs/af[af-name=IPv4]/flows/flow[flow-notation=Dest:10.11.10.10/32,Proto:=6,DPort:=5201]\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;flow-spec/vrfs/vrf/afs/af/flows/flow\u0026#34;: { \u0026#34;active-flow-client\u0026#34;: { \u0026#34;action\u0026#34;: [ { \u0026#34;dscp\u0026#34;: 0, \u0026#34;ipv4-nh\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;ipv6-nh\u0026#34;: \u0026#34;::\u0026#34;, \u0026#34;rate\u0026#34;: \u0026#34;40000\u0026#34; } ] }, \u0026#34;flow-notation\u0026#34;: \u0026#34;Dest:10.11.10.10/32,Proto:=6,DPort:=5201\u0026#34;, \u0026#34;flow-statistics\u0026#34;: { \u0026#34;classified\u0026#34;: { \u0026#34;bytes\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;packets\u0026#34;: \u0026#34;0\u0026#34; }, \u0026#34;dropped\u0026#34;: { \u0026#34;bytes\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;packets\u0026#34;: \u0026#34;0\u0026#34; } }, \u0026#34;matches\u0026#34;: { \u0026#34;destination-port\u0026#34;: { \u0026#34;uint16_rng_array\u0026#34;: [ { \u0026#34;max\u0026#34;: 5201, \u0026#34;min\u0026#34;: 5201 } ] }, \u0026#34;destination-prefix-ipv4\u0026#34;: { \u0026#34;mask\u0026#34;: \u0026#34;255.255.255.255\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;10.11.10.10\u0026#34; }, \u0026#34;destination-prefix-ipv6\u0026#34;: { \u0026#34;mask\u0026#34;: 0, \u0026#34;prefix\u0026#34;: \u0026#34;::\u0026#34; }, \u0026#34;icmp\u0026#34;: { \u0026#34;code\u0026#34;: 0, \u0026#34;type\u0026#34;: 0 }, \u0026#34;ip-protocol\u0026#34;: { \u0026#34;uint16_rng_array\u0026#34;: [ { \u0026#34;max\u0026#34;: 6, \u0026#34;min\u0026#34;: 6 } ] }, \u0026#34;source-prefix-ipv4\u0026#34;: { \u0026#34;mask\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;0.0.0.0\u0026#34; }, \u0026#34;source-prefix-ipv6\u0026#34;: { \u0026#34;mask\u0026#34;: 0, \u0026#34;prefix\u0026#34;: \u0026#34;::\u0026#34; }, \u0026#34;tcp-flag\u0026#34;: { \u0026#34;match-any\u0026#34;: false, \u0026#34;value\u0026#34;: 0 } } } } }, { \u0026#34;Path\u0026#34;: \u0026#34;Cisco-IOS-XR-flowspec-oper:flow-spec/vrfs/vrf[vrf-name=default]/afs/af[af-name=IPv4]/flows/flow[flow-notation=Dest:10.11.10.10/32,ICMPType:=8]\u0026#34;, \u0026#34;values\u0026#34;: { \u0026#34;flow-spec/vrfs/vrf/afs/af/flows/flow\u0026#34;: { \u0026#34;active-flow-client\u0026#34;: { \u0026#34;action\u0026#34;: [ { \u0026#34;dscp\u0026#34;: 0, \u0026#34;ipv4-nh\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;ipv6-nh\u0026#34;: \u0026#34;::\u0026#34;, \u0026#34;rate\u0026#34;: \u0026#34;0\u0026#34; } ] }, \u0026#34;flow-notation\u0026#34;: \u0026#34;Dest:10.11.10.10/32,ICMPType:=8\u0026#34;, \u0026#34;flow-statistics\u0026#34;: { \u0026#34;classified\u0026#34;: { \u0026#34;bytes\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;packets\u0026#34;: \u0026#34;0\u0026#34; }, \u0026#34;dropped\u0026#34;: { \u0026#34;bytes\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;packets\u0026#34;: \u0026#34;0\u0026#34; } }, \u0026#34;matches\u0026#34;: { \u0026#34;destination-prefix-ipv4\u0026#34;: { \u0026#34;mask\u0026#34;: \u0026#34;255.255.255.255\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;10.11.10.10\u0026#34; }, \u0026#34;destination-prefix-ipv6\u0026#34;: { \u0026#34;mask\u0026#34;: 0, \u0026#34;prefix\u0026#34;: \u0026#34;::\u0026#34; }, \u0026#34;icmp\u0026#34;: { \u0026#34;code\u0026#34;: 255, \u0026#34;type\u0026#34;: 8 }, \u0026#34;source-prefix-ipv4\u0026#34;: { \u0026#34;mask\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;0.0.0.0\u0026#34; }, \u0026#34;source-prefix-ipv6\u0026#34;: { \u0026#34;mask\u0026#34;: 0, \u0026#34;prefix\u0026#34;: \u0026#34;::\u0026#34; }, \u0026#34;tcp-flag\u0026#34;: { \u0026#34;match-any\u0026#34;: false, \u0026#34;value\u0026#34;: 0 } } } } } ] } ]And the equivalent list of paths: sub --mode once -e json_ietf --path Cisco-IOS-XR-flowspec-oper:flow-spec/vrfs/vrf/afs/af/flows/flow/ --format event [ { \u0026#34;name\u0026#34;: \u0026#34;default-1714934425\u0026#34;, \u0026#34;timestamp\u0026#34;: 1714934426719000000, \u0026#34;tags\u0026#34;: { \u0026#34;af_af-name\u0026#34;: \u0026#34;IPv4\u0026#34;, \u0026#34;flow_flow-notation\u0026#34;: \u0026#34;Dest:10.11.10.10/32,Proto:=6,DPort:=5201\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;10.12.255.1:57344\u0026#34;, \u0026#34;subscription-name\u0026#34;: \u0026#34;default-1714934425\u0026#34;, \u0026#34;vrf_vrf-name\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;values\u0026#34;: { \u0026#34;Cisco-IOS-XR-flowspec-oper:/flow-spec/vrfs/vrf/afs/af/flows/flow/active-flow-client/action.0/dscp\u0026#34;: 0, \u0026#34;Cisco-IOS-XR-flowspec-oper:/flow-spec/vrfs/vrf/afs/af/flows/flow/active-flow-client/action.0/ipv4-nh\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;Cisco-IOS-XR-flowspec-oper:/fl","date":"2024-05-04","objectID":"/posts/kustom-metrics/:4:0","tags":["network","monitoring","telemetry","snmp"],"title":"K~ustom Metrics in Kentik NMS","uri":"/posts/kustom-metrics/#kentik-portal-2"},{"categories":["network monitoring"],"collections":null,"content":"Outro Well we covered enough ground to demonstrate what is possible currently with Kentik NMS when it comes to custom metrics: How to enable Kentik Universal Agent to poll for specific metrics from devices Polling and reporting on a custom SNMP Table Polling and reporting on a custom ST path via GRPC Associating custom metrics to specific device types ...till next time...have fun!!! ","date":"2024-05-04","objectID":"/posts/kustom-metrics/:5:0","tags":["network","monitoring","telemetry","snmp"],"title":"K~ustom Metrics in Kentik NMS","uri":"/posts/kustom-metrics/#outro"},{"categories":["network monitoring"],"collections":null,"content":"Influences and Reference How to Configure Kentik NMS to Collect Custom SNMP Metrics Adding Multiple Custom Metrics to Kentik NMS Adjusting Data Before Sending It to Kentik NMS Repo for this post ","date":"2024-05-04","objectID":"/posts/kustom-metrics/:6:0","tags":["network","monitoring","telemetry","snmp"],"title":"K~ustom Metrics in Kentik NMS","uri":"/posts/kustom-metrics/#influences-and-reference"},{"categories":["telemetry"],"collections":null,"content":"One of the mostly raised questions when you start leveraging MDT is which YANG model and xpath to use to configure the subscription on the device. Up to now, there was little to none help from the box itself, and a lot of searching and experimentation on the models was needed. Apparently this has changed now, and there are two sets of commands that will help in your quest. Let\u0026rsquo;s jump on DevNet\u0026rsquo;s always-on XRv9000 sandbox and issue some commands. $ ssh admin@sandbox-iosxr-1.cisco.com Password: RP/0/RP0/CPU0:ansible-iosxr# RP/0/RP0/CPU0:ansible-iosxr#show version Sun Jul 10 22:02:49.475 UTC Cisco IOS XR Software, Version 7.3.2 Copyright (c) 2013-2021 by Cisco Systems, Inc. Build Information: Built By : ingunawa Built On : Wed Oct 13 20:00:36 PDT 2021 Built Host : iox-ucs-017 Workspace : /auto/srcarchive17/prod/7.3.2/xrv9k/ws Version : 7.3.2 Location : /opt/cisco/XR/packages/ Label : 7.3.2-0 cisco IOS-XRv 9000 () processor System uptime is 2 weeks 5 days 10 hours 35 minutesLet\u0026rsquo;s say we want to find out which model and xpath will give us the sensors for show interfaces. There was the old command schema-describe, but now there is a new kid in town called show telemetry internal that does the trick. RP/0/RP0/CPU0:ansible-iosxr#schema-describe \u0026#34;show interfaces\u0026#34; Sun Jul 10 22:09:54.304 UTC Action: get Path: RootOper.Interfaces.Interface RP/0/RP0/CPU0:ansible-iosxr#show telemetry internal xpath \u0026#34;show interfaces\u0026#34; Sun Jul 10 22:11:04.759 UTC Cisco-IOS-XR-pfi-im-cmd-oper:interfaces/interface-xr/interfaceSo now, we\u0026rsquo;ve got the model and xpath, let\u0026rsquo;s see what will be streamed for just the Bundle interfaces Or, by using the old method: Now, let\u0026rsquo;s try to start from an openconfig model. That is all! Keep a note on the old commands that exists: schema-describe \u0026ldquo;show command\u0026rdquo; run mdt_get_gather_paths \u0026lt;yang model\u0026gt; run mdt_exec -s \u0026lt;xpath\u0026gt; -c \u0026lt;sample interval in msec\u0026gt; And the new one: RP/0/RP0/CPU0:ansible-iosxr#show telemetry internal ? json Display yang sensor paths data in json format xpath Display yang sensor pathsFind out more on xrdocs.io, in the telemetry tutorials ...till next time...have fun! ","date":"2022-07-11","objectID":"/posts/xr-get-xpath/:0:0","tags":["ios-xr","telemetry"],"title":"IOS-XR :: Get Your XPATHs, Get Your XPATHs Honey","uri":"/posts/xr-get-xpath/#"},{"categories":["ansible"],"collections":null,"content":"Intro Throughout this post, I am documenting my ansible orientation and ramping-up process towards automating the provisioning of a k8s cluster on ubuntu linux. I believe it is not something too fancy or something that has not been visited over and over again in various other posts, but this is the scrub of my exposure with ansible automation. I tried to make it modular with several playbook imports and task includes executed from a parent playbook file, rather than having a single long playbook. Ansible roles are not leveraged \u0026#x1f601;. The code for this project can be found here. ","date":"2022-07-06","objectID":"/posts/k8s-ansible/:1:0","tags":["k8s","ansible","automation"],"title":"k8s Ansible","uri":"/posts/k8s-ansible/#intro"},{"categories":["ansible"],"collections":null,"content":"The Intent Let\u0026rsquo;s begin by setting the in-scope items for the project: Ubuntu 20.04 VMs based on cloud images provisioned with a known username/passwd and assigned DHCP IPv4 addresses. (Setting up the VMs reference) Kubernetes single node cluster version 1.22 Docker CRI Flannel CNI Single playbook which calls subsequent playbooks and task lists at runtime Kubeadm for cluster setup ","date":"2022-07-06","objectID":"/posts/k8s-ansible/:2:0","tags":["k8s","ansible","automation"],"title":"k8s Ansible","uri":"/posts/k8s-ansible/#the-intent"},{"categories":["ansible"],"collections":null,"content":"Playbook Overview The following mindmap diagram represents the automation tasks flows. Red lines represent imported playbook files. Green lines are included task lists files, and blue lines are the actual tasks to be executed. k8s Ansible Playbook Flow In short, we start from a single playbook file called start.yml and we have broken down our flow in three phases. The init phase covers Steps 01 and 02, and refer to assigning static IPs to our dynamic VMs. Step 01 is about building the mapping from DHCP to Static IPs of our nodes and generating the relevant netplan configuration files using a jinja2 template. Step 02 covers the application of these configuration files to the VMs. Next, is the pre-deployment phase, or Step 03, which is about preparing the VMs for k8s deployment, i.e. setting hostnames, establishing SSH key authentication with ansible, upgrading, etc. The final phase is the deployment one, where we call our tasks to deploy the k8s cluster upon the relevant control and worker nodes. The check phase, although not depicted in the flow, is a very important one during which we do our environment pre-checks in order for the playbook to be executed successfully, i.e. are all hosts declared properly in the ansible hosts file, are all variables declared as they should be,etc. Let\u0026rsquo;s expand on the directory structure in order to make things more explicit. ","date":"2022-07-06","objectID":"/posts/k8s-ansible/:3:0","tags":["k8s","ansible","automation"],"title":"k8s Ansible","uri":"/posts/k8s-ansible/#playbook-overview"},{"categories":["ansible"],"collections":null,"content":"Directory Layout Here is how the tree of our ansible playbook home directory looks like: . ├── .secrets -------------------\u0026lt; Custom directory to hold sensitive data │ ├── passwd.yml -------------\u0026lt; Encrypted secrets file │ └── vault_passwd -----------\u0026lt; Ansible-vault clear text password ├── ansible.cfg ----------------\u0026lt; ansible configuration file ├── checks.yml -----------------\u0026lt; \u0026#34;Pre-checks\u0026#34; playbook - Step 00 ├── configs --------------------\u0026lt; Directory to store generated static ip netplan files ├── deployment.yml -------------\u0026lt; \u0026#34;Deploy\u0026#34; phase playbook - Step 04 ├── hosts ----------------------\u0026lt; ansible inventory file in ini format ├── init_play.yml --------------\u0026lt; \u0026#34;Init\u0026#34; phase playbook - Step 01 ├── pre_deploy.yml -------------\u0026lt; \u0026#34;Pre-deploy\u0026#34; phase playbook - Step 03 ├── start.yml ---------------\u0026gt;\u0026gt;\u0026gt;\u0026gt; \u0026#34;Master\u0026#34; playbook ├── tasks ----------------------\u0026lt; Folder to hold all task lists files arranged per phase │ ├── deploy │ │ ├── boot_control.yml │ │ ├── cni.yml │ │ ├── cri.yml │ │ ├── packages_general.yml │ │ ├── packages_kube.yml │ │ ├── swap.yml │ │ └── workers.yml │ ├── init │ │ └── apply_netplan.yml --\u0026lt; Step 02 playbook │ └── pre │ ├── cloud-init.yml │ ├── hostname.yml │ ├── hosts.yml │ ├── reboot.yml │ ├── sshkeys.yml │ ├── sudo.yml │ └── upgrade.yml ├── templates ------------------\u0026lt; Templates default folder │ └── netplan.j2 └── vars -----------------------\u0026lt; Custom directory to host variables └── netplan.ymlBelow is the ansible configuration file where we define which file contains the inventory. We instruct ansible not complain if the SSH target hosts are not known. For facts gathering, we only need the network facts and not the hardware ones. We define where is the encryption password for ansible-vault. The callback_whitelist is very useful if we want to see timing statistics of our playbook execution. [defaults] inventory = hosts host_key_checking = False gather_subset=!hardware, network vault_password_file=.secrets/vault_passwd #callback_whitelist = timer, profile_tasksansible.cfg ","date":"2022-07-06","objectID":"/posts/k8s-ansible/:3:1","tags":["k8s","ansible","automation"],"title":"k8s Ansible","uri":"/posts/k8s-ansible/#directory-layout"},{"categories":["ansible"],"collections":null,"content":"Ansible key files overview Here is our inventory file: [all:vars] ansible_ssh_common_args=\u0026#39;-o UserKnownHostsFile=/dev/null\u0026#39; ansible_user=ubuntu ansible_ssh_private_key_file=~/.ssh/id_ansible k8s_version=1.22.10 k8s_pod_cidr=10.244.0.0/16 [control] kates-control ansible_host=192.168.1.40 [workers] kates-node-01 ansible_host=192.168.1.41 kates-node-02 ansible_host=192.168.1.42 kates-node-03 ansible_host=192.168.1.43 [dhcp_hosts] 192.168.1.203 192.168.1.206 192.168.1.208 192.168.1.209hosts We split our targets into three groups and declare common variables in the all section. The ansible_ssh_common_args variable prevents ansible to complain about host key changes, since these VMs were re-provisioned a lot of times during testing the playbook and were getting the same DHCP addresses. We declare here the SSH user that is used for the initial connection to the dynamic hosts till the point that these hosts are assigned static IPs and configured with an SSH public key in order for ansible to continue further with the tasks. The passwd.yml file contains the passwords that ansible will use for SSH and sudo access on the targets. These are used initially and then publickey and sudoless access is enabled. $ ansible-vault view .secrets/passwd.yml ansible_sudo_pass: \u0026#34;ubuntu123\u0026#34; ansible_become_pass: \u0026#34;ubuntu123\u0026#34; ansible_ssh_pass: \u0026#34;ubuntu123\u0026#34;.secrets/passwd.yml Let\u0026rsquo;s now check the variables file that will be loaded during play runtime. --- netplan: gateway4: 192.168.1.1 subnet: 24 dns: - 192.168.1.1 - 8.8.8.8vars/netplan.yml It is a dictionary containing all variables needed to populate the jinja2 template and produce the respective netplan configuration file that will enable assigning static ip to our hosts. Here is the jinja2 template file: network: ethernets: {{ item.intf }}: dhcp4: false dhcp6: false addresses: [{{ item.newIP }}/{{ netplan.subnet }}] gateway4: {{ netplan.gateway4 }} nameservers: addresses: [{% for dns in netplan.dns %}{{dns}}{%- if not loop.last %}, {% endif %}{% endfor %}] version: 2templates/netplan.j2 So, this template will iterate over a list of dictionaries that will be created in runtime containing a mapping of dynamic IP to static one along with the netplan variables for each hosts. The outcome will be a netplan config file that will be placed and applied on the target host. Note For DNS entries, since we can have more than one, we append a comma if it is not the last item in order to build the list. Last but not least, here is the master playbook, or playbook zero that I like to call it. #!/usr/bin/env ansible-playbook --- - name: ONE PLAY TO CONTROL ALL hosts: all gather_facts: no - name: \u0026#34;\u0026lt;! ################# CHECK :: PRE-FLIGHT CHECKING ################### !\u0026gt;\u0026#34; import_playbook: checks.yml - name: \u0026#34;\u0026lt;! ################# INIT :: PREPARE FOR STATIC IPs ################# !\u0026gt;\u0026#34; import_playbook: init_play.yml - name: \u0026#34;\u0026lt;! ################# INIT :: CHANGE TO STATIC IPs ################### !\u0026gt;\u0026#34; import_playbook: tasks/init/apply_netplan.yml - name: \u0026#34;\u0026lt;! ################# PRE :: PERFORM PRE-DEPLOYMENT TASKS ############ !\u0026gt;\u0026#34; import_playbook: pre_deploy.yml - name: \u0026#34;\u0026lt;! ################# DEPLOY :: PERFORM DEPLOYMENT TASKS ############# !\u0026gt;\u0026#34; import_playbook: deployment.ymlstart.yml Pretty straight forward. It is just importing all other playbooks one by one \u0026#x1f604; ","date":"2022-07-06","objectID":"/posts/k8s-ansible/:3:2","tags":["k8s","ansible","automation"],"title":"k8s Ansible","uri":"/posts/k8s-ansible/#ansible-key-files-overview"},{"categories":["ansible"],"collections":null,"content":"Ansible modules in play The following table lists all ansible modules used in this project in alphabetical order along with a short description on their use. Short Name FQCN Description apt ansible.builtin.apt Manages apt packages in Debian/Ubuntu distros apt_key ansible.builtin.apt_key Manages keys in apt keyring apt_repository ansible.builtin.apt_repository Manages apt repositories assert ansible.builtin.assert Evaluates if given expressions are true authorized_key ansible.posix.authorized_key Manages SSH authorized keys of user accounts blockinfile ansible.builtin.blockinfile Manages a block of multi-line text surrounded by customizable marker lines in files command ansible.builtin.command Executes commands on linux targets without invoking a shell copy ansible.builtin.copy Copy files to remote linux locations debug ansible.builtin.debug Print statements during execution and can be useful for debugging variables or expressions without necessarily halting the playbook file ansible.builtin.file Manage files, attributes of files, symlinks or directories in linux nodes hostname ansible.builtin.hostname Set the hostname in most linux distros import_playbook ansible.builtin.import_playbook Imports a playbook file in the current playbook for execution include_tasks ansible.builtin.include_tasks Dynamically includes a file with a list of tasks to be executed in the current playbook include_vars ansible.builtin.include_vars Loads YAML/JSON variables dynamically from a file or directory, recursively, during task runtime lineinfile ansible.builtin.lineinfile Manage lines in a text file meta ansible.builtin.meta Execute ansible actions openssh_keypair community.crypto.openssh_keypair (Re)Generate OpenSSH private and public keys reboot ansible.builtin.reboot Reboot a machine, wait for it to go down, come back up, and respond to commands replace ansible.builtin.replace Replace all instances of a particular string in a file using a back-referenced regular expression set_fact ansible.builtin.set_fact Set host variable(s) and fact(s) setup ansible.builtin.setup Gathers facts about remote hosts shell ansible.builtin.shell Execute shell commands on targets stat ansible.builtin.stat Retrieve file or file system status systemd ansible.builtin.systemd Manage systemd units template ansible.builtin.template Template a file out to a target host using jinja2 user ansible.builtin.user Manage user accounts and user attributes wait_for ansible.builtin.wait_for Waits for a condition before continuing ","date":"2022-07-06","objectID":"/posts/k8s-ansible/:3:3","tags":["k8s","ansible","automation"],"title":"k8s Ansible","uri":"/posts/k8s-ansible/#ansible-modules-in-play"},{"categories":["ansible"],"collections":null,"content":"Phased Execution Let\u0026rsquo;s delve a bit deeper on the phases now. We will just analyse some key tasks to make the logic more explicit. ","date":"2022-07-06","objectID":"/posts/k8s-ansible/:4:0","tags":["k8s","ansible","automation"],"title":"k8s Ansible","uri":"/posts/k8s-ansible/#phased-execution"},{"categories":["ansible"],"collections":null,"content":"The check phase As previously states, during this phase we are aiming to check if all variables, files, and configurations are how they are supposed to be, or better how are playbooks want them to be in order to execute correctly. The most popular modules used here are those of debug and assert. Let\u0026rsquo;s see for example a simple check on the inventory groups: - name: INVENTORY CHECK - SINGLE CONTROL NODE debug: msg: \u0026#34;control group is not correctly populated for a single (1) control node\u0026#34; when: (not groups.control) or (groups.control|length \u0026gt; 1) failed_when: (not groups.control) or (groups.control|length \u0026gt; 1)What we do here is making sure the group control is defined and it contains only one target host definition, since we are deploying a single control node cluster. This task will be executed only when the condition we are checking is false and will be declared as failed using the failed_when clause. Another simple example is using assert: - name: CHECK QUANTITIES assert: that: groups.dhcp_hosts|length == (groups.control+groups.workers)|length quiet: yes fail_msg: \u0026#34;Number of DHCP hosts cannot be different of number of K8s nodes\u0026#34;Here, we are checking that we have the same number of dynamic hosts defined as the total number of k8s nodes, workers and control nodes together, since there will be a one-to-one mapping in the configuration and deployment. Now, let\u0026rsquo;s see how we check the variables file: - name: CHECK FOR NETPLAN VARIABLES FILE stat: path: \u0026#34;{{ lookup(\u0026#39;ansible.builtin.env\u0026#39;, \u0026#39;PWD\u0026#39;) }}/vars/netplan.yml\u0026#34; register: netplan_file - debug: msg=\u0026#34;Netplan variables file is not found\u0026#34; when: not netplan_file.stat.exists failed_when: not netplan_file.stat.exists - name: INCLUDING NETPLAN VARIABLES include_vars: dir: \u0026#34;{{ lookup(\u0026#39;ansible.builtin.env\u0026#39;, \u0026#39;PWD\u0026#39;) }}/vars\u0026#34; files_matching: netplan.yml - name: CHECKING NETPLAN VARIABLES assert: that: - netplan is defined and netplan - netplan.gateway4 is defined and netplan.gateway4 - netplan.subnet is defined and netplan.subnet|int \u0026gt; 0 and netplan.subnet \u0026lt;= 31 - netplan.dns is defined and netplan.dns quiet: yes fail_msg: \u0026#34;Make sure all netplan variables are defined correctly\u0026#34; success_msg: \u0026#34;Looks Good!\u0026#34;So, we use stat and debug to make sure that the file exists in the correct directory. Then we load it in the play with include_vars so we can have access to the variables, and finally we check the values with assert. Finally, using wait_for we check if there is connectivity to the target hosts and act accordingly. We need the dynamic_hosts to be alive and the k8s node\u0026rsquo;s IPs to be unreachable. - name: CHECK SSH CONNECTIVITY TO DHCP HOSTS wait_for: port: 22 host: \u0026#34;{{item}}\u0026#34; search_regex: OpenSSH delay: 0 timeout: 2 loop: \u0026#34;{{groups.dhcp_hosts}}\u0026#34; - name: CHECK SSH CONNECTIVITY TO K8S NODES wait_for: port: 22 host: \u0026#34;{{hostvars[item].ansible_host}}\u0026#34; search_regex: OpenSSH delay: 0 timeout: 2 msg: Should not be alive state: stopped loop: \u0026#34;{{groups.control+groups.workers}}\u0026#34; ","date":"2022-07-06","objectID":"/posts/k8s-ansible/:4:1","tags":["k8s","ansible","automation"],"title":"k8s Ansible","uri":"/posts/k8s-ansible/#the-check-phase"},{"categories":["ansible"],"collections":null,"content":"The init phase (Steps 01 and 02) This phase calls two playbook files. As Step 01, we build the dynamic host mapping to static k8s node and we create the netplan config files for the hosts locally, thus this playbook is run against localhost. As Step 02, we upload the netplan files and apply the new configuration on the targets. Here is the playbook for the first step: #!/usr/bin/env ansible-playbook --- - name: \u0026#34;\u0026lt;! ################# INIT :: PREPARE FOR STATIC IPs ################# !\u0026gt;\u0026#34; hosts: localhost gather_facts: yes vars_files: - .secrets/passwd.yml - vars/netplan.yml tasks: - name: GET FACTS OF DHCP NODES setup: delegate_to: \u0026#34;{{ item }}\u0026#34; delegate_facts: true loop: \u0026#34;{{groups.dhcp_hosts}}\u0026#34; - name: PREPARE STATIC MAPPINGS set_fact: dhcp_map=\u0026#34;{{ dhcp_map|default([]) + [{ \u0026#39;oldIP\u0026#39;:hostvars[item.0].ansible_default_ipv4.address, \u0026#39;intf\u0026#39;:hostvars[item.0].ansible_default_ipv4.interface, \u0026#39;newIP\u0026#39;:hostvars[item.1].ansible_host }] }}\u0026#34; loop: \u0026#34;{{groups.dhcp_hosts|zip(groups.control+groups.workers)|list}}\u0026#34; - name: GENERATE NETPLAN CONFIGS template: src: netplan.j2 dest: configs/netplan.{{item.oldIP}} loop: \u0026#34;{{dhcp_map}}\u0026#34; init_play.yml We use the setup module to connect to dhcp_hosts and gather their facts. We are interested in their IPv4 assigned address and the network interface name. We then use set_fact to build our list of dictionaries in order to use it for jinja2 template config creation. A sample of the list of dictionaries looks like this: Once the list is populated, we use the template module to get the configuration files and output them in the configs directory appending the \u0026ldquo;OldIP\u0026rdquo; to the filename. As a second step, we apply the configs to our target hosts by getting the netplan filename and replacing it with the respective netplan config file generated in the previous step. We make the change and wait for connectivity to the static IPs. #!/usr/bin/env ansible-playbook --- - name: \u0026#34;\u0026lt;! ################# INIT :: CHANGE TO STATIC IPs ################### !\u0026gt;\u0026#34; hosts: dhcp_hosts gather_facts: yes vars_files: - \u0026#34;{{inventory_dir}}/.secrets/passwd.yml\u0026#34; tasks: - name: GET NETPLAN FILENAME shell: ls /etc/netplan register: netplan_file - name: UPLOAD NETPLAN CONFIGS copy: content: \u0026#34;{{lookup(\u0026#39;file\u0026#39;, \u0026#39;{{inventory_dir}}/configs/netplan.{{ansible_host}}\u0026#39;)}}\\n\u0026#34; dest: \u0026#34;/etc/netplan/{{netplan_file.stdout}}\u0026#34; become: yes - name: APPLY NETPLAN SETTINGS command: netplan apply become: yes async: 10 poll: 0 - name: CHECK CONNECTIVITY TO STATIC IPs wait_for: port: 22 host: \u0026#34;{{hostvars[item].ansible_host}}\u0026#34; search_regex: OpenSSH delay: 20 timeout: 120 loop: \u0026#34;{{groups.control+groups.workers}}\u0026#34; connection: local run_once: yestasks/init/apply_netplan.yml ","date":"2022-07-06","objectID":"/posts/k8s-ansible/:4:2","tags":["k8s","ansible","automation"],"title":"k8s Ansible","uri":"/posts/k8s-ansible/#the-init-phase-steps-01-and-02"},{"categories":["ansible"],"collections":null,"content":"The pre-deploy phase (Step 03) Now that we have established that our targets will use their final static IPs, we are going to perform some tasks to prepare them for k8s deployment. #!/usr/bin/env ansible-playbook --- - name: \u0026#34;\u0026lt;! ################# PRE :: PERFORM PRE-DEPLOYMENT TASKS ################### !\u0026gt;\u0026#34; hosts: all:!dhcp_hosts gather_facts: yes #any_errors_fatal: no vars_files: - .secrets/passwd.yml tasks: - name: \u0026#34;\u0026lt;======================== SSH KEYS ========================\u0026gt;\u0026#34; include_tasks: tasks/pre/sshkeys.yml - name: \u0026#34;\u0026lt;======================== SUDO ============================\u0026gt;\u0026#34; include_tasks: tasks/pre/sudo.yml - name: \u0026#34;\u0026lt;======================== CLOUD-INIT ======================\u0026gt;\u0026#34; include_tasks: tasks/pre/cloud-init.yml - name: \u0026#34;\u0026lt;======================== HOSTNAMES =======================\u0026gt;\u0026#34; include_tasks: tasks/pre/hostname.yml - name: \u0026#34;\u0026lt;======================== HOSTS FILES =====================\u0026gt;\u0026#34; include_tasks: tasks/pre/hosts.yml - name: \u0026#34;\u0026lt;======================== UPDATE/UPGRADE ==================\u0026gt;\u0026#34; include_tasks: tasks/pre/upgrade.yml - name: \u0026#34;\u0026lt;======================== REBOOT ========================-\u0026gt;\u0026#34; include_tasks: tasks/pre/reboot.yml - debug: msg=\u0026#34;\\u2705 NODE READY FOR DEPLOYMENT\u0026#34;pre_deploy.yml We first generate an SSH key on localhost and distribute it to all the nodes in order for ansible to access them without using the user password. Then we enable passwordless sudo access for the ansible user. We disable cloud-init if the service is there. Set the correct hostname and configure all k8s nodes in the hosts files, and, finally perform pings between hosts to verify that name resolution and connectivity is successful. ","date":"2022-07-06","objectID":"/posts/k8s-ansible/:4:3","tags":["k8s","ansible","automation"],"title":"k8s Ansible","uri":"/posts/k8s-ansible/#the-pre-deploy-phase-step-03"},{"categories":["ansible"],"collections":null,"content":"The deploy phase (Step 04) The last phase is to deploy the k8s cluster. #!/usr/bin/env ansible-playbook --- - name: K8S DEPLOYMENT hosts: all:!dhcp_hosts gather_facts: yes vars_files: - .secrets/passwd.yml tasks: - name: \u0026#34;\u0026lt;======================== PRE-REQUISITES ===================\u0026gt;\u0026#34; include_tasks: tasks/deploy/packages_general.yml args: apply: become: yes - name: \u0026#34;\u0026lt;======================== TURN OFF SWAP ====================\u0026gt;\u0026#34; include_tasks: tasks/deploy/swap.yml - name: \u0026#34;\u0026lt;======================== DOCKER CRI =======================\u0026gt;\u0026#34; include_tasks: tasks/deploy/cri.yml args: apply: become: yes - name: \u0026#34;\u0026lt;======================== KUBE PACKAGES ====================\u0026gt;\u0026#34; include_tasks: tasks/deploy/packages_kube.yml args: apply: become: yes - name: \u0026#34;\u0026lt;======================= PREPARE THE CONTROL NODE ==========\u0026gt;\u0026#34; block: - name: \u0026#34;\u0026lt;======================== INIT CONTROL NODE ==========\u0026gt;\u0026#34; include_tasks: tasks/deploy/boot_control.yml - name: \u0026#34;\u0026lt;======================== DEPLOY CNI =================\u0026gt;\u0026#34; include_tasks: tasks/deploy/cni.yml when: inventory_hostname in groups.control - name: \u0026#34;\u0026lt;===================== ADD WORKER NODES ====================\u0026gt;\u0026#34; include_tasks: tasks/deploy/workers.yml deployment.yml We start off by installing the prerequisite packages, apt keys and repositories. We then turn off swap if any, in order for kubelet service to run without issues. Then, we proceed installing docker CRI, changing cgroup to systemd and assigning the user to docker group. We download kubeadm, kubectl, and kubelet packages. Next, we initialise the control node and install the network CNI plugin. Finally, we join the worker nodes to the cluster. ","date":"2022-07-06","objectID":"/posts/k8s-ansible/:4:4","tags":["k8s","ansible","automation"],"title":"k8s Ansible","uri":"/posts/k8s-ansible/#the-deploy-phase-step-04"},{"categories":["ansible"],"collections":null,"content":"Optimisations If we\u0026rsquo;ve reached this stage, it means that everything is working fine, we are happy, so the code is good and we do not need any optimisations or fine tuning \u0026#x1f604;. Well, there is always room for improvement and fine tuning, the only enemy is time. I am sure that a more experienced eye could spot many ways to improve the playbook and suggest better ways to do things or use tasks, but here are my observations after finishing the playbook: Improving Speed: During the check phase, where we check for reachability, I initially used to ping the hosts and act on the result. This was adding a nearly 45 second delay in execution, so I tested with wait_for towards SSH connectivity and this improved script time tremendously. After all, SSH connectivity is somewhat better than ping reachability, since it is one step further \u0026#x1f604;. What I am suggesting here is to take in mind the end goal of what you are after and figure out a way to do it more meaningful and perhaps faster. For example, if you have a task to create a file in a directory, should you first check if the directory exists? Well, it depends on the use case, but in general the task to create the file will fail if the directory does not exist in the first place and you would be able catch this. Task Consolidation: Although this playbook does the job, I think it is just automating a list of steps to achieve the result like one could perform manually on a system. Let\u0026rsquo;s take for example the apt module that installs packages. The module is called several times to install packages according to the phase of the playbook, but what about if we just used a single apt task to install all our needed packages from all phases. It would be like having a package installation phase. Unless for some reason we need discrete stages and actions, I think that looking at the playbook as a holistic entity most probably will reveal tasks that can be consolidated. Task Reduction: Well, here, the main focus is about the question should I use several tasks to do something or it can be done with a single task or less. As an example, we need to check for the existence of a file. We use the stat module and register a variable. We then use the debug module to check the output for success or not. For sure, it will depend on the use case, but another way of treating this could be by just the debug module and using a file lookup conditional clause for this task. Best Practices: Here, I will not expand more on using ansible roles, or group_vars folder, or avoiding the declarations of variables in the inventory file \u0026#x1f604; ...till next time...have fun! ","date":"2022-07-06","objectID":"/posts/k8s-ansible/:5:0","tags":["k8s","ansible","automation"],"title":"k8s Ansible","uri":"/posts/k8s-ansible/#optimisations"},{"categories":["Linux"],"collections":null,"content":"There has been a need sometimes that you are on a system that is not permanently set to use a proxy server for internet access and you just want to give one or two curl or wget commands to fetch some files, or vice versa \u0026#x1f604;. In order to avoid exporting the appropriate environment variables in the correct place, you can use the following two methods to enable or disable ad-hoc usage of the proxy server in linux bash. ","date":"2022-06-14","objectID":"/posts/proxy-or-not/:0:0","tags":["bash"],"title":"Proxy or not...here I come...","uri":"/posts/proxy-or-not/#"},{"categories":["Linux"],"collections":null,"content":"Method One - as an executable preceding your actual command Just put the following commands in a bash script, make it executable and put it in your path somewhere on the system. becos@fossa:~$ cat proxy #!/bin/bash -eu ########### ## Adjust to your environment ## HTTP_PROXY=\u0026#34;http://10.10.10.10:8080\u0026#34; HTTPS_PROXY=\u0026#34;http://10.10.10.10:8080\u0026#34; NO_PROXY=\u0026#34;localhost,127.0.0.1,$(hostname -i),.domain.com\u0026#34; ########### http_proxy=\u0026#34;$HTTP_PROXY\u0026#34; https_proxy=\u0026#34;$HTTPS_PROXY\u0026#34; no_proxy=\u0026#34;$NO_PROXY\u0026#34; export HTTP_PROXY HTTPS_PROXY NO_PROXY http_proxy https_proxy no_proxy exec \u0026#34;$@\u0026#34; becos@fossa:~$ chmod +x proxy \u0026amp;\u0026amp; sudo mv !$ /usr/local/bin/ chmod +x proxy \u0026amp;\u0026amp; sudo mv proxy /usr/local/bin/Now you can precede the proxy command just before calling your command that needs the proxy environment variables. becos@fossa:~$ proxy curl -sv http://www.google.com \u0026gt; /dev/null * Uses proxy env variable no_proxy == \u0026#39;localhost,127.0.0.1,192.168.1.5,.domain.com\u0026#39; * Uses proxy env variable http_proxy == \u0026#39;http://10.10.10.10:8080\u0026#39; * Trying 10.10.10.10:8080... * TCP_NODELAY set ^C becos@fossa:~$","date":"2022-06-14","objectID":"/posts/proxy-or-not/:1:0","tags":["bash"],"title":"Proxy or not...here I come...","uri":"/posts/proxy-or-not/#method-one---as-an-executable-preceding-your-actual-command"},{"categories":["Linux"],"collections":null,"content":"Method Two - as functions to the user profile You can add these two functions in say your .profile function proxyoff { unset http_proxy HTTP_PROXY https_proxy HTTPS_PROXY no_proxy NO_PROXY } function proxyon { http_proxy=\u0026#34;http://10.10.10.10:8080\u0026#34; HTTP_PROXY=\u0026#34;$http_proxy\u0026#34; https_proxy=\u0026#34;http://10.10.10.10:8080\u0026#34; HTTPS_PROXY=\u0026#34;$http_proxy\u0026#34; no_proxy=\u0026#34;localhost,127.0.0.1,$(hostname -i),.domain.com\u0026#34; NO_PROXY=\u0026#34;$no_proxy\u0026#34; export http_proxy HTTP_PROXY https_proxy HTTPS_PROXY no_proxy NO_PROXY }So, after a new shell, you can use the on and off functions according to your intent. becos@fossa:~$ env | grep -i proxy becos@fossa:~$ proxyon becos@fossa:~$ env | grep -i proxy no_proxy=localhost,127.0.0.1,192.168.1.5,.domain.com https_proxy=http://10.10.10.10:8080 NO_PROXY=localhost,127.0.0.1,192.168.1.5,.domain.com HTTPS_PROXY=http://10.10.10.10:8080 HTTP_PROXY=http://10.10.10.10:8080 http_proxy=http://10.10.10.10:8080 becos@fossa:~$ proxyoff becos@fossa:~$ env | grep -i proxyThat\u0026rsquo;s it, I hope it finds a use case. ...till next time...have fun! ","date":"2022-06-14","objectID":"/posts/proxy-or-not/:2:0","tags":["bash"],"title":"Proxy or not...here I come...","uri":"/posts/proxy-or-not/#method-two---as-functions-to-the-user-profile"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"Intro ","date":"2022-06-06","objectID":"/posts/kube-my-router-pt3/:1:0","tags":["k8s","kne","netsim"],"title":"Kube my router up! - Last Part","uri":"/posts/kube-my-router-pt3/#intro"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"Part One - Setting up the k8s VMs in EVE-NG ","date":"2022-06-06","objectID":"/posts/kube-my-router-pt3/:2:0","tags":["k8s","kne","netsim"],"title":"Kube my router up! - Last Part","uri":"/posts/kube-my-router-pt3/#part-one---setting-up-the-k8s-vms-in-eve-ng"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"Part Two - Deploying the k8s cluster with kubeadm ","date":"2022-06-06","objectID":"/posts/kube-my-router-pt3/:3:0","tags":["k8s","kne","netsim"],"title":"Kube my router up! - Last Part","uri":"/posts/kube-my-router-pt3/#part-two---deploying-the-k8s-cluster-with-kubeadm"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"Last Part - Installing and testing KNE ","date":"2022-06-06","objectID":"/posts/kube-my-router-pt3/:4:0","tags":["k8s","kne","netsim"],"title":"Kube my router up! - Last Part","uri":"/posts/kube-my-router-pt3/#last-part---installing-and-testing-kne"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"The Intent Install Google\u0026rsquo;s KNE on the k8s control node Use KNE to create sample topologies Test with Nokia srlinux Test with Arista cEOS The time has come to install Google\u0026rsquo;s KNE!!!. We are going to use the k8s control node for KNEs \u0026ldquo;control\u0026rdquo; server, so everything will be installed on it. ","date":"2022-06-06","objectID":"/posts/kube-my-router-pt3/:4:1","tags":["k8s","kne","netsim"],"title":"Kube my router up! - Last Part","uri":"/posts/kube-my-router-pt3/#the-intent"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"Prepare for KNE installation We need go so we are going to install it. # Download go ubuntu@kates-control:~$ wget -q https://go.dev/dl/go1.18.2.linux-amd64.tar.gz # Install it under our HOME directory ubuntu@kates-control:~$ tar -xvzf go1.18.2.linux-amd64.tar.gz # Put it in your PATH ubuntu@kates-control:~$ export PATH=$PATH:~/go/bin ubuntu@kates-control:~$ echo !! \u0026gt;\u0026gt; ~/.profile echo export PATH=$PATH:~/go/bin \u0026gt;\u0026gt; ~/.profile # Check if it\u0026#39;s working ubuntu@kates-control:~$ go version go version go1.18.2 linux/amd64 ","date":"2022-06-06","objectID":"/posts/kube-my-router-pt3/:4:2","tags":["k8s","kne","netsim"],"title":"Kube my router up! - Last Part","uri":"/posts/kube-my-router-pt3/#prepare-for-kne-installation"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"Deploy KNE Use of kind KNE requires kind if you are going to use it in a single node. KNE can also deploy a single node k8s using kind and have it run inside a container. Then all the labs will be deployed into this containerised k8s node. OK! We are now ready to install KNE. There are two ways, either install it via go $ go install github.com/google/kne/kne_cli@latestor, clone the repo from github and build it locally, which is what I did. ubuntu@kates-control:~$ git clone https://github.com/google/kne.git Cloning into \u0026#39;kne\u0026#39;... remote: Enumerating objects: 2146, done. remote: Counting objects: 100% (154/154), done. remote: Compressing objects: 100% (89/89), done. remote: Total 2146 (delta 66), reused 145 (delta 62), pack-reused 1992 Receiving objects: 100% (2146/2146), 41.28 MiB | 200.00 KiB/s, done. Resolving deltas: 100% (1052/1052), done. ubuntu@kates-control:~$ ubuntu@kates-control:~$ cd kne/kne_cli/ ubuntu@kates-control:~/kne/kne_cli$ GOPATH=~/go go install ubuntu@kates-control:~/kne/kne_cli$ cd ubuntu@kates-control:~$ kne_cli Kubernetes Network Emulation CLI. Works with meshnet to create layer 2 topology used by containers to layout networks in a k8s environment. Usage: kne_cli [command] Available Commands: completion Generate the autocompletion script for the specified shell create Create Topology delete Delete Topology deploy Deploy cluster. help Help about any command show Show Topology topology Topology commands. Flags: -h, --help help for kne_cli --kubecfg string kubeconfig file (default \u0026#34;/home/ubuntu/.kube/config\u0026#34;) -v, --verbosity string log level (default \u0026#34;info\u0026#34;) Use \u0026#34;kne_cli [command] --help\u0026#34; for more information about a command. ubuntu@kates-control:~$ ","date":"2022-06-06","objectID":"/posts/kube-my-router-pt3/:4:3","tags":["k8s","kne","netsim"],"title":"Kube my router up! - Last Part","uri":"/posts/kube-my-router-pt3/#deploy-kne"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"Prepare k8s cluster for KNE We need to install meshnet CNI. # Install meshnet from local manifests ubuntu@kates-control:~$ kubectl apply -k kne/manifests/meshnet/base/ namespace/meshnet created customresourcedefinition.apiextensions.k8s.io/topologies.networkop.co.uk created serviceaccount/meshnet created clusterrole.rbac.authorization.k8s.io/meshnet-clusterrole created clusterrolebinding.rbac.authorization.k8s.io/meshnet-clusterrolebinding created daemonset.apps/meshnet created # Check for any issues ubuntu@kates-control:~$ kubectl get events -n meshnet LAST SEEN TYPE REASON OBJECT MESSAGE 15m Normal Scheduled pod/meshnet-72zz4 Successfully assigned meshnet/meshnet-72zz4 to kates-node-02 15m Normal Pulling pod/meshnet-72zz4 Pulling image \u0026#34;hfam/meshnet:latest\u0026#34; 15m Normal Pulled pod/meshnet-72zz4 Successfully pulled image \u0026#34;hfam/meshnet:latest\u0026#34; in 9.581184132s 15m Normal Created pod/meshnet-72zz4 Created container meshnet 15m Normal Started pod/meshnet-72zz4 Started container meshnet 15m Normal Scheduled pod/meshnet-9vqr2 Successfully assigned meshnet/meshnet-9vqr2 to kates-node-01 15m Normal Pulling pod/meshnet-9vqr2 Pulling image \u0026#34;hfam/meshnet:latest\u0026#34; 15m Normal Pulled pod/meshnet-9vqr2 Successfully pulled image \u0026#34;hfam/meshnet:latest\u0026#34; in 9.716760651s 15m Normal Created pod/meshnet-9vqr2 Created container meshnet 15m Normal Started pod/meshnet-9vqr2 Started container meshnet 15m Normal Scheduled pod/meshnet-fkv2s Successfully assigned meshnet/meshnet-fkv2s to kates-control 15m Normal Pulling pod/meshnet-fkv2s Pulling image \u0026#34;hfam/meshnet:latest\u0026#34; 15m Normal Pulled pod/meshnet-fkv2s Successfully pulled image \u0026#34;hfam/meshnet:latest\u0026#34; in 10.072682996s 15m Normal Created pod/meshnet-fkv2s Created container meshnet 15m Normal Started pod/meshnet-fkv2s Started container meshnet 15m Normal SuccessfulCreate daemonset/meshnet Created pod: meshnet-fkv2s 15m Normal SuccessfulCreate daemonset/meshnet Created pod: meshnet-72zz4 15m Normal SuccessfulCreate daemonset/meshnet Created pod: meshnet-9vqr2 # Check the new namespace ubuntu@kates-control:~$ kubectl get all -n meshnet NAME READY STATUS RESTARTS AGE pod/meshnet-72zz4 1/1 Running 0 15m pod/meshnet-9vqr2 1/1 Running 0 15m pod/meshnet-fkv2s 1/1 Running 0 15m NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/meshnet 3 3 3 3 3 kubernetes.io/arch=amd64 15m # Check the cni ubuntu@kates-control:~$ sudo cat /etc/cni/net.d/00-meshnet.conflist { \u0026#34;cniVersion\u0026#34;: \u0026#34;0.3.1\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;cbr0\u0026#34;, \u0026#34;plugins\u0026#34;: [ { \u0026#34;delegate\u0026#34;: { \u0026#34;hairpinMode\u0026#34;: true, \u0026#34;isDefaultGateway\u0026#34;: true }, \u0026#34;type\u0026#34;: \u0026#34;flannel\u0026#34; }, { \u0026#34;capabilities\u0026#34;: { \u0026#34;portMappings\u0026#34;: true }, \u0026#34;type\u0026#34;: \u0026#34;portmap\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;meshnet\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;meshnet\u0026#34;, \u0026#34;ipam\u0026#34;: {}, \u0026#34;dns\u0026#34;: {} } ] }Then we need to install metallb in order to access our nodes from outside k8s. # Install from local manifests ubuntu@kates-control:~$ kubectl apply -f kne/manifests/metallb/namespace.yaml namespace/metallb-system created ubuntu@kates-control:~$ kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey=\u0026#34;$(openssl rand -base64 128)\u0026#34; secret/memberlist created ubuntu@kates-control:~$ kubectl apply -f kne/manifests/metallb/metallb.yaml Warning: policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ podsecuritypolicy.policy/controller created podsecuritypolicy.policy/speaker created serviceaccount/controller created serviceaccount/speaker created clusterrole.rbac.authorization.k8s.io/metallb-system:controller created clusterrole.rbac.authorization.k8s.io/metallb-system:speaker created role.rbac.authorization.k8s.io/config-watcher created role.rbac.authorization.k8s.io/pod-lister created role.rbac.authorization.k8s.io/controller created clusterrolebinding.rbac.authorization.k8s.io/metallb-system:controller crea","date":"2022-06-06","objectID":"/posts/kube-my-router-pt3/:4:4","tags":["k8s","kne","netsim"],"title":"Kube my router up! - Last Part","uri":"/posts/kube-my-router-pt3/#prepare-k8s-cluster-for-kne"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"Use KNE to create a basic topology Okay now! Let pick a simple sample lab with two linux nodes and spin it up. ubuntu@kates-control:~$ kne_cli create kne/examples/2node-host.pb.txt INFO[0000] /home/ubuntu/kne/examples INFO[0000] Creating manager for: 2node-host INFO[0000] Trying in-cluster configuration INFO[0000] Falling back to kubeconfig: \u0026#34;/home/ubuntu/.kube/config\u0026#34; INFO[0000] Topology: name: \u0026#34;2node-host\u0026#34; nodes: \u0026lt; name: \u0026#34;vm-1\u0026#34; type: HOST config: \u0026lt; image: \u0026#34;hfam/ubuntu:latest\u0026#34; \u0026gt; \u0026gt; nodes: \u0026lt; name: \u0026#34;vm-2\u0026#34; type: HOST \u0026gt; links: \u0026lt; a_node: \u0026#34;vm-1\u0026#34; a_int: \u0026#34;eth1\u0026#34; z_node: \u0026#34;vm-2\u0026#34; z_int: \u0026#34;eth1\u0026#34; \u0026gt; INFO[0000] Adding Link: vm-1:eth1 vm-2:eth1 INFO[0000] Adding Node: vm-1:UNKNOWN:HOST INFO[0000] Adding Node: vm-2:UNKNOWN:HOST INFO[0000] Creating namespace for topology: \u0026#34;2node-host\u0026#34; INFO[0000] Server Namespace: \u0026amp;Namespace{ObjectMeta:{2node-host 13677a1e-f4fc-42cc-bee8-4484107facb6 139020 0 2022-06-01 13:29:27 +0000 UTC \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; map[kubernetes.io/metadata.name:2node-host] map[] [] [] [{kne_cli Update v1 2022-06-01 13:29:27 +0000 UTC FieldsV1 {\u0026#34;f:metadata\u0026#34;:{\u0026#34;f:labels\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:kubernetes.io/metadata.name\u0026#34;:{}}}} }]},Spec:NamespaceSpec{Finalizers:[kubernetes],},Status:NamespaceStatus{Phase:Active,Conditions:[]NamespaceCondition{},},} INFO[0000] Getting topology specs for namespace 2node-host INFO[0000] Getting topology specs for node vm-1 INFO[0000] Getting topology specs for node vm-2 INFO[0000] Creating topology for meshnet node vm-2 INFO[0000] Meshnet Node: \u0026amp;{TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:vm-2 GenerateName: Namespace:2node-host SelfLink: UID:b6de0742-35e3-478b-8c40-e7b6f59dc8bf ResourceVersion:139023 Generation:1 CreationTimestamp:2022-06-01 13:29:27 +0000 UTC DeletionTimestamp:\u0026lt;nil\u0026gt; DeletionGracePeriodSeconds:\u0026lt;nil\u0026gt; Labels:map[] Annotations:map[] OwnerReferences:[] Finalizers:[] ZZZ_DeprecatedClusterName: ManagedFields:[{Manager:kne_cli Operation:Update APIVersion:networkop.co.uk/v1beta1 Time:2022-06-01 13:29:27 +0000 UTC FieldsType:FieldsV1 FieldsV1:{\u0026#34;f:spec\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:links\u0026#34;:{}}} Subresource:}]} Status:{TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name: GenerateName: Namespace: SelfLink: UID: ResourceVersion: Generation:0 CreationTimestamp:0001-01-01 00:00:00 +0000 UTC DeletionTimestamp:\u0026lt;nil\u0026gt; DeletionGracePeriodSeconds:\u0026lt;nil\u0026gt; Labels:map[] Annotations:map[] OwnerReferences:[] Finalizers:[] ZZZ_DeprecatedClusterName: ManagedFields:[]} Skipped:[] SrcIp: NetNs:} Spec:{TypeMeta:{Kind: APIVersion:} Links:[{LocalIntf:eth1 LocalIP: PeerIntf:eth1 PeerIP: PeerPod:vm-1 UID:0}]}} INFO[0000] Creating topology for meshnet node vm-1 INFO[0000] Meshnet Node: \u0026amp;{TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:vm-1 GenerateName: Namespace:2node-host SelfLink: UID:140df03a-15a1-44fe-a57f-47d464f8b9fc ResourceVersion:139024 Generation:1 CreationTimestamp:2022-06-01 13:29:27 +0000 UTC DeletionTimestamp:\u0026lt;nil\u0026gt; DeletionGracePeriodSeconds:\u0026lt;nil\u0026gt; Labels:map[] Annotations:map[] OwnerReferences:[] Finalizers:[] ZZZ_DeprecatedClusterName: ManagedFields:[{Manager:kne_cli Operation:Update APIVersion:networkop.co.uk/v1beta1 Time:2022-06-01 13:29:27 +0000 UTC FieldsType:FieldsV1 FieldsV1:{\u0026#34;f:spec\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:links\u0026#34;:{}}} Subresource:}]} Status:{TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name: GenerateName: Namespace: SelfLink: UID: ResourceVersion: Generation:0 CreationTimestamp:0001-01-01 00:00:00 +0000 UTC DeletionTimestamp:\u0026lt;nil\u0026gt; DeletionGracePeriodSeconds:\u0026lt;nil\u0026gt; Labels:map[] Annotations:map[] OwnerReferences:[] Finalizers:[] ZZZ_DeprecatedClusterName: ManagedFields:[]} Skipped:[] SrcIp: NetNs:} Spec:{TypeMeta:{Kind: APIVersion:} Links:[{LocalIntf:eth1 LocalIP: PeerIntf:eth1 PeerIP: PeerPod:vm-2 UID:0}]}} INFO[0000] Creating Node Pods INFO[0000] Creating Pod: name:\u0026#34;vm-1\u0026#34; type:HOST config:{command:\u0026#34;/bin","date":"2022-06-06","objectID":"/posts/kube-my-router-pt3/:4:5","tags":["k8s","kne","netsim"],"title":"Kube my router up! - Last Part","uri":"/posts/kube-my-router-pt3/#use-kne-to-create-a-basic-topology"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"Test Nokia srlinux In order to use the srlinux container we need to have the srlinux controller installed in k8s. ubuntu@kates-control:~$ kubectl apply -k https://github.com/srl-labs/srl-controller/config/default namespace/srlinux-controller created customresourcedefinition.apiextensions.k8s.io/srlinuxes.kne.srlinux.dev created serviceaccount/srlinux-controller-controller-manager created role.rbac.authorization.k8s.io/srlinux-controller-leader-election-role created clusterrole.rbac.authorization.k8s.io/srlinux-controller-manager-role created clusterrole.rbac.authorization.k8s.io/srlinux-controller-metrics-reader created clusterrole.rbac.authorization.k8s.io/srlinux-controller-proxy-role created rolebinding.rbac.authorization.k8s.io/srlinux-controller-leader-election-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/srlinux-controller-manager-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/srlinux-controller-proxy-rolebinding created configmap/srlinux-controller-manager-config created service/srlinux-controller-controller-manager-metrics-service created deployment.apps/srlinux-controller-controller-manager created ubuntu@kates-control:~$ kubectl get all -n srlinux-controller NAME READY STATUS RESTARTS AGE pod/srlinux-controller-controller-manager-69f6579c6f-skg2j 2/2 Running 0 40s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/srlinux-controller-controller-manager-metrics-service ClusterIP 10.100.185.117 \u0026lt;none\u0026gt; 8443/TCP 40s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/srlinux-controller-controller-manager 1/1 1 1 40s NAME DESIRED CURRENT READY AGE replicaset.apps/srlinux-controller-controller-manager-69f6579c6f 1 1 1 40s ubuntu@kates-control:~$ kubectl get events -n srlinux-controller LAST SEEN TYPE REASON OBJECT MESSAGE 49s Normal LeaderElection configmap/8bce046c.srlinux.dev srlinux-controller-controller-manager-69f6579c6f-skg2j_9e9d41e6-e29c-4abd-849f-39fd019eca72 became leader 49s Normal LeaderElection lease/8bce046c.srlinux.dev srlinux-controller-controller-manager-69f6579c6f-skg2j_9e9d41e6-e29c-4abd-849f-39fd019eca72 became leader 62s Normal Scheduled pod/srlinux-controller-controller-manager-69f6579c6f-skg2j Successfully assigned srlinux-controller/srlinux-controller-controller-manager-69f6579c6f-skg2j to kates-node-02 61s Normal Pulling pod/srlinux-controller-controller-manager-69f6579c6f-skg2j Pulling image \u0026#34;gcr.io/kubebuilder/kube-rbac-proxy:v0.8.0\u0026#34; 57s Normal Pulled pod/srlinux-controller-controller-manager-69f6579c6f-skg2j Successfully pulled image \u0026#34;gcr.io/kubebuilder/kube-rbac-proxy:v0.8.0\u0026#34; in 4.485800395s 57s Normal Created pod/srlinux-controller-controller-manager-69f6579c6f-skg2j Created container kube-rbac-proxy 57s Normal Started pod/srlinux-controller-controller-manager-69f6579c6f-skg2j Started container kube-rbac-proxy 57s Normal Pulling pod/srlinux-controller-controller-manager-69f6579c6f-skg2j Pulling image \u0026#34;ghcr.io/srl-labs/srl-controller:0.3.1\u0026#34; 53s Normal Pulled pod/srlinux-controller-controller-manager-69f6579c6f-skg2j Successfully pulled image \u0026#34;ghcr.io/srl-labs/srl-controller:0.3.1\u0026#34; in 4.313788546s 52s Normal Created pod/srlinux-controller-controller-manager-69f6579c6f-skg2j Created container manager 52s Normal Started pod/srlinux-controller-controller-manager-69f6579c6f-skg2j Started container manager 62s Normal SuccessfulCreate replicaset/srlinux-controller-controller-manager-69f6579c6f Created pod: srlinux-controller-controller-manager-69f6579c6f-skg2j 62s Normal ScalingReplicaSet deployment/srlinux-controller-controller-manager Scaled up replica set srlinux-controller-controller-manager-69f6579c6f to 1Deploy a sample topology using KNEs sample topologies files. ubuntu@kates-control:~$ cat kne/examples/srlinux/2node-srl-with-cert.pbtxt name: \u0026#34;2srl-certs\u0026#34; nodes: { name: \u0026#34;r1\u0026#34; type: NOKIA_SRL config: { cert: { self_signed: { cert_name: \u0026#34;kne-profile\u0026#34;, key_name: \u0026#34;N/A\u0026#34;, key_size: 4096, } } } ","date":"2022-06-06","objectID":"/posts/kube-my-router-pt3/:4:6","tags":["k8s","kne","netsim"],"title":"Kube my router up! - Last Part","uri":"/posts/kube-my-router-pt3/#test-nokia-srlinux"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"Test Arista cEOS Get the arista container image and copy it to all k8s nodes or only to the workers if you control-plane is tainted. Then import it in each node. ubuntu@kates-control:~$ sudo ctr -n k8s.io image import ceos.tgz unpacking docker.io/library/ceos:latest (sha256:bfd3f2fea1cad2d06f0e5113cb3a1dc81f0ccbb28acf11949e15dadcf68eda9a)...doneLet\u0026rsquo;s create a simple lab file. ubuntu@kates-control:~/labs$ cat ceos.pbtxt name: \u0026#34;arista\u0026#34; nodes: { name: \u0026#34;r1\u0026#34; type: ARISTA_CEOS services:{ key: 22 value: { name: \u0026#34;ssh\u0026#34; inside: 22 } } } nodes: { name: \u0026#34;r2\u0026#34; type: ARISTA_CEOS services:{ key: 22 value: { name: \u0026#34;ssh\u0026#34; inside: 22 } } } links: { a_node: \u0026#34;r1\u0026#34; a_int: \u0026#34;eth1\u0026#34; z_node: \u0026#34;r2\u0026#34; z_int: \u0026#34;eth1\u0026#34; } links: { a_node: \u0026#34;r1\u0026#34; a_int: \u0026#34;eth2\u0026#34; z_node: \u0026#34;r2\u0026#34; z_int: \u0026#34;eth2\u0026#34; }And fire up the topology. ubuntu@kates-control:~/labs$ kne_cli create ceos.pbtxt INFO[0000] /home/ubuntu/labs INFO[0000] Creating manager for: arista INFO[0000] Trying in-cluster configuration INFO[0000] Falling back to kubeconfig: \u0026#34;/home/ubuntu/.kube/config\u0026#34; INFO[0000] Topology: name: \u0026#34;arista\u0026#34; nodes: \u0026lt; name: \u0026#34;r1\u0026#34; type: ARISTA_CEOS services: \u0026lt; key: 22 value: \u0026lt; name: \u0026#34;ssh\u0026#34; inside: 22 \u0026gt; \u0026gt; \u0026gt; nodes: \u0026lt; name: \u0026#34;r2\u0026#34; type: ARISTA_CEOS services: \u0026lt; key: 22 value: \u0026lt; name: \u0026#34;ssh\u0026#34; inside: 22 \u0026gt; \u0026gt; \u0026gt; links: \u0026lt; a_node: \u0026#34;r1\u0026#34; a_int: \u0026#34;eth1\u0026#34; z_node: \u0026#34;r2\u0026#34; z_int: \u0026#34;eth1\u0026#34; \u0026gt; links: \u0026lt; a_node: \u0026#34;r1\u0026#34; a_int: \u0026#34;eth2\u0026#34; z_node: \u0026#34;r2\u0026#34; z_int: \u0026#34;eth2\u0026#34; \u0026gt; INFO[0000] Adding Link: r1:eth1 r2:eth1 INFO[0000] Adding Link: r1:eth2 r2:eth2 INFO[0000] Adding Node: r1:UNKNOWN:ARISTA_CEOS INFO[0000] Adding Node: r2:UNKNOWN:ARISTA_CEOS INFO[0000] Creating namespace for topology: \u0026#34;arista\u0026#34; INFO[0000] Server Namespace: \u0026amp;Namespace{ObjectMeta:{arista 82fe4826-a7dd-4fa3-bf2d-36d09236c2d7 71849 0 2022-06-08 20:49:14 +0000 UTC \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; map[kubernetes.io/metadata.name:arista] map[] [] [] [{kne_cli Update v1 2022-06-08 20:49:14 +0000 UTC FieldsV1 {\u0026#34;f:metadata\u0026#34;:{\u0026#34;f:labels\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:kubernetes.io/metadata.name\u0026#34;:{}}}}}]},Spec:NamespaceSpec{Finalizers:[kubernetes],},Status:NamespaceStatus{Phase:Active,Conditions:[]NamespaceCondition{},},} INFO[0000] Getting topology specs for namespace arista INFO[0000] Getting topology specs for node r1 INFO[0000] Getting topology specs for node r2 INFO[0000] Creating topology for meshnet node r1 INFO[0000] Meshnet Node: \u0026amp;{TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:r1 GenerateName: Namespace:arista SelfLink: UID:ba43116d-1c55-4e43-9201-cd7f2c9f3259 ResourceVersion:71852 Generation:1 CreationTimestamp:2022-06-08 20:49:14 +0000 UTC DeletionTimestamp:\u0026lt;nil\u0026gt; DeletionGracePeriodSeconds:\u0026lt;nil\u0026gt; Labels:map[] Annotations:map[] OwnerReferences:[] Finalizers:[] ClusterName: ManagedFields:[{Manager:kne_cli Operation:Update APIVersion:networkop.co.uk/v1beta1 Time:2022-06-08 20:49:14 +0000 UTC FieldsType:FieldsV1 FieldsV1:{\u0026#34;f:spec\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:links\u0026#34;:{}}}}]} Status:{TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name: GenerateName: Namespace: SelfLink: UID: ResourceVersion: Generation:0 CreationTimestamp:0001-01-01 00:00:00 +0000 UTC DeletionTimestamp:\u0026lt;nil\u0026gt; DeletionGracePeriodSeconds:\u0026lt;nil\u0026gt; Labels:map[] Annotations:map[] OwnerReferences:[] Finalizers:[] ClusterName: ManagedFields:[]} Skipped:[] SrcIp: NetNs:} Spec:{TypeMeta:{Kind: APIVersion:} Links:[{LocalIntf:eth1 LocalIP: PeerIntf:eth1 PeerIP: PeerPod:r2 UID:0} {LocalIntf:eth2 LocalIP: PeerIntf:eth2 PeerIP: PeerPod:r2 UID:1}]}} INFO[0000] Creating topology for meshnet node r2 INFO[0000] Meshnet Node: \u0026amp;{TypeMeta:{Kind: APIVersion:} ObjectMeta:{Name:r2 GenerateName: Namespace:arista SelfLink: UID:e677078e-0dc1-4e02-8258-b759acb61cfd ResourceVersion:71853 Generation:1 CreationTimestamp:2022-06-08 20:49:14 +000","date":"2022-06-06","objectID":"/posts/kube-my-router-pt3/:4:7","tags":["k8s","kne","netsim"],"title":"Kube my router up! - Last Part","uri":"/posts/kube-my-router-pt3/#test-arista-ceos"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"References and influences KNE on github Inspired by this Blog post metallb srl-labs on github srlinux ","date":"2022-06-06","objectID":"/posts/kube-my-router-pt3/:4:8","tags":["k8s","kne","netsim"],"title":"Kube my router up! - Last Part","uri":"/posts/kube-my-router-pt3/#references-and-influences"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"Outro Well, this was just a bare miminum and focused on how to set things up. I hope my post will help someone to start kicking the tyres with KNE. I also hope the project gets traction and evolves since the applications of it seem endless \u0026#x1f604; from all aspects. Thanking you, for reading the post, as well as all people sharing and contributing to the community making it sustainable and strong. ...till next time...have fun! ","date":"2022-06-06","objectID":"/posts/kube-my-router-pt3/:5:0","tags":["k8s","kne","netsim"],"title":"Kube my router up! - Last Part","uri":"/posts/kube-my-router-pt3/#outro"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"Intro ","date":"2022-05-31","objectID":"/posts/kube-my-router-pt2/:1:0","tags":["k8s","kubeadm","Ubuntu"],"title":"Kube my router up! - Part Two","uri":"/posts/kube-my-router-pt2/#intro"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"Part One - Setting up the k8s VMs in EVE-NG ","date":"2022-05-31","objectID":"/posts/kube-my-router-pt2/:2:0","tags":["k8s","kubeadm","Ubuntu"],"title":"Kube my router up! - Part Two","uri":"/posts/kube-my-router-pt2/#part-one---setting-up-the-k8s-vms-in-eve-ng"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"Part Two - Deploying the k8s cluster with kubeadm ","date":"2022-05-31","objectID":"/posts/kube-my-router-pt2/:3:0","tags":["k8s","kubeadm","Ubuntu"],"title":"Kube my router up! - Part Two","uri":"/posts/kube-my-router-pt2/#part-two---deploying-the-k8s-cluster-with-kubeadm"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"The Intent Deploy k8s cluster with kubeadm Single control plane, two workers Use containerd as the container runtime Flannel as network overlay CNI plugin The main question points while preparing the k8s cluster for KNE where which CRI and which CNI to use, since I am not very deep in to k8s. A brief research done on the kind container that KNE uses to deploy the cluster and run the simulations in it revealed that it uses containerd as the CRI. ❯ kubectl get nodes -owide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME kne-control-plane Ready control-plane,master 14d v1.22.1 172.19.0.2 \u0026lt;none\u0026gt; Ubuntu Impish Indri (development branch) 5.4.0-107-generic containerd://1.5.5Kindnet as the primary the CNI. ❯ kubectl get ds -n kube-system NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE kindnet 1 1 1 1 1 \u0026lt;none\u0026gt; 14d kube-proxy 1 1 1 1 1 kubernetes.io/os=linux 14dIn terms of network configuration: ❯ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a099e5c3f7d3 kindest/node:v1.22.1 \u0026#34;/usr/local/bin/entr…\u0026#34; 2 weeks ago Up 2 weeks 127.0.0.1:34459-\u0026gt;6443/tcp kne-control-plane ❯ docker exec -it a099e5c3f7d3 bash root@kne-control-plane:/# root@kne-control-plane:/# ll /etc/cni/net.d/ total 16 drwx------ 2 root root 4096 May 22 22:16 ./ drwx------ 3 root root 4096 May 22 22:14 ../ -r--r--r-- 1 root root 518 May 22 22:16 00-meshnet.conflist -rw-r--r-- 1 root root 409 May 22 22:15 10-kindnet.conflist root@kne-control-plane:/# cat /etc/cni/net.d/10-kindnet.conflist { \u0026#34;cniVersion\u0026#34;: \u0026#34;0.3.1\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;kindnet\u0026#34;, \u0026#34;plugins\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;ptp\u0026#34;, \u0026#34;ipMasq\u0026#34;: false, \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;host-local\u0026#34;, \u0026#34;dataDir\u0026#34;: \u0026#34;/run/cni-ipam-state\u0026#34;, \u0026#34;routes\u0026#34;: [ { \u0026#34;dst\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34; } ], \u0026#34;ranges\u0026#34;: [ [ { \u0026#34;subnet\u0026#34;: \u0026#34;10.244.0.0/24\u0026#34; } ] ] } , \u0026#34;mtu\u0026#34;: 1500 }, { \u0026#34;type\u0026#34;: \u0026#34;portmap\u0026#34;, \u0026#34;capabilities\u0026#34;: { \u0026#34;portMappings\u0026#34;: true } } ] } root@kne-control-plane:/# cat /etc/cni/net.d/00-meshnet.conflist { \u0026#34;cniVersion\u0026#34;: \u0026#34;0.3.1\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;kindnet\u0026#34;, \u0026#34;plugins\u0026#34;: [ { \u0026#34;ipMasq\u0026#34;: false, \u0026#34;ipam\u0026#34;: { \u0026#34;dataDir\u0026#34;: \u0026#34;/run/cni-ipam-state\u0026#34;, \u0026#34;ranges\u0026#34;: [ [ { \u0026#34;subnet\u0026#34;: \u0026#34;10.244.0.0/24\u0026#34; } ] ], \u0026#34;routes\u0026#34;: [ { \u0026#34;dst\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34; } ], \u0026#34;type\u0026#34;: \u0026#34;host-local\u0026#34; }, \u0026#34;mtu\u0026#34;: 1500, \u0026#34;type\u0026#34;: \u0026#34;ptp\u0026#34; }, { \u0026#34;capabilities\u0026#34;: { \u0026#34;portMappings\u0026#34;: true }, \u0026#34;type\u0026#34;: \u0026#34;portmap\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;meshnet\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;meshnet\u0026#34;, \u0026#34;ipam\u0026#34;: {}, \u0026#34;dns\u0026#34;: {} } ] }root@kne-control-plane:/#So it is a simple Layer 3 CNI, and meshnet will be installed on top of it. Let\u0026rsquo;s use a more common CNI, like flannel for our use case. Using flannel When using flannel, the main drawback was the interface MTU inside the device container, seen on srlinux. Flannel, since it uses VXLAN, will inherit the MTU of the main ethernet interface of the host and create its bridge interface flannel.1 with an MTU of minus 50, i.e. 1450 by default. When spinning up the containers, they use this MTU and from what I saw, srlinux cannot bring up its interfaces that need a minimum of 1500 MTU. The only way of mitigating this easily is to use higher MTU on the host interface, so flannel can inherit this, which I presume in a production environment should be something common and the network underlay would of course support jumbo\u0026rsquo;s. More on this during final part. Let\u0026rsquo;s change gears now and deploy the k8s cluster on the three VMs that are running on the two EVE-NG servers. Here\u0026rsquo;s what we have up and running: EVE-NG node Role VM name IP address CPU RAM eve-01 control-node kates-control 192.168.1.30 4 4096 eve-01 worker kates-node-01 192.168.1.31 8 28672 eve-02 worker ","date":"2022-05-31","objectID":"/posts/kube-my-router-pt2/:3:1","tags":["k8s","kubeadm","Ubuntu"],"title":"Kube my router up! - Part Two","uri":"/posts/kube-my-router-pt2/#the-intent"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"Prepare all nodes for kubeadm In all nodes, fix the /etc/hosts since we are not using proper DNS resolution and we need to statically be able to resolve. $ diff /etc/hosts.orig /etc/hosts -p *** /etc/hosts.orig 2022-05-30 19:28:32.679321412 +0000 --- /etc/hosts 2022-05-30 19:29:04.931472825 +0000 *************** *** 1,4 **** --- 1,7 ---- 127.0.0.1 localhost + 192.168.1.30 kates-control + 192.168.1.31 kates-node-01 + 192.168.1.32 kates-node-02 Install the pre-requisite packages if missing and add the k8s sources in apt. $ sudo apt -y install vim git curl wget apt-transport-https ca-certificates $ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - OK $ echo \u0026#34;deb https://apt.kubernetes.io/ kubernetes-xenial main\u0026#34; | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main $ sudo apt updateYou may upgrade any packages needed after the last apt update. Next, we are now ready to install kubeadm along with kubelet and kubectl packages $ sudo apt -y install kubeadm kubelet kubectl # Lock the software version on these packages to avoid any accidental upgrade $ sudo apt-mark hold kubelet kubeadm kubectl # This is the version used while I was deploying $ kubectl version --output=yaml --client \u0026amp;\u0026amp; kubeadm version -o yaml clientVersion: buildDate: \u0026#34;2022-05-24T12:26:19Z\u0026#34; compiler: gc gitCommit: 3ddd0f45aa91e2f30c70734b175631bec5b5825a gitTreeState: clean gitVersion: v1.24.1 goVersion: go1.18.2 major: \u0026#34;1\u0026#34; minor: \u0026#34;24\u0026#34; platform: linux/amd64 kustomizeVersion: v4.5.4 clientVersion: buildDate: \u0026#34;2022-05-24T12:24:38Z\u0026#34; compiler: gc gitCommit: 3ddd0f45aa91e2f30c70734b175631bec5b5825a gitTreeState: clean gitVersion: v1.24.1 goVersion: go1.18.2 major: \u0026#34;1\u0026#34; minor: \u0026#34;24\u0026#34; platform: linux/amd64 Specific Version If you want to deploy a specific version of the packages and not the latest available you could execute: $ sudo apt list -a kubelet kubeadm kubectl | grep 1.22.2 WARNING: apt does not have a stable CLI interface. Use with caution in scripts. kubeadm/kubernetes-xenial 1.22.2-00 amd64 kubectl/kubernetes-xenial 1.22.2-00 amd64 kubelet/kubernetes-xenial 1.22.2-00 amd64And then install with: sudo apt -y install kubeadm=1.22.2-00 kubelet=1.22.2-00 kubectl=1.22.2-00Remember that if you are using a specific version you will have to specify that while downloading the k8s images and/or during kubeadm init bootstrap of the control node. We now need to disable swap if we are using any (cloud images do not by default), since kubelet will not run unless swap is disabled. $ sudo swapoff -a # Comment out the relevant line in fstab to disable swap from loading on boot (usually last line) $ sudo vi /etc/fstabEnable kernel modules and bridge NF to iptables chains. ubuntu@kates-control:~$ sudo modprobe overlay ubuntu@kates-control:~$ sudo modprobe br_netfilter ubuntu@kates-control:~$ cat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/k8s-cri.conf \u0026gt; overlay \u0026gt; br_netfilter \u0026gt; EOF overlay br_netfilter ubuntu@kates-control:~$ sudo tee /etc/sysctl.d/99-k8s-cri.conf\u0026lt;\u0026lt;EOF \u0026gt; net.bridge.bridge-nf-call-ip6tables = 1 \u0026gt; net.bridge.bridge-nf-call-iptables = 1 \u0026gt; net.ipv4.ip_forward = 1 \u0026gt; EOF net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 # Reload sysctl to apply changes ubuntu@kates-control:~$ sudo sysctl --system ","date":"2022-05-31","objectID":"/posts/kube-my-router-pt2/:3:2","tags":["k8s","kubeadm","Ubuntu"],"title":"Kube my router up! - Part Two","uri":"/posts/kube-my-router-pt2/#prepare-all-nodes-for-kubeadm"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"Install a container runtime As far as CRI, there are three options available. Go with either docker, containerd or use CRI-O. For this post we are going with containerd, so the first step is to add the repo to our nodes and install the service. ubuntu@kates-control:~$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - OK ubuntu@kates-control:~$ sudo add-apt-repository \u0026#34;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026#34; ubuntu@kates-control:~$ sudo apt update -y ubuntu@kates-control:~$ sudo apt install -y containerd.ioNext, put in place a default configuration. ubuntu@kates-control:~$ sudo mv /etc/containerd/config.toml /etc/containerd/config.toml.orig ubuntu@kates-control:~$ containerd config default | sudo tee /etc/containerd/config.tomlNext, set the cgroup to use systemd and re-start the service. ubuntu@kates-control:~$ sudo sed -i s/\u0026#34;SystemdCgroup = false\u0026#34;/\u0026#34;SystemdCgroup = true\u0026#34;/g /etc/containerd/config.toml ubuntu@kates-control:~$ sudo systemctl restart containerdTest that pulling images works. ubuntu@kates-control:~$ sudo ctr image pull docker.io/library/nginx:latest ubuntu@kates-control:~$ sudo ctr image ls -q docker.io/library/nginx:latest # Delete the image ubuntu@kates-control:~$ sudo ctr image rm docker.io/library/nginx:latest docker.io/library/nginx:latest Access via proxy If you are behind a proxy, you can expose the proxy environment variables to the daemon. $ sudo mkdir /etc/systemd/system/containerd.service.d/ $ sudo tee /etc/systemd/system/containerd.service.d/http-proxy.conf\u0026lt;\u0026lt;EOF \u0026gt; [Service] \u0026gt; Environment=\u0026#34;HTTP_PROXY=http://\u0026lt;your_proxy\u0026gt;:\u0026lt;port\u0026gt;\u0026#34; \u0026gt; Environment=\u0026#34;HTTPS_PROXY=http://\u0026lt;your_proxy\u0026gt;:\u0026lt;port\u0026gt;\u0026#34; \u0026gt; Environment=\u0026#34;NO_PROXY=localhost,127.0.0.1,.domain.com\u0026#34; \u0026gt; EOF $ sudo systemctl daemon-reload $ sudo systemctl restart containerdNOTE: The no_proxy variable will not accept subnet masks, only full /32 ip addresses. ","date":"2022-05-31","objectID":"/posts/kube-my-router-pt2/:3:3","tags":["k8s","kubeadm","Ubuntu"],"title":"Kube my router up! - Part Two","uri":"/posts/kube-my-router-pt2/#install-a-container-runtime"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"Bootstrap k8s control node We are ready now to initialise the control node on the first VM. First we need to enable the kubelet service if not already on. ubuntu@kates-control:~$ systemctl is-enabled kubelet \u0026gt;/dev/null \\ \u0026amp;\u0026amp; echo Service is already enabled \\ || echo Enabling kubelet \\ \u0026amp;\u0026amp; sudo systemctl enable kubelet Service is already enabledNo worries if the service does not load correctly, it will, after kubeadm finishes configuring the control node. Download k8s images from the registry, optionally, specifying the k8s version. # Check for the images first ubuntu@kates-control:~$ sudo kubeadm config images list --kubernetes-version stable-1.24 k8s.gcr.io/kube-apiserver:v1.24.1 k8s.gcr.io/kube-controller-manager:v1.24.1 k8s.gcr.io/kube-scheduler:v1.24.1 k8s.gcr.io/kube-proxy:v1.24.1 k8s.gcr.io/pause:3.7 k8s.gcr.io/etcd:3.5.3-0 k8s.gcr.io/coredns/coredns:v1.8.6 # and then pull them ubuntu@kates-control:~$ sudo kubeadm config images pull --kubernetes-version stable-1.24 --cri-socket unix:///run/containerd/containerd.sock [config/images] Pulled k8s.gcr.io/kube-apiserver:v1.24.1 [config/images] Pulled k8s.gcr.io/kube-controller-manager:v1.24.1 [config/images] Pulled k8s.gcr.io/kube-scheduler:v1.24.1 [config/images] Pulled k8s.gcr.io/kube-proxy:v1.24.1 [config/images] Pulled k8s.gcr.io/pause:3.7 [config/images] Pulled k8s.gcr.io/etcd:3.5.3-0 [config/images] Pulled k8s.gcr.io/coredns/coredns:v1.8.6 ubuntu@kates-control:~$ sudo ctr -n k8s.io image ls -q | grep -v sha k8s.gcr.io/coredns/coredns:v1.8.6 k8s.gcr.io/etcd:3.5.3-0 k8s.gcr.io/kube-apiserver:v1.24.1 k8s.gcr.io/kube-controller-manager:v1.24.1 k8s.gcr.io/kube-proxy:v1.24.1 k8s.gcr.io/kube-scheduler:v1.24.1 k8s.gcr.io/pause:3.7 Using crictl instead If you prefer to use crictl, you can create a configuration file for containerd to avoid the warnings ubuntu@kates-control:~$ sudo tee cat /etc/crictl.yaml \u0026lt;\u0026lt;EOF \u0026gt; runtime-endpoint: unix:///var/run/containerd/containerd.sock \u0026gt; image-endpoint: unix:///run/containerd/containerd.sock \u0026gt; timeout: 10 \u0026gt; debug: false \u0026gt; EOF runtime-endpoint: unix:///var/run/containerd/containerd.sock image-endpoint: unix:///run/containerd/containerd.sock timeout: 10 debug: false ubuntu@kates-control:~$ sudo crictl images IMAGE TAG IMAGE ID SIZE k8s.gcr.io/coredns/coredns v1.8.6 a4ca41631cc7a 13.6MB k8s.gcr.io/etcd 3.5.3-0 aebe758cef4cd 102MB k8s.gcr.io/kube-apiserver v1.24.1 e9f4b425f9192 33.8MB k8s.gcr.io/kube-controller-manager v1.24.1 b4ea7e648530d 31MB k8s.gcr.io/kube-proxy v1.24.1 beb86f5d8e6cd 39.5MB k8s.gcr.io/kube-scheduler v1.24.1 18688a72645c5 15.5MB k8s.gcr.io/pause 3.7 221177c6082a8 311kB Now we can use kubeadm to bootstrap the node setting the POD network and, again, by optionally specifying the version. The init process will get the appropriate images if they do not exist locally already. ubuntu@kates-control:~$ sudo kubeadm init --pod-network-cidr=10.244.0.0/16 --cri-socket /run/containerd/containerd.sock --kubernetes-version stable-1.24 W0606 09:39:22.344649 33934 initconfiguration.go:120] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme \u0026#34;unix\u0026#34; to the \u0026#34;criSocket\u0026#34; with value \u0026#34;/run/containerd/containerd.sock\u0026#34;. Please update your configuration! [init] Using Kubernetes version: v1.24.1 [preflight] Running pre-flight checks [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using \u0026#39;kubeadm config images pull\u0026#39; [certs] Using certificateDir folder \u0026#34;/etc/kubernetes/pki\u0026#34; [certs] Generating \u0026#34;ca\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver\u0026#34; certificate and key [certs] apiserver serving cert is signed for DNS names [kates-control kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.sv","date":"2022-05-31","objectID":"/posts/kube-my-router-pt2/:3:4","tags":["k8s","kubeadm","Ubuntu"],"title":"Kube my router up! - Part Two","uri":"/posts/kube-my-router-pt2/#bootstrap-k8s-control-node"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"Install network CNI Now, still on the control node, we are going to deploy the Flannel CNI plugin. ubuntu@kates-control:~$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml Warning: policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ podsecuritypolicy.policy/psp.flannel.unprivileged created clusterrole.rbac.authorization.k8s.io/flannel created clusterrolebinding.rbac.authorization.k8s.io/flannel created serviceaccount/flannel created configmap/kube-flannel-cfg created daemonset.apps/kube-flannel-ds createdCheck how it looks on control-plane. ubuntu@kates-control:~$ kubectl get all -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/coredns-6d4b75cb6d-fvskj 1/1 Running 0 8m12s kube-system pod/coredns-6d4b75cb6d-xrflm 1/1 Running 0 8m12s kube-system pod/etcd-kates-control 1/1 Running 0 8m25s kube-system pod/kube-apiserver-kates-control 1/1 Running 0 8m27s kube-system pod/kube-controller-manager-kates-control 1/1 Running 0 8m25s kube-system pod/kube-flannel-ds-55st2 1/1 Running 0 65s kube-system pod/kube-proxy-8wjpp 1/1 Running 0 8m13s kube-system pod/kube-scheduler-kates-control 1/1 Running 0 8m25s NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 8m27s kube-system service/kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 8m25s NAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE kube-system daemonset.apps/kube-flannel-ds 1 1 1 1 1 \u0026lt;none\u0026gt; 65s kube-system daemonset.apps/kube-proxy 1 1 1 1 1 kubernetes.io/os=linux 8m25s NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE kube-system deployment.apps/coredns 2/2 2 2 8m25s NAMESPACE NAME DESIRED CURRENT READY AGE kube-system replicaset.apps/coredns-6d4b75cb6d 2 2 2 8m13sAnd from the network side. ubuntu@kates-control:~$ ip ro ; ip add default via 192.168.1.1 dev ens3 proto static 10.244.0.0/24 dev cni0 proto kernel scope link src 10.244.0.1 192.168.1.0/24 dev ens3 proto kernel scope link src 192.168.1.30 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: ens3: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 00:50:02:00:01:00 brd ff:ff:ff:ff:ff:ff inet 192.168.1.30/24 brd 192.168.1.255 scope global ens3 valid_lft forever preferred_lft forever inet6 2a02:587:e44a:d37:250:2ff:fe00:100/64 scope global dynamic mngtmpaddr noprefixroute valid_lft 604763sec preferred_lft 86363sec inet6 fe80::250:2ff:fe00:100/64 scope link valid_lft forever preferred_lft forever 3: flannel.1: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue state UNKNOWN group default link/ether f2:64:03:ac:1f:68 brd ff:ff:ff:ff:ff:ff inet 10.244.0.0/32 scope global flannel.1 valid_lft forever preferred_lft forever inet6 fe80::f064:3ff:feac:1f68/64 scope link valid_lft forever preferred_lft forever 4: cni0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue state UP group default qlen 1000 link/ether e2:92:cd:73:85:3d brd ff:ff:ff:ff:ff:ff inet 10.244.0.1/24 brd 10.244.0.255 scope global cni0 valid_lft forever preferred_lft forever inet6 fe80::e092:cdff:fe73:853d/64 scope link valid_lft forever preferred_lft forever 5: vethdc5600c6@if3: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue master cni0 state UP group default link/ether f2:8f:bd:a7:dc:82 brd ff:ff:ff:ff:ff:ff link-netns cni-0155810e-5cb5-58a5-22c4-dc8038081174 inet6 fe80::f08f:bdff:fea7:dc82/64 scope link valid_lft forever preferred_lft forever 6: vethe9a1a648@if3: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue master cni0 state UP group default link/ether aa:09:c5:8c:bb:66 brd ff:ff:ff:ff:ff:ff link-netns cni-d71bf00b","date":"2022-05-31","objectID":"/posts/kube-my-router-pt2/:3:5","tags":["k8s","kubeadm","Ubuntu"],"title":"Kube my router up! - Part Two","uri":"/posts/kube-my-router-pt2/#install-network-cni"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"Add worker nodes Let\u0026rsquo;s go and add the two other nodes to our cluster now. We can do this either by using the init token or we could create a new one. ubuntu@kates-control:~$ kubeadm token create --print-join-command kubeadm join 192.168.1.30:6443 --token r8ydn9.nkbo1b94w7a6s8em --discovery-token-ca-cert-hash sha256:42a47eb0e4872ebdd7ebbd6935a6e1c42f51a67d29d80fd53098d27323c59b2eNow on both worker nodes. ubuntu@kates-node-01:~$ sudo kubeadm join 192.168.1.30:6443 --token r8ydn9.nkbo1b94w7a6s8em --discovery-token-ca-cert-hash sha256:42a47eb0e4872ebdd7ebbd6935a6e1c42f51a67d29d80fd53098d27323c59b2e [preflight] Running pre-flight checks [preflight] Reading configuration from the cluster... [preflight] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -o yaml\u0026#39; [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [kubelet-start] Writing kubelet environment file with flags to file \u0026#34;/var/lib/kubelet/kubeadm-flags.env\u0026#34; [kubelet-start] Starting the kubelet [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap... This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run \u0026#39;kubectl get nodes\u0026#39; on the control-plane to see this node join the cluster.If all went well then we should have a 3 node k8s cluster \u0026#x1f604; ubuntu@kates-control:~$ kubectl get nodes -owide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME kates-control Ready control-plane 11m v1.24.1 192.168.1.30 \u0026lt;none\u0026gt; Ubuntu 20.04.4 LTS 5.4.0-117-generic containerd://1.6.6 kates-node-01 Ready \u0026lt;none\u0026gt; 66s v1.24.1 192.168.1.31 \u0026lt;none\u0026gt; Ubuntu 20.04.4 LTS 5.4.0-117-generic containerd://1.6.6 kates-node-02 Ready \u0026lt;none\u0026gt; 59s v1.24.1 192.168.1.32 \u0026lt;none\u0026gt; Ubuntu 20.04.4 LTS 5.4.0-117-generic containerd://1.6.6Final touch is to set the worker role to the workers and, if you like to run pods on the control node as well, untaint the node. ubuntu@kates-control:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION kates-control Ready control-plane 4h37m v1.24.1 kates-node-01 Ready \u0026lt;none\u0026gt; 139m v1.24.1 kates-node-02 Ready \u0026lt;none\u0026gt; 138m v1.24.1 ubuntu@kates-control:~$ kubectl label nodes kates-node-01 kubernetes.io/role=worker node/kates-node-01 labeled ubuntu@kates-control:~$ kubectl label nodes kates-node-02 kubernetes.io/role=worker node/kates-node-02 labeled ubuntu@kates-control:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION kates-control Ready control-plane 4h37m v1.24.1 kates-node-01 Ready worker 140m v1.24.1 kates-node-02 Ready worker 138m v1.24.1 ubuntu@kates-control:~$ # Untaint the control plane to run pods if you wish ubuntu@kates-control:~$ kubectl taint nodes kates-control node-role.kubernetes.io/control-plane=:NoSchedule- node-role.kubernetes.io/master=:NoSchedule- node/kates-control untainted ","date":"2022-05-31","objectID":"/posts/kube-my-router-pt2/:3:6","tags":["k8s","kubeadm","Ubuntu"],"title":"Kube my router up! - Part Two","uri":"/posts/kube-my-router-pt2/#add-worker-nodes"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"Testing applications Let\u0026rsquo;s deploy a test application to verify the cluster is running fine. We can create a deployment of nginx demo image with three replicas and test connectivity. The three pods will normaly be distributed across the three nodes, so we can use a Loadbalancer service to expose the deployment and check the operation. # Create a 3 replica deployment of nginx demo image ubuntu@kates-control:~$ kubectl create deployment web --image=nginxdemos/hello --replicas=3 --port 80 deployment.apps/web created # Check to see that pods are distributed across the three nodes ubuntu@kates-control:~$ kubectl get all -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/web-54b75887bb-2glrx 1/1 Running 0 11s 10.244.2.3 kates-node-02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/web-54b75887bb-7jrf4 1/1 Running 0 11s 10.244.1.2 kates-node-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/web-54b75887bb-j76kz 1/1 Running 0 11s 10.244.2.2 kates-node-02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 12m \u0026lt;none\u0026gt; NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR deployment.apps/web 3/3 3 3 11s hello nginxdemos/hello app=web NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR replicaset.apps/web-54b75887bb 3 3 3 11s hello nginxdemos/hello app=web,pod-template-hash=54b75887bb # Create a LoadBalancer service ubuntu@kates-control:~$ kubectl expose deployment web --type=LoadBalancer --name=web-service service/web-service exposed ubuntu@kates-control:~$ kubectl get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 13m \u0026lt;none\u0026gt; web-service LoadBalancer 10.101.141.116 \u0026lt;pending\u0026gt; 80:30721/TCP 10s app=web # Test communication to each pod ubuntu@kates-control:~$ curl -s 10.244.2.3 | grep Server \u0026lt;p\u0026gt;\u0026lt;span\u0026gt;Server\u0026amp;nbsp;address:\u0026lt;/span\u0026gt; \u0026lt;span\u0026gt;10.244.2.3:80\u0026lt;/span\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span\u0026gt;Server\u0026amp;nbsp;name:\u0026lt;/span\u0026gt; \u0026lt;span\u0026gt;web-54b75887bb-2glrx\u0026lt;/span\u0026gt;\u0026lt;/p\u0026gt; ubuntu@kates-control:~$ curl -s 10.244.1.2 | grep Server \u0026lt;p\u0026gt;\u0026lt;span\u0026gt;Server\u0026amp;nbsp;address:\u0026lt;/span\u0026gt; \u0026lt;span\u0026gt;10.244.1.2:80\u0026lt;/span\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span\u0026gt;Server\u0026amp;nbsp;name:\u0026lt;/span\u0026gt; \u0026lt;span\u0026gt;web-54b75887bb-7jrf4\u0026lt;/span\u0026gt;\u0026lt;/p\u0026gt; ubuntu@kates-control:~$ curl -s 10.244.2.2 | grep Server \u0026lt;p\u0026gt;\u0026lt;span\u0026gt;Server\u0026amp;nbsp;address:\u0026lt;/span\u0026gt; \u0026lt;span\u0026gt;10.244.2.2:80\u0026lt;/span\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span\u0026gt;Server\u0026amp;nbsp;name:\u0026lt;/span\u0026gt; \u0026lt;span\u0026gt;web-54b75887bb-j76kz\u0026lt;/span\u0026gt;\u0026lt;/p\u0026gt; # Now test on the exposed port on the k8s hosts ubuntu@kates-control:~$ for server in kates-control kates-node-01 kates-node-02 ; \\ \u0026gt; do \\ \u0026gt; echo \u0026#34;*** $server ***\u0026#34; ; \\ \u0026gt; for i in {1..5}; \\ \u0026gt; do \\ \u0026gt; curl -s $server:30721 | grep address ; \\ \u0026gt; done ; \\ \u0026gt; done *** kates-control *** \u0026lt;p\u0026gt;\u0026lt;span\u0026gt;Server\u0026amp;nbsp;address:\u0026lt;/span\u0026gt; \u0026lt;span\u0026gt;10.244.2.3:80\u0026lt;/span\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span\u0026gt;Server\u0026amp;nbsp;address:\u0026lt;/span\u0026gt; \u0026lt;span\u0026gt;10.244.2.3:80\u0026lt;/span\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span\u0026gt;Server\u0026amp;nbsp;address:\u0026lt;/span\u0026gt; \u0026lt;span\u0026gt;10.244.2.2:80\u0026lt;/span\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span\u0026gt;Server\u0026amp;nbsp;address:\u0026lt;/span\u0026gt; \u0026lt;span\u0026gt;10.244.2.2:80\u0026lt;/span\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span\u0026gt;Server\u0026amp;nbsp;address:\u0026lt;/span\u0026gt; \u0026lt;span\u0026gt;10.244.2.3:80\u0026lt;/span\u0026gt;\u0026lt;/p\u0026gt; *** kates-node-01 *** \u0026lt;p\u0026gt;\u0026lt;span\u0026gt;Server\u0026amp;nbsp;address:\u0026lt;/span\u0026gt; \u0026lt;span\u0026gt;10.244.2.3:80\u0026lt;/span\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span\u0026gt;Server\u0026amp;nbsp;address:\u0026lt;/span\u0026gt; \u0026lt;span\u0026gt;10.244.2.3:80\u0026lt;/span\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span\u0026gt;Server\u0026amp;nbsp;address:\u0026lt;/span\u0026gt; \u0026lt;span\u0026gt;10.244.1.2:80\u0026lt;/span\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span\u0026gt;Server\u0026amp;nbsp;address:\u0026lt;/span\u0026gt; \u0026lt;span\u0026gt;10.244.1.2:80\u0026lt;/span\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span\u0026gt;Serve","date":"2022-05-31","objectID":"/posts/kube-my-router-pt2/:3:7","tags":["k8s","kubeadm","Ubuntu"],"title":"Kube my router up! - Part Two","uri":"/posts/kube-my-router-pt2/#testing-applications"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"References and influences Bootstrapping clusters with kubeadm Install Kubernetes Cluster on Ubuntu 20.04 with kubeadm Kubernetes 1.23 + containerd Tracing the path of network traffic in Kubernetes Exposing an External IP Address to Access an Application in a Cluster ","date":"2022-05-31","objectID":"/posts/kube-my-router-pt2/:3:8","tags":["k8s","kubeadm","Ubuntu"],"title":"Kube my router up! - Part Two","uri":"/posts/kube-my-router-pt2/#references-and-influences"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"Last Part - Installing and testing KNE ","date":"2022-05-31","objectID":"/posts/kube-my-router-pt2/:4:0","tags":["k8s","kubeadm","Ubuntu"],"title":"Kube my router up! - Part Two","uri":"/posts/kube-my-router-pt2/#last-part---installing-and-testing-kne"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"Outro ","date":"2022-05-31","objectID":"/posts/kube-my-router-pt2/:5:0","tags":["k8s","kubeadm","Ubuntu"],"title":"Kube my router up! - Part Two","uri":"/posts/kube-my-router-pt2/#outro"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"Intro While I was ramping up my skills in k8s, I bumped into google\u0026rsquo;s Kubernetes based Network Emulation (KNE), and since I\u0026rsquo;ve been a fool for network emulation software over the last twenty years, I knew I had to give it a go and see where it can get me. I first heard of KNE during episode #015 of this podcast and it seemed very promising to me since it is a way to spin up network topologies from devices running in containers orchestrated by a kubernetes cluster that, guess what, can span more than one host. So, in these series of posts I will try to document my experiences setting it up and having fun with it \u0026#x1f604; since up to now there is too little documentation around it and also I think this blog post was the first to cover a basic orientation but still it covers running it inside a kind k8s node, i.e. in a container, where as my intent was to have the k8s spanning more than one machine since network devices are expensive in resources and my home lab is limited to 2x32GB EVE-NG servers. Part One - Setting up the k8s VMs in EVE-NG Part Two - Deploying the k8s cluster with kubeadm Last Part - Installing and testing KNE ","date":"2022-05-28","objectID":"/posts/kube-my-router-pt1/:1:0","tags":["EVE-NG","Ubuntu"],"title":"Kube my router up! - Part One","uri":"/posts/kube-my-router-pt1/#intro"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"Part One - Setting up the VMs ","date":"2022-05-28","objectID":"/posts/kube-my-router-pt1/:2:0","tags":["EVE-NG","Ubuntu"],"title":"Kube my router up! - Part One","uri":"/posts/kube-my-router-pt1/#part-one---setting-up-the-vms"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"The Intent EVE-NG for hosting linux VMs Ubuntu 20.04 cloud image as base 2xEVE-NG servers, 2.0.3-112 community edition - 3 VMs in total Deployment of 3xLinux VMs for our k8s cluster Well the first step towards testing KNE is to set up a k8s cluster. I have two EVE-NG bare metal servers at home so I could spin up three VMs split in those two servers of 32GM RAM each. I\u0026rsquo;ve chosen Ubuntu 20.04 linux flavor and decided to start by deploying the VMs using the cloud-image files. The approach followed here is to download the cloud image in both EVE-NG servers and create a seed file in order to pass the user password during first boot. This will be used as the base ubuntu image for all VMs. Finally, once the base VM is ready, the topology can be created and all the VMs can be brought up and configured with the appropriate settings. ","date":"2022-05-28","objectID":"/posts/kube-my-router-pt1/:2:1","tags":["EVE-NG","Ubuntu"],"title":"Kube my router up! - Part One","uri":"/posts/kube-my-router-pt1/#the-intent"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"Prepare Ubuntu base image in EVE-NG Download Ubuntu 20.04 cloud image and place it into a temporary directory on the first server, optionally, verifying the md5sum. root@eve-01# mkdir -p /opt/cloud \u0026amp;\u0026amp; cd !$ mkdir -p /opt/cloud \u0026amp;\u0026amp; cd /opt/cloud/ root@eve-01# wget https://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64.img \u0026lt; ...omitted... \u0026gt; root@eve-01# ll total 581388 drwxr-xr-x 2 root root 4096 May 29 21:19 ./ drwxr-xr-x 16 root root 4096 May 29 21:19 ../ -rw-r--r-- 1 root root 595329024 May 24 01:51 focal-server-cloudimg-amd64.img root@eve-01# curl -ks https://cloud-images.ubuntu.com/focal/current/MD5SUMS | md5sum -c --ignore-missing focal-server-cloudimg-amd64.img: OKCopy the image to the appropriate linux- directory of EVE-NG in so that is categorised as a linux image. You can resize the image to your linking as well. root@eve-01# mkdir -p /opt/unetlab/addons/qemu/linux-focal-server-cloudimg/ root@eve-01# cp focal-server-cloudimg-amd64.img !$hda.qcow2 cp focal-server-cloudimg-amd64.img /opt/unetlab/addons/qemu/linux-focal-server-cloudimg/hda.qcow2 root@eve-01# cd /opt/unetlab/addons/qemu/linux-focal-server-cloudimg/ root@eve-01# ll total 581384 drwxr-xr-x 2 root root 4096 May 29 21:46 ./ drwxr-xr-x 12 root root 4096 May 29 21:45 ../ -rw-r--r-- 1 root root 595329024 May 29 21:46 hda.qcow2 root@eve-01# qemu-img info hda.qcow2 image: hda.qcow2 file format: qcow2 virtual size: 2.2G (2361393152 bytes) disk size: 568M cluster_size: 65536 Format specific information: compat: 0.10 refcount bits: 16 root@eve-01# qemu-img resize hda.qcow2 50G Image resized. root@eve-01# qemu-img info hda.qcow2 image: hda.qcow2 file format: qcow2 virtual size: 50G (53687091200 bytes) disk size: 568M cluster_size: 65536 Format specific information: compat: 0.10 refcount bits: 16 ","date":"2022-05-28","objectID":"/posts/kube-my-router-pt1/:2:2","tags":["EVE-NG","Ubuntu"],"title":"Kube my router up! - Part One","uri":"/posts/kube-my-router-pt1/#prepare-ubuntu-base-image-in-eve-ng"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"Install cloud-image utilities and prepare the seed file Install the required package. root@eve-01# cd /opt/cloud/ root@eve-01# apt install cloud-image-utils \u0026lt; ...omitted...\u0026gt;Prepare the seed file and copy it to the base image location. root@eve-01# cat \u0026lt;\u0026lt;EOF \u0026gt; cloud-config #cloud-config system_info: default_user: name: \u0026#34;ubuntu\u0026#34; home: /home/ubuntu password: \u0026#34;ubuntu123\u0026#34; chpasswd: { expire: False } hostname: \u0026#34;ubuntu\u0026#34; ssh_pwauth: True EOF root@eve-01# cloud-localds seed cloud-config root@eve-01# qemu-img info seed image: seed file format: raw virtual size: 366K (374784 bytes) disk size: 368K root@eve-01# cp seed /opt/unetlab/addons/qemu/linux-focal-server-cloudimg/cdrom.iso Heads Up Make sure you include the comment #cloud-config in the seed file ","date":"2022-05-28","objectID":"/posts/kube-my-router-pt1/:2:3","tags":["EVE-NG","Ubuntu"],"title":"Kube my router up! - Part One","uri":"/posts/kube-my-router-pt1/#install-cloud-image-utilities-and-prepare-the-seed-file"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"Build the VM topology in EVE-NG Once both boot disk and seed cdrom files are in place, copy them onto the other node and run the fixpermissions script. root@eve-01# cd /opt/unetlab/addons/qemu/linux-focal-server-cloudimg/ root@eve-01# ll total 581760 drwxr-xr-x 2 root root 4096 May 29 22:02 ./ drwxr-xr-x 12 root root 4096 May 29 21:45 ../ -rw-r--r-- 1 root root 374784 May 29 22:02 cdrom.iso -rw-r--r-- 1 root root 595330048 May 29 21:47 hda.qcow2 root@eve-01# /opt/unetlab/wrappers/unl_wrapper -a fixpermissions root@eve-01# ssh eve-02 \u0026#34;mkdir -p /opt/unetlab/addons/qemu/linux-focal-server-cloudimg\u0026#34; root@eve-01# scp hda.qcow2 eve-02:/opt/unetlab/addons/qemu/linux-focal-server-cloudimg/ root@eve-01# scp cdrom.iso 192.168.1.22:/opt/unetlab/addons/qemu/linux-focal-server-cloudimg/ root@eve-01# ssh eve-02 \u0026#34;/opt/unetlab/wrappers/unl_wrapper -a fixpermissions\u0026#34;Now everything is ready to start creating the topology via the GUI. The following table lists the planning for my deployment: EVE-NG node Role VM name IP address CPU RAM eve-01 control-node kates-control 192.168.1.30 4 4096 eve-01 worker kates-node-01 192.168.1.31 8 28672 eve-02 worker kates-node-02 192.168.1.32 8 28672 Here is how it looks like from GUI perspective: EVE-01 Topology EVE-02 Topology You can also download the lab exports for eve-01 and eve-02 Heads Up Due to the fact that EVE-NG allocates MAC addresses sequentially from a pool which is allocated per-POD and if you are using the same LAB POD on both servers, kates-control and kates-node-02 (i.e. the first VMs on each node, since only EVE-01 has two nodes) will end up having the same MAC address. In order to avoid this, we can either use dummy VMs, i.e. powered off VMs or we can specify the value for the first MAC address on the kates-node-02 VM to cause EVE-NG to allocate a different MAC to the NIC, or simply we can use different EVE-NG PODs across the two servers to create the lab \u0026#x1f604;. I went for the latter and simpler method. Change MAC address of VM on second EVE server Have this also in mind for all your other labs and topologies. The MAC allocation is per lab POD. ","date":"2022-05-28","objectID":"/posts/kube-my-router-pt1/:2:4","tags":["EVE-NG","Ubuntu"],"title":"Kube my router up! - Part One","uri":"/posts/kube-my-router-pt1/#build-the-vm-topology-in-eve-ng"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"Start the labs and perform initial configuration All VMs are ready to start. Once all nodes are booted you can access them from the VNC or HTML5 console and perform the initial configuration, which includes: Disabling cloud-init Setting the hostname Configuring network addresses Updating and Upgrading But, let\u0026rsquo;s first check that all VMs are different and also the MAC addresses are not the same. The outputs shown are when using the same LAB POD in EVE-NG. # On kates-control ubuntu@ubuntu:~$ sudo dmidecode -s system-uuid \u0026amp;\u0026amp; cat /etc/machine-id \u0026amp;\u0026amp; ip link df8f1876-080b-4bb0-b000-f63819c781f3 df8f1876080b4bb0b000f63819c781f3 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: ens3: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000 link/ether 00:50:00:00:01:00 brd ff:ff:ff:ff:ff:ff# On kates-node-01 ubuntu@ubuntu:~$ sudo dmidecode -s system-uuid \u0026amp;\u0026amp; cat /etc/machine-id \u0026amp;\u0026amp; ip link 2b748bae-16c2-4abe-b65e-21a359658480 2b748bae16c24abeb65e21a359658480 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: ens3: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000 link/ether 00:50:00:00:02:00 brd ff:ff:ff:ff:ff:ff# On kates-node-02 ubuntu@ubuntu:~$ sudo dmidecode -s system-uuid \u0026amp;\u0026amp; cat /etc/machine-id \u0026amp;\u0026amp; ip link 9ba9c500-5d18-40e8-98e4-3e027f84e971 9ba9c5005d1840e898e43e027f84e971 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: ens3: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000 link/ether 00:60:00:00:01:00 brd ff:ff:ff:ff:ff:ffOkay, now it is time to perform the initial configuration by disabling first the cloud-init service and then moving on. Note that If the cloud-init service is not disabled, then we will not be able to change the hostname, so either we disable cloud init or modify the appropriate config file in order not to reset the hostname according to the seed file So from the VM console: ubuntu@ubuntu:~$ sudo touch /etc/cloud/cloud-init.disabled ubuntu@ubuntu:~$ sudo hostnamectl set-hostname kates-controlAdjust the netplan file according to your environment and apply the configuration. I prefer to remove the static MAC assignment to the NIC ubuntu@ubuntu:~$ sudo vi /etc/netplan/50-cloud-init.yaml ubuntu@ubuntu:~$ cat /etc/netplan/50-cloud-init.yaml network: ethernets: ens3: dhcp4: false dhcp6: false addresses: [192.168.1.30/24] gateway4: 192.168.1.1 nameservers: addresses: [192.168.1.1, 8.8.8.8] version: 2 ubuntu@ubuntu:~$ sudo netplan apply Now that the basic config has been done for all 3 nodes, we can reboot and login from a proper SSH terminal \u0026#x1f604; to verify network connectivity and by the next single command we perform any upgrade available and reboot the nodes if needed. ubuntu@kates-*:~$ sudo apt update \\ \u0026amp;\u0026amp; sudo apt -y full-upgrade \\ \u0026amp;\u0026amp; [ -f /var/run/reboot-required ] \\ \u0026amp;\u0026amp; sudo reboot -f ","date":"2022-05-28","objectID":"/posts/kube-my-router-pt1/:2:5","tags":["EVE-NG","Ubuntu"],"title":"Kube my router up! - Part One","uri":"/posts/kube-my-router-pt1/#start-the-labs-and-perform-initial-configuration"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"References and influences EVE-NG official site Cloud config examples Using Cloud Images in KVM ","date":"2022-05-28","objectID":"/posts/kube-my-router-pt1/:2:6","tags":["EVE-NG","Ubuntu"],"title":"Kube my router up! - Part One","uri":"/posts/kube-my-router-pt1/#references-and-influences"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"Part Two - Deploying the k8s cluster with kubeadm ","date":"2022-05-28","objectID":"/posts/kube-my-router-pt1/:3:0","tags":["EVE-NG","Ubuntu"],"title":"Kube my router up! - Part One","uri":"/posts/kube-my-router-pt1/#part-two---deploying-the-k8s-cluster-with-kubeadm"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"Last Part - Installing and testing KNE ","date":"2022-05-28","objectID":"/posts/kube-my-router-pt1/:4:0","tags":["EVE-NG","Ubuntu"],"title":"Kube my router up! - Part One","uri":"/posts/kube-my-router-pt1/#last-part---installing-and-testing-kne"},{"categories":["Art of Labbing"],"collections":["Kube My Router Up!"],"content":"Outro ","date":"2022-05-28","objectID":"/posts/kube-my-router-pt1/:5:0","tags":["EVE-NG","Ubuntu"],"title":"Kube my router up! - Part One","uri":"/posts/kube-my-router-pt1/#outro"},{"categories":["humour"],"collections":null,"content":"Have you noticed that in some parts of the world, in some cafeterias, especially when the weather is hot, after you take your seat, a waiter comes and serves you a glass of water? Furthermore, in these places, the waiter has a frequent task to have this glass filled several times during your stay. Hence, we could say that the waiter has a task every once in a while to take a jug of water and re-fill the glasses of those customers that are not full, even if they are half-empty or half-full. We can say that the glass of water, in that case, is a mutable object, i.e. the glass is brought full at first and then its status can be changed by refilling it again. In addition, the task is idempotent because every time it is executed, the result would be the same, i.e. all glasses are full - unless someone asks no to have it filled \u0026#x1f604;. This drinking water idempotency task can also be seen in expensive restaurants, where again there is a waiter (maybe call him idempotent) that takes care of re-filling your glass and in such places it happens much more often and usually it happens per-table rather than for all customers at the same time \u0026#x1f604;. Now, this glass idempotency thing does not only apply to drinking water, but also to the wine glasses if you have ordered a bottle of wine, that is. But, what happens if you have only ordered and consumed a single glass of wine? Your glass is empty and the waiter will come and ask you if you\u0026rsquo;d like another one. You say yes, and then it happens. The waiter will take the empty glass and will go and fetch a new glass of wine for you, i.e. replacing the previous one. At this moment, you realize that the wine glass is not a mutable object, but it is immutable \u0026#x1f604;. It cannot be re-filled, it cannot be changed and it has to be replaced by another glass of wine that could be the same or some other type, or something completely different, but they all will refer to the object you drink. You enjoyed your wine, it fulfilled its purpose, but if you want it to serve it\u0026rsquo;s purpose again, it has to be replaced with a new one or a new drink \u0026#x1f604; ","date":"2022-04-30","objectID":"/posts/mutable-immutable/:0:0","tags":["automation"],"title":"Mutable vs. Immutable","uri":"/posts/mutable-immutable/#"},{"categories":["humour"],"collections":null,"content":"Imagine a beautiful sunny morning you walk into a dinner to have some brunch. You take a seat on a free table and the waitress comes over to take your order while filling a glass of water for you. -\u0026gt; May I have two eggs, sunny side up, and some crispy bacon, please? =\u0026gt; Sure darling, anything to drink? -\u0026gt; I\u0026rsquo;d like some orange juice - no ice, please. =\u0026gt; Coming right up! The waitress takes down your order in a peace of paper, goes over to the kitchen service area and hands in the paper to the chef. The chef reads the order and starts shouting to his newbie cooks: -\u0026gt; Miguel, Heat a pan with four bacon strips in it Take two eggs out of the fridge When the bacon is crisp enough, move it from the pan to a plate Crack the two eggs as close to the pan as you can and lower the heat When the eggs are ready, remove the pan from the stove and put the eggs on a plate -\u0026gt; Karla, Lay some butter on both sides on two toast bread slices Put the toast in a heated oven for 6 minutes When done and the bread is crisp enough, remove them from the oven and put them on a plate Poor a glass of orange juice from the fridge with no ice Eventually, after Karla and Miguel have followed the chef\u0026rsquo;s instructions and prepared all the components, the chef can assemble them on a plate, season the eggs with salt and pepper, check if all is ok and along with the orange juice pass them to the waitress who will bring them over to your table and you will enjoy your brunch. Well I guess by now you would have noticed the difference of what declarative versus imperative means \u0026#x1f604; Your order is the declarative way of doing things. You just describe what you want to be done, the end result. On the other hand, what the chef is doing, could be though of as the imperative way. He is giving commands, instructions on how to do individual things and tasks in order to achieve the end result. Now, if we want to take it one step further, let\u0026rsquo;s say Karla and Miguel are, by now, much more experienced and the chef does not have to explicitly tell them how to prepare every step of the recipe. In plain, they can automate the tasks and the chef has just to say: eggs - bacon - bread - orange juice, and that\u0026rsquo;s it. It all happens faster. So, the chef is able to orchesrtate the automated tasks that the cooks can perform to produce the result. Now, let\u0026rsquo;s go back to the top and say that you just order the sunny sunday morning from the menu, i.e. the service catalog, won\u0026rsquo;t the whole process be faster and more clear? Yes, as long as the waitress tells the chef and he does not forget to instruct Karla not to put ice in the orange juice \u0026#x1f604; Now I am getting hungry\u0026hellip; ","date":"2022-04-28","objectID":"/posts/declarative-imperative/:0:0","tags":["automation"],"title":"Declarative vs. Imperative","uri":"/posts/declarative-imperative/#"},{"categories":["humour"],"collections":null,"content":" Πώς λέγεται όταν χρησιμοποιούμε terraform με το Cisco ACI; ","date":"2022-04-21","objectID":"/posts/tf-aci/:0:0","tags":["bad-jokes"],"title":"TF-ACI","uri":"/posts/tf-aci/#"},{"categories":["humour"],"collections":null,"content":"TF-ACI\u0026hellip;Τι φάααση! \u0026#x1f44e; \u0026#x1f47f; \u0026lt;click again to collapse and let\u0026rsquo;s pretend you didn\u0026rsquo;t read the joke at all\u0026gt; ","date":"2022-04-21","objectID":"/posts/tf-aci/:1:0","tags":["bad-jokes"],"title":"TF-ACI","uri":"/posts/tf-aci/#tf-aciτι-φάααση--1-imp"}]